{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Numerical-Optimization\" data-toc-modified-id=\"Numerical-Optimization-1\">Numerical Optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-set-3-(14-points)\" data-toc-modified-id=\"Problem-set-3-(14-points)-1.1\">Problem set 3 (14 points)</a></span></li><li><span><a href=\"#Bonus-(4-points)\" data-toc-modified-id=\"Bonus-(4-points)-1.2\">Bonus (4 points)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rafa≈Ç Nowak\n",
    "# Numerical Optimization\n",
    "## Problem set 3 (14 points)\n",
    "\n",
    "**Submission deadline**: Thursday, 02.02.2022\n",
    "\n",
    "* All submissions should contain single file.<br/>This can be single Jupyter notebook file (with extension `ipynb`) or ZIP archive in case the are some additional files needed.\n",
    "* It is recommended to write the reports using LaTeX. \n",
    "* One can report the answers, comments and results in PDF or notebook file.\n",
    "* All the source code should be written in Python or Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ortho_group\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5.1** (total 8 pts)\n",
    "\n",
    "(2 pts) Complete the implementation of Newton's method (see [Boyd, *Convex Optimization*, $\\S 9.5.2$])\n",
    "<img src=\"https://i.ibb.co/RvqY16d/Boyd-Newton-method.png\" alt=\"Boyd-Newton-method\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_EVALS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_line_search(f, x, direction, iterations, epsilon=10e-10, s=1e-2, k=2.0):\n",
    "    global COUNT_EVALS\n",
    "    def find_initial_bracket(f, s=1e-2, k=2.0):\n",
    "        global COUNT_EVALS\n",
    "        v = f(0)\n",
    "        t1 = 0\n",
    "        while v > 0:\n",
    "            t1 += s\n",
    "            s *= k\n",
    "            v = f(t1)\n",
    "        \n",
    "        t2 = t1\n",
    "        while v < 0:\n",
    "            t1 = t2\n",
    "            t2 += s\n",
    "            s *= k\n",
    "            v = f(t2)\n",
    "        return t1, t2\n",
    "  \n",
    "    def g(t):\n",
    "        global COUNT_EVALS\n",
    "        COUNT_EVALS += 1\n",
    "        return f(x + t*direction, order=1)[1].T @ direction\n",
    "    \n",
    "    def bisection(f, t1, t2, epsilon):\n",
    "        global COUNT_EVALS\n",
    "        mid = (t1+t2)/2\n",
    "        v = f(mid)\n",
    "        while (t2-t1) > epsilon:\n",
    "            if v < 0:\n",
    "                t1 = mid\n",
    "            else:\n",
    "                t2 = mid\n",
    "            mid = (t1+t2)/2\n",
    "            v = f(mid)\n",
    "        return (t1+t2)/2\n",
    "\n",
    "    t1, t2 = find_initial_bracket(g, s=s, k=k)\n",
    "    t = bisection(g, t1, t2, epsilon)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(func, x, direction, iterations, alpha, beta):\n",
    "    global COUNT_EVALS\n",
    "    t = 1.0\n",
    "    value, gradient = func( x , order=1 )\n",
    "    value_x = np.double( value )\n",
    "    gradient_x = np.matrix( gradient )\n",
    "    while func(x+t*direction, order=0) > value + alpha*t*gradient_x.T@direction:\n",
    "        COUNT_EVALS += 1\n",
    "        t *= beta\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remark: Implement bisection method first\n",
    "def newton(func, initial_x, *linesearch_args, eps=1e-5, maximum_iterations=65536, linesearch=backtracking,\n",
    "          print_decrement=False):\n",
    "    \"\"\" \n",
    "    Newton's Method\n",
    "    func:               the function to optimize It is called as \"value, gradient, hessian = func( x, 2 )\n",
    "    initial_x:          the starting point\n",
    "    eps:                the maximum allowed error in the resulting stepsize t\n",
    "    maximum_iterations: the maximum allowed number of iterations\n",
    "    linesearch:         the linesearch routine\n",
    "    *linesearch_args:   the extra arguments of linesearch routine\n",
    "    \"\"\"\n",
    "    \n",
    "    if eps <= 0:\n",
    "        raise ValueError(\"Epsilon must be positive\")\n",
    "    x = np.asarray( initial_x.copy() )\n",
    "    \n",
    "    # initialization\n",
    "    values = []\n",
    "    runtimes = []\n",
    "    xs = []\n",
    "    start_time = time.time()\n",
    "    iterations = 0\n",
    "    \n",
    "    # Newton's method updates\n",
    "    while True:\n",
    "        \n",
    "        value, gradient, hessian = func( x , order=2 )\n",
    "        value = np.double( value )\n",
    "        gradient = np.matrix( gradient )\n",
    "        hessian = np.matrix( hessian ) \n",
    "        \n",
    "        # updating the logs\n",
    "        values.append( value )\n",
    "        runtimes.append( time.time() - start_time )\n",
    "        xs.append( x.copy() )\n",
    "\n",
    "        ### TODO: Compute the Newton update direction\n",
    "        direction = - np.linalg.inv(hessian) @ gradient\n",
    "\n",
    "        ### TODO: Compute the Newton decrement\n",
    "        newton_decrement = gradient.T @ np.linalg.inv(hessian) @ gradient\n",
    "        if print_decrement:\n",
    "            print(newton_decrement)\n",
    "        \n",
    "\n",
    "        if newton_decrement / 2 <= eps:   ### TODO: TERMINATION CRITERION\n",
    "            break\n",
    "        \n",
    "        t = linesearch(func, x, direction, iterations, *linesearch_args)\n",
    "\n",
    "        ### TODO: update x\n",
    "        x = x + t * direction\n",
    "\n",
    "        iterations += 1\n",
    "        if iterations >= maximum_iterations:\n",
    "            raise ValueError(\"Too many iterations\")\n",
    "    \n",
    "    return (x, values, runtimes, xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation and compare the results for the following functions\n",
    "* (1 pts) function $$ f(x) = x^4 + 16x^2 + 18(x-4) e^x\\qquad (x\\in\\mathbb R). $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_example(x, order=2):\n",
    "    value = x**4 + 16 * x**2 + 18 * (x - 4) * np.exp(x)\n",
    "    if order==0:\n",
    "        return value\n",
    "    elif order==1:\n",
    "        gradient = 4 * x**3 + 32 * x + 18 * np.exp(x) * (x - 3)\n",
    "        return value, gradient\n",
    "    elif order==2:\n",
    "        gradient = 4 * x**3 + 32 * x + 18 * np.exp(x) * (x - 3)\n",
    "        hessian = 12 * x**2 + 18 * np.exp(x) * (x - 2) + 32\n",
    "        return value, gradient, hessian\n",
    "    else:\n",
    "        raise ValueError(\"The argument \\\"order\\\" should be 0, 1 or 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x252443f5550>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAlJklEQVR4nO3de3xcdZ3/8dcnk3ubS9OmbZqkN1qwLUiBUIqgIgoUdQXd1S0osC5a9QFeHuv+fiuu+/i5j/2h7sXLsgprBRYQBPtTV1gFFJCrQEuAllJ6S69JkzZJ09yaS5PM5/fHnNShTdO0zcyZZN7Px2Mec+Y758x8hsv7nHzP95yvuTsiIpIeMsIuQEREkkehLyKSRhT6IiJpRKEvIpJGFPoiImkkM+wCjmfKlCk+e/bssMsQERlTXn311WZ3Lz2yPeVDf/bs2VRXV4ddhojImGJmu4ZqV/eOiEgaUeiLiKQRhb6ISBpR6IuIpBGFvohIGlHoi4ikEYW+iEgaUeiLiKSYNTta+Pcnt9J9aGDUP1uhLyKSYl6oaeYHT20hM2Kj/tkKfRGRFLOvrYcpE3PIiox+RCv0RURSzN72HqYX5ibksxX6IiIpZl97D9MKcxLy2Qp9EZEUEwt9HemLiIx7PX0DHOjqU/eOiEg6aGzvBWBaUUihb2a5ZrbGzNaZ2QYz+8eg/ZtmtsfM1gaPD8Ztc4uZ1ZjZZjO7Iq79PDNbH7x3m5mN/ngkEZExbG97D0DCundGMolKL3Cpu3eaWRbwgpk9Frz3fXf/t/iVzWwhsBxYBMwAnjSz0919ALgDWAG8DDwKLAMeQ0REgFh/PhBe947HdAYvs4KHD7PJVcBD7t7r7juAGmCJmZUBhe7+krs7cB9w9SlVLyIyzoQe+gBmFjGztUAj8IS7rw7eutnM3jCzu81sUtBWDtTGbV4XtJUHy0e2D/V9K8ys2syqm5qaRv5rRETGuL1tPeRmZVCYl5jZbEcU+u4+4O6LgQpiR+1nEuuqOQ1YDDQA3w1WH6qf3odpH+r7Vrp7lbtXlZYeNa+viMi4NXhhVqJOeZ7Q6B13bwWeAZa5+75gZxAFfgIsCVarAyrjNqsA6oP2iiHaRUQksK+9h6kJ6tqBkY3eKTWz4mA5D/gAsCnoox/0UeDNYPkRYLmZ5ZjZHGA+sMbdG4AOM1sajNq5Hnh49H6KiMjYt6+9N2H9+TCy0TtlwL1mFiG2k1jl7r8xs5+a2WJiXTQ7gc8BuPsGM1sFvAX0AzcFI3cAvgDcA+QRG7WjkTsiIgF3j3XvJGiMPowg9N39DeCcIdqvG2abW4Fbh2ivBs48wRpFRNJCa1cfh/qjCRujD7oiV0QkZexN8HBNUOiLiKSMP12Nm5g7bIJCX0QkZTQm+BYMoNAXEUkZe9uCm60p9EVExr+97T1MnpBNdmbiolmhLyKSIhrauikrTtxRPij0RURSRn1rNzOK8hL6HQp9EZEU0dDaw4xihb6IyLjX3tNHR28/M9S9IyIy/jW0xoZrlql7R0Rk/Ktv7QZQ946ISDrYczj01b0jIjLuNbR1E8kwphYo9EVExr361tiMWZGMxMyYNUihLyKSAupbuxPetQMKfRGRlFDf1p3wk7gwsukSc81sjZmtM7MNZvaPQXuJmT1hZluD50lx29xiZjVmttnMrohrP8/M1gfv3WaJmvlXRGQMiUadvW09CR+uCSM70u8FLnX3s4HFwDIzWwp8DXjK3ecDTwWvMbOFwHJgEbAMuD2YahHgDmAFsXlz5wfvi4iktebOXvoGnPJU6N7xmM7gZVbwcOAq4N6g/V7g6mD5KuAhd+919x1ADbAkmEi90N1fcncH7ovbRkQkbdW3JefCLBhhn76ZRcxsLdAIPOHuq4Fp7t4AEDxPDVYvB2rjNq8L2sqD5SPbRUTSWrIuzIIRhr67D7j7YqCC2FH7cJObD9VP78O0H/0BZivMrNrMqpuamkZSoojImFWfpAuz4ARH77h7K/AMsb74fUGXDcFzY7BaHVAZt1kFUB+0VwzRPtT3rHT3KnevKi0tPZESRUTGnPrWHvKzIxTlZSX8u0YyeqfUzIqD5TzgA8Am4BHghmC1G4CHg+VHgOVmlmNmc4idsF0TdAF1mNnSYNTO9XHbiIikrYZguGYyBjRmjmCdMuDeYAROBrDK3X9jZi8Bq8zsRmA38HEAd99gZquAt4B+4CZ3Hwg+6wvAPUAe8FjwEBFJa3UHkjNGH0YQ+u7+BnDOEO37gfcfY5tbgVuHaK8GhjsfICKSduoOdPHOiqKkfJeuyBURCVFnbz8HuvqomJSflO9T6IuIhKi2pQuAypLkdO8o9EVEQlR3IDZcs1JH+iIi49/gkX7FJB3pi4iMe3UHusnPjlAyITsp36fQFxEJUe2BLiomJWeMPij0RURCVdvSlbT+fFDoi4iExt3Zc6CbyhKFvojIuNfW3UdHb3/STuKCQl9EJDS1LbHhmsm6MAsU+iIioak7kNzhmqDQFxEJTe2BwatxdaQvIjLu1bZ0U5ibmZT76A9S6IuIhKTuQFdS+/NBoS8iEpraA91Ju9HaIIW+iEgIolFnd0sXsyZPSOr3KvRFREKwt72HQ/1RZk1Ose4dM6s0s6fNbKOZbTCzLwft3zSzPWa2Nnh8MG6bW8ysxsw2m9kVce3nmdn64L3bLFk3mxARSTE79x8EYHaSj/RHMkduP/BVd3/NzAqAV83sieC977v7v8WvbGYLgeXAImAG8KSZnR7Mk3sHsAJ4GXgUWIbmyRWRNLRrf2y4Zsod6bt7g7u/Fix3ABuB8mE2uQp4yN173X0HUAMsMbMyoNDdX3J3B+4Drj7VHyAiMhbt3H+Q7EgGZUUpfCLXzGYTmyR9ddB0s5m9YWZ3m9mkoK0cqI3brC5oKw+Wj2wf6ntWmFm1mVU3NTWdSIkiImPCzuaDVJbkEclIbi/3iEPfzCYCvwS+4u7txLpqTgMWAw3AdwdXHWJzH6b96Eb3le5e5e5VpaWlIy1RRGTM2LW/K+n9+TDC0DezLGKB/4C7/wrA3fe5+4C7R4GfAEuC1euAyrjNK4D6oL1iiHYRkbTi7uzcf5DZU1Iw9IMRNncBG939e3HtZXGrfRR4M1h+BFhuZjlmNgeYD6xx9wagw8yWBp95PfDwKP0OEZExo7Gjl56+KLOTfBIXRjZ65yLgOmC9ma0N2r4OXGNmi4l10ewEPgfg7hvMbBXwFrGRPzcFI3cAvgDcA+QRG7WjkTsiknZ2NseGayb7wiwYQei7+wsM3R//6DDb3ArcOkR7NXDmiRQoIjLeDA7XTNk+fRERGT079x8kM8OYUZyb9O9W6IuIJNmu/V1UluSTGUl+BCv0RUSSbOf+g0m/EneQQl9EJIncPbQx+qDQFxFJqqaOXjp7+0MZrgkKfRGRpKpp6gRg3tSCUL5foS8ikkTbmmJj9E+bqu4dEZFxb3tTJ/nZEaYXJn+4Jij0RUSSalvTQeaWTiCsOaQU+iIiSbStsZPTSieG9v0KfRGRJOk+NMCe1m6FvohIOtjeHBu5o9AXEUkD20MeuQMKfRGRpNnW1IlZOHfXHKTQFxFJkm1NB6mYlEduViS0GhT6IiJJEvbIHRjZdImVZva0mW00sw1m9uWgvcTMnjCzrcHzpLhtbjGzGjPbbGZXxLWfZ2brg/dus7AGqoqIJFk06mxvHgOhT2zKw6+6+wJgKXCTmS0EvgY85e7zgaeC1wTvLQcWAcuA281s8G+ZO4AVxObNnR+8LyIy7tW3ddPTF2VuaXj9+TCC0Hf3Bnd/LVjuADYC5cBVwL3BavcCVwfLVwEPuXuvu+8AaoAlwUTqhe7+krs7cF/cNiIi49qWfR0AnDEtnButDTqhPn0zmw2cA6wGprl7A8R2DMDUYLVyoDZus7qgrTxYPrJ9qO9ZYWbVZlbd1NR0IiWKiKSkzXtjY/Tnj5XQN7OJwC+Br7h7+3CrDtHmw7Qf3ei+0t2r3L2qtLR0pCWKiKSsLfs6KCvKpSgvK9Q6RhT6ZpZFLPAfcPdfBc37gi4bgufGoL0OqIzbvAKoD9orhmgXERn3Nu/t4PSQj/JhZKN3DLgL2Oju34t76xHghmD5BuDhuPblZpZjZnOInbBdE3QBdZjZ0uAzr4/bRkRk3OofiFLT1MkZ08MP/cwRrHMRcB2w3szWBm1fB74DrDKzG4HdwMcB3H2Dma0C3iI28ucmdx8ItvsCcA+QBzwWPERExrVdLV0c6o+mxJH+cUPf3V9g6P54gPcfY5tbgVuHaK8GzjyRAkVExrote2Mjd96RAkf6uiJXRCTBNu3twAzmTQ33wixQ6IuIJNyWfR3Mnjwh1HvuDFLoi4gk2OZ9HZw+LfyjfFDoi4gkVE/fADubD4Z+Je4ghb6ISAJta+ok6nB6CpzEBYW+iEhCbWxInZE7oNAXEUmoDfVt5GVFmDNFffoiIuPehvp23lFWQCQjNaYPUeiLiCRINOpsrG9n0YzCsEs5TKEvIpIgtQe66OjtZ9GMorBLOUyhLyKSIBvqY3eh15G+iEgaeKu+nUiGpcSN1gYp9EVEEmRDfRvzSiemxO0XBin0RUQSZEOKncQFhb6ISEI0dfTS2NHLQoW+iMj4t6G+DUChLyKSDg6P3ClLneGaMLI5cu82s0YzezOu7ZtmtsfM1gaPD8a9d4uZ1ZjZZjO7Iq79PDNbH7x3WzBProjIuPT67lbmTplAUX5W2KW8zUiO9O8Blg3R/n13Xxw8HgUws4XAcmBRsM3tZjZ42voOYAWxidLnH+MzRUTGPHdnbW0riyuLwy7lKMcNfXd/DmgZ4eddBTzk7r3uvgOoAZaYWRlQ6O4vubsD9wFXn2TNIiIpraGth+bOXs4ei6E/jJvN7I2g+2dS0FYO1MatUxe0lQfLR7YPycxWmFm1mVU3NTWdQokiIsm3rrYVYFyF/h3AacBioAH4btA+VD+9D9M+JHdf6e5V7l5VWlp6kiWKiIRjbV0rWRFjQVnqXIk76KRC3933ufuAu0eBnwBLgrfqgMq4VSuA+qC9Yoh2EZFxZ11tKwvLCsnJTJ0rcQedVOgHffSDPgoMjux5BFhuZjlmNofYCds17t4AdJjZ0mDUzvXAw6dQt4hIShqIOuvr2lKyawcg83grmNmDwCXAFDOrA/4PcImZLSbWRbMT+ByAu28ws1XAW0A/cJO7DwQf9QViI4HygMeCh4jIuLKtqZODhwZScuQOjCD03f2aIZrvGmb9W4Fbh2ivBs48oepERMaYtbtbgdQ8iQu6IldEZFS9XnuAgtxM5kyeEHYpQ1Loi4iMold2HqBq1iQyUmRO3CMp9EVERknLwUPUNHZSNbsk7FKOSaEvIjJKXt11AIAlcxT6IiLjXvXOFrIjGZxVnlp31oyn0BcRGSWv7GzhnRVFKTU94pEU+iIio6Cnb4D1e9pSuj8fFPoiIqNiXW0rfQPO+bMnHX/lECn0RURGwSs7Y3egP2+WQl9EZNxbvaOF06dNpDg/O+xShqXQFxE5Rb39A7yys4V3nTYl7FKOS6EvInKK1u5upacvyrtOmxx2Kcel0BcROUV/3LafDIML5ir0RUTGvRdrmjmrvIiivKywSzkuhb6IyCk42NvP2tpW3jUv9fvzQaEvInJK1uxsoT/qY6I/H0YQ+mZ2t5k1mtmbcW0lZvaEmW0NnifFvXeLmdWY2WYzuyKu/TwzWx+8d1swbaKIyJj2Yk0z2ZEMqmal9pW4g0ZypH8PsOyItq8BT7n7fOCp4DVmthBYDiwKtrndzAZvQnEHsILYvLnzh/jMUePu/NNv3uL+l3cl6itERAB4oWY/584qJi87de+3E++4oe/uzwEtRzRfBdwbLN8LXB3X/pC797r7DqAGWBJMpF7o7i+5uwP3xW0z6vqjzo7mg/zDw2/y8No9ifoaEUlze9t62NjQzntPnxp2KSN2sn3609y9ASB4HvzF5UBt3Hp1QVt5sHxk+5DMbIWZVZtZdVNT0wkXlxXJ4PZPnsuS2SV8ddU6ntq474Q/Q0TkeJ7Z3AjApe8Y/6F/LEP10/sw7UNy95XuXuXuVaWlpSdVSG5WhDtvqGLhjEK+8MBrvLRt/0l9jojIsTy9uZEZRbmcPm1i2KWM2MmG/r6gy4bguTForwMq49arAOqD9ooh2hOqIDeLez69hFkl+Xzm3ldYV9ua6K8UkTRxqD/KC1ubueQdUxlL41JONvQfAW4Ilm8AHo5rX25mOWY2h9gJ2zVBF1CHmS0NRu1cH7dNQpVMyOanN15AycRsrr97DW/uaUvG14rIOFe9s4WDhwZ43xljp2sHRjZk80HgJeAMM6szsxuB7wCXmdlW4LLgNe6+AVgFvAU8Dtzk7gPBR30BuJPYyd1twGOj/FuOaXpRLj/7zFIm5mRy7U9eZn2dgl9ETs0fNjWSHcngonljY3z+IIsNpkldVVVVXl1dPSqfVdvSxfKVL9PR08dPb7yAsyuLR+VzRST9vP+7zzCjOI+f3nhB2KUMycxedfeqI9vT6orcypJ8HlqxlMK8LD5112rWqo9fRE5CTWMn25oO8oEF08Iu5YSlVehDLPh//rkLKc7P4ro7Vx+e7UZEZKQef7MBgCsWTQ+5khOXdqEPUF6cx89XXEhpQQ7X3bWapzc1Hn8jEZHA4xv2cs7MYqYX5YZdyglLy9AHmFGcx6rPX8i8qRP57H3V/Pp1XbkrIsdX29LFm3vaufLMsXeUD2kc+gBTJubw4GeXUjV7El/5+Vru+eOOsEsSkRT3uw17AVi2qCzkSk5OWoc+/OkCrssXTuOb//MW//L4JqLR1B7RJCLheezNvSwsK2Tm5PywSzkpaR/6ELtlw+2fPJdrlszk9me28cWHXqenb+D4G4pIWmlo6+a13QfGbNcOQGbYBaSKzEgG3/romcyZks+3H9vEngPd/OT6KkoLcsIuTURSxMNr63GHjyyeEXYpJ01H+nHMjBXvOY07Pnkem/a289Hb/8iWfR1hlyUiKeLXr+/h3JnFzJo8IexSTppCfwjLzpzOz1dcSG9/lI/d/iK/D07ciEj62tjQzqa9HXz0nGPeFX5MUOgfw9mVxTx800XMLZ3Aip++ynd/v5kBneAVSVu/fn0PmRnGh945drt2QKE/rBnFeaz63IX8ZVUl//GHGj59zyu0dh0KuywRSbKBqPPw2nouOaOUkgnZYZdzShT6x5GbFeGf/+KdfPtjZ/Hytv18+D9e0O2ZRdLMH2ua2dvew9VjvGsHFPojds2Smaz6/IUMRJ2P3f4idz6/XeP5RdLEz1bvpmRCNpctHHs3WDuSQv8ELK4s5tEvvZv3nlHK//3tRv763ldo7uwNuywRSaDG9h6e2LiPvzivgpzMSNjlnDKF/gmaNCGbldedxz9dfSYvbdvPsh88z3NbTnzydhEZG1ZV1zIQda5ZMjPsUkaFQv8kmBnXLZ3FIzdfTMmELK6/ew3/8Os3OdjbH3ZpIjKKBqLOg2tqeddpk5kzZeyOzY93SqFvZjvNbL2ZrTWz6qCtxMyeMLOtwfOkuPVvMbMaM9tsZlecavFhO2N6AY/cfDE3XjyH+1fv4oofPMeLNc1hlyUio+S5LU3sae3m2gvGx1E+jM6R/vvcfXHctFxfA55y9/nAU8FrzGwhsBxYBCwDbjezMd9BlpsV4R8+vJBVn7uQrEgG1965mm/8ej2dOuoXGfPufGE70wpzuHzh2L3XzpES0b1zFXBvsHwvcHVc+0Pu3uvuO4hNkL4kAd8fivNnl/Dol97NZy6ewwOrd3PZ957l8TcbSPU5iEVkaBvq2/hjzX7+6l1zyM4cPz3hp/pLHPi9mb1qZiuCtmnu3gAQPE8N2suB2rht64K2o5jZCjOrNrPqpqaxc5I0LzvCNz68kF98/kKK8rL4/P2v8el7XmHX/oNhlyYiJ+iu53eQnx3h2nFyAnfQqYb+Re5+LnAlcJOZvWeYdW2ItiEPg919pbtXuXtVaWnpKZaYfOfNKuE3X7yYb3xoAa/saOGy7z/Hvz+5VbdrFhkj9rb18Mi6ej5RVUlRflbY5YyqUwp9d68PnhuB/ybWXbPPzMoAgufBCWjrgMq4zSuA+lP5/lSWGcngM++ey1NfvYTLF07j+09u4QPfe5ZH1tWry0ckxd35/Hai7tx48ZywSxl1Jx36ZjbBzAoGl4HLgTeBR4AbgtVuAB4Olh8BlptZjpnNAeYDa072+8eK6UW5/PDac3ngMxdQkJvFlx58natvf5E1O1rCLk1EhtDY0cP9q3dx9TnlVJaMzdmxhnMqR/rTgBfMbB2x8P6tuz8OfAe4zMy2ApcFr3H3DcAq4C3gceAmd0+b/o6L5k3hN1+8mH/7+Nnsa+vhEz9+iRX3VVPTqPv1i6SSHz+7nb4B54uXzg+7lISwVO9qqKqq8urq6rDLGFXdhwa464Xt3PHMNrr6BvjI2TP44qXzmTd1YtiliaS1xvYe3v0vT/Phd87gu584O+xyTomZvRo3lP4wTZcYgrzsCDdfOp9rlsxk5fPbue/FXfzPuno+cvYMvvT++cwtVfiLhOFHT9fQH3W+9P55YZeSMONn8OkYNHliDrdcuYAX/u59fPbdc/ndhn184HvPctPPXuONutawyxNJKzWNndy/ejd/eX7lmJ4O8Xh0pJ8CJk/M4ZYPLuCz75nLT57fzs9e3s1v32jggjklrHjPXN53xlQyMoYa8Soio+Xbj24kPyvC31x2etilJJSO9FPIlODI/8VbLuUbH1pAbUsXN95bzeU/eI77X96lWzuIJMgLW5t5alMjN106jykTc8IuJ6F0IjeF9Q1EeXR9Ayuf286G+nYmZEe4+pxyPrV0FgvKCsMuT2RcONQf5UO3PU933wBP/s17yc0a87cEA3Qid0zKimRw1eJyPnL2DF6vbeX+l3fx/16t44HVuzl3ZjHXLJnJlWeVMTFH/xpFTtZ/PruNrY2d/NdfnT9uAn84OtIfY1q7DvGLIPh3NB8kNyuDKxZN52PnVnDxvClE1PcvMmLbmjq58gfPc/miafzw2nPDLmdUHetIX6E/Rrk7r+0+wC9f28Nv1tXT3tPP1IIcrlo8gw+eVcbiymLMtAMQOZaBqHPNypfZtLedp756CaUF46svX6E/jvX2D/CHjY386vU9PLO5kb4Bp6wolysWTefKM6dTNbtEfwGIHOFHT9fwr7/bzHc/fjZ/fl5F2OWMOvXpj2M5mRGuPKuMK88qo62rjyc37uOxN/fyszW7uefFnUyZmM1lC6fxvjOm8q55U3QOQNLe67sP8L0ntvBnZ8/gY+cOeYf3cUtH+uNYZ28/T29q5PE39/LM5kYOHhogK2KcP7uES84o5ZIzpjJ/6kR1A0laaevq489++AIDUefRL7+borzxdevkQereSXOH+qNU72rh2c1NPLO5ic37Yjd6KyvK5cK5k1k6dzIXzC1hZkm+dgIybg1EnU/f8wovbWvmoRVLOW9WSdglJYxCX96mvrWbZ7c08fzWJlZvb2H/wUNAbCewdO5kLphTQtXsScydMlFXA8u48e1HN/Lj57bz7Y+dxTXjbEasI6lPX95mRnEe1yyZyTVLZuLu1DR28vL2/by8o4Xntzbx36/vAaAgN5OzK4pZXBk8ZhaP+ysWZXx6YPUufvzcdj61dOa4D/zh6EhfjuLubGvq5LXdraytbWXt7lY27+tgIBr7b6ViUh5nzihiQVkhC2cUsqCsgPLiPHULScr67RsN3Pzga7zvjKn8+LrzyIqM/zvQ6EhfRszMmDe1gHlTC/hEVWyGy+5DA6zf08a62tiO4K2Gdn731l4GjxkKcjNZMD22A1hQVshpUydyWulESiZkh/hLROAPm/bxlZ+/znkzJ/Gja89Ni8AfTtJD38yWAf8ORIA73f07ya5BTlxedoQlc0pYMudPJ74O9vazeV8HGxvag0cHv3i1joOH/jQh2qT8LE4rncjc0gmcVjrx8HLFpHyyM9P7fz5JvMfWN/DFB19nQVkhd91wPnnZ4/82C8eT1O4dM4sAW4hNo1gHvAJc4+5vHWsbde+MLdGos6e1m5qmTrY1drK9+SDbGjvZ1nSQ5s7ew+tlGJQV5VFZkkflpHxmluRTGTxmluQzZWK2uovklDy4Zjff+PWbLK4s5r8+fT6FueNzaOaxpEr3zhKgxt23B0U9BFxFbN5cGQcyMuxweL/vjKlve6+tq49tzZ1sbzrI7pYu6lq62N3SxbNbmmjs6H3bunlZEcon5VFWlMv0wlzKioPlolzKinIpK8qjMDdTOwY5Sv9AlG89uom7/7iD955eyu2fPJcJuiDxsGT/kygHauNe1wEXHLmSma0AVgDMnJm+Z9nHm6L8LM6dOYlzZ0466r2evgHqDsR2ArUt3exu6aK+tZv6th627IvtFI78ozQ/O3J4JzC9MI/SghymFuS87bm0IIeJOdo5pIvGjh6+umodz29t5q8vmsPXP/gOMtO8D/9IyQ79of7PO6p/yd1XAish1r2T6KIkfLlZkcMnj4fSNxClqaOXhrZuGtp62NvWQ0Nbz+HXL25rprmzl76Bo/9zycuKHN4BDLVTmFqQS2lBDpMnZCsgxrCnNu7jf/3iDQ729vPPf34Wf3m+DhiHkuzQrwMq415XAPVJrkHGoKxIBjOK85hRnHfMdaJRp627j8aOXpo6emnq7KGxPbY82LZlXwd/rGmmvefoWcjMoCQ/+/DOoHTin3YMU+KWSyfmUJyfpb8eUkRjRw/f+u1Gfr22ngVlhfzHNYuPefAgyQ/9V4D5ZjYH2AMsB65Ncg0yTmVkGJMmZDNpQjZnTB/+f/qevoFgx/CnnUJz3Oumjl62Nx2kqbOXQ/3Ro7bPitifdgTxO4S414Pvqz85MXr6BvjpS7u47amt9PZH+eKl87j50nnkZGqEznCS+l+ju/eb2c3A74gN2bzb3TckswYRiHUnDZ5wHo67097Tf3hH0NR59M6hoa2HN/a0sb+zl+gQnZH52ZGjdw5H7igKcpg8IUfDWEegp2+Ah9bs5kfPbKOpo5f3nF7KP35kEXOmTAi7tDEh6Ycg7v4o8Giyv1fkZJgZRXlZFOVlMW/qxGHXHYg6LQcPHXPn0NTRy9bGTl7ctp+27r4hP6M4P+uoncPUwth5h6kFwXJhLgVpeHJ6Z/PBw1OGtnX3ccGcEn54zTlcMHdy2KWNKfq7U2SURDLscFgfT2//AM2dh2I7hiF2Dk2dvby+u5XGjh56+o7uXsrNynj7jqAg9+idQ0Euk8b4uYfali4eXd/Ao+sbWFfXRmaGccWZ07l+6SyF/UlS6IuEICczQnlxHuXDnJiGWPdSR28/je29NHb0xM4/BMuNwfKmvR08v6WZjt6jT05nRSz4a+GIHcQRO4vJE3JCn10tGnV2t3Sxrq6Vl7e38NK2Znbu7wLgrPIi/m7ZO/jzc8uZWpgbap1jnUJfJIWZGYW5WRTmHr97qfvQwNt2BoPL+9pjO4td+7t4ZWcLB7qO7lrKMJhyZFdSQQ5TCnJi35+XSWFuFgVxy/nZkRP6K+JQf5S27j7aug/RcrCPPa1d1LV0U3egm21NnWxsaD98C4+CnEwumFvCp5bO4vKF05k5efhzLzJyCn2RcSIvO8KsyROYNXn4E5qH+qM0dfbS2D64g3j7jmJfew/rhzkxPSjDIDszg+xIxuHnrMwMIhnGQNTpH3D6BqL0R52evgG64u7JFG9qQQ6zJ0/g41WVLAzu3PqO6QW6ZiJBFPoiaSY7M2NEXUv9A1EOdPXR0dNHe09/7Lm7n/aePtq7++jo6efQQJRD/VEODUTp64/SNxClL+pkZRiRjAyyIkZmxMjJjFCcl0VxfhZF+dlMys9iRlBDbpaGWCaTQl9EhpQZyRjxiWkZO/T3k4hIGlHoi4ikEYW+iEgaUeiLiKQRhb6ISBpR6IuIpBGFvohIGlHoi4ikEfMjJx5NMWbWBOwKu44TNAVoDruIJNNvTg/6zWPHLHcvPbIx5UN/LDKzanevCruOZNJvTg/6zWOfundERNKIQl9EJI0o9BNjZdgFhEC/OT3oN49x6tMXEUkjOtIXEUkjCn0RkTSi0E8wM/tbM3MzmxJ2LYlmZv9qZpvM7A0z+28zKw67pkQxs2VmttnMaszsa2HXk2hmVmlmT5vZRjPbYGZfDrumZDCziJm9bma/CbuW0aLQTyAzqwQuA3aHXUuSPAGc6e7vBLYAt4RcT0KYWQT4EXAlsBC4xswWhltVwvUDX3X3BcBS4KY0+M0AXwY2hl3EaFLoJ9b3gf8NpMXZcnf/vbv3By9fBirCrCeBlgA17r7d3Q8BDwFXhVxTQrl7g7u/Fix3EAvC8nCrSiwzqwA+BNwZdi2jSaGfIGb2EWCPu68Lu5aQ/DXwWNhFJEg5UBv3uo5xHoDxzGw2cA6wOuRSEu0HxA7aoiHXMao0MfopMLMngelDvPX3wNeBy5NbUeIN95vd/eFgnb8n1h3wQDJrSyIboi0t/pozs4nAL4GvuHt72PUkipl9GGh091fN7JKQyxlVCv1T4O4fGKrdzM4C5gDrzAxi3RyvmdkSd9+bxBJH3bF+8yAzuwH4MPB+H78XgdQBlXGvK4D6kGpJGjPLIhb4D7j7r8KuJ8EuAj5iZh8EcoFCM7vf3T8Vcl2nTBdnJYGZ7QSq3H0s3qlvxMxsGfA94L3u3hR2PYliZpnETlS/H9gDvAJc6+4bQi0sgSx29HIv0OLuXwm5nKQKjvT/1t0/HHIpo0J9+jKafggUAE+Y2Voz+8+wC0qE4GT1zcDviJ3QXDWeAz9wEXAdcGnw73ZtcBQsY4yO9EVE0oiO9EVE0ohCX0QkjSj0RUTSiEJfRCSNKPRFRNKIQl9EJI0o9EVE0sj/B2l2+m1Gykk4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a,b = -5.0, 5.0\n",
    "xvals = np.linspace(a,b,1000)\n",
    "yvals = list(map(lambda x: f_example(x, order=0), xvals))\n",
    "plt.plot(xvals, yvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[2.32232497]]),\n",
       " [-160.6135081652129,\n",
       "  -189.84951383040067,\n",
       "  -192.57004383001936,\n",
       "  -192.6241500150598,\n",
       "  -192.624181830695],\n",
       " [0.0,\n",
       "  0.0019943714141845703,\n",
       "  0.0019943714141845703,\n",
       "  0.0019943714141845703,\n",
       "  0.0029914379119873047],\n",
       " [array([1.5]),\n",
       "  matrix([[2.50046563]]),\n",
       "  matrix([[2.34845971]]),\n",
       "  matrix([[2.32296334]]),\n",
       "  matrix([[2.32232497]])])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newton(f_example, [1.5],  0.4, 0.9, eps=1e-5, maximum_iterations=65536, linesearch=backtracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[2.32231969]]),\n",
       " [2390458.3058391255, -192.62418182884093],\n",
       " [0.0, 0.003958940505981445],\n",
       " [array([10.]), matrix([[2.32231969]])])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newton(f_example, [10.0], 10e-5, 1e-2, 2.0, maximum_iterations=65536, linesearch=exact_line_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (1 pt) function `boyd_example_func` written in Python below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boyd_example_func(x, order=0):\n",
    "    a=np.matrix('1  3')\n",
    "    b=np.matrix('1  -3')\n",
    "    c=np.matrix('-1  0')\n",
    "    x=np.asmatrix(x)\n",
    "\n",
    "    value = np.exp(a@x-0.1)[0,0]+np.exp(b@x-0.1)[0,0]+np.exp(c@x-0.1)[0,0]\n",
    "    if order==0:\n",
    "        return value\n",
    "    elif order==1:\n",
    "        gradient = a.T*np.exp(a@x-0.1)[0,0]+b.T*np.exp(b@x-0.1)[0,0]+c.T*np.exp(c@x-0.1)[0,0]\n",
    "        return (value, gradient)\n",
    "    elif order==2:\n",
    "        gradient = a.T*np.exp(a@x-0.1)[0,0]+b.T*np.exp(b@x-0.1)[0,0]+c.T*np.exp(c@x-0.1)[0,0]\n",
    "        hessian = a.T@a*np.exp(a@x-0.1)[0,0]+b.T@b*np.exp(b@x-0.1)[0,0]+c.T@c*np.exp(c@x-0.1)[0,0]\n",
    "        return (value, gradient, hessian)\n",
    "    else:\n",
    "        raise ValueError(\"The argument \\\"order\\\" should be 0, 1 or 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[-3.46570354e-01],\n",
       "         [ 2.15728916e-06]]),\n",
       " [438995622.7416436,\n",
       "  161499312.5741365,\n",
       "  59412163.542642444,\n",
       "  21856503.10521,\n",
       "  8040560.07140453,\n",
       "  2957956.6661045603,\n",
       "  1088171.4733925308,\n",
       "  400315.93303926685,\n",
       "  147268.02857460102,\n",
       "  54176.91738987129,\n",
       "  19930.626164446126,\n",
       "  7332.14028498872,\n",
       "  2697.445089022108,\n",
       "  992.4761317380412,\n",
       "  365.3090942812051,\n",
       "  134.6653529757698,\n",
       "  49.92516804145454,\n",
       "  18.902370956267397,\n",
       "  7.6977691488430855,\n",
       "  3.850030487036647,\n",
       "  2.7462171900764103,\n",
       "  2.5662933209294745,\n",
       "  2.5592791266168398,\n",
       "  2.5592666966984137],\n",
       " [0.00099945068359375,\n",
       "  0.002958536148071289,\n",
       "  0.002958536148071289,\n",
       "  0.003957033157348633,\n",
       "  0.004955291748046875,\n",
       "  0.005982160568237305,\n",
       "  0.005982160568237305,\n",
       "  0.006981611251831055,\n",
       "  0.007978439331054688,\n",
       "  0.007978439331054688,\n",
       "  0.008978843688964844,\n",
       "  0.008978843688964844,\n",
       "  0.00994110107421875,\n",
       "  0.00994110107421875,\n",
       "  0.010973215103149414,\n",
       "  0.011967897415161133,\n",
       "  0.011967897415161133,\n",
       "  0.012932538986206055,\n",
       "  0.012932538986206055,\n",
       "  0.013959884643554688,\n",
       "  0.013959884643554688,\n",
       "  0.014959573745727539,\n",
       "  0.014959573745727539,\n",
       "  0.01595783233642578],\n",
       " [array([[5.],\n",
       "         [5.]]),\n",
       "  matrix([[5.94750977],\n",
       "          [4.35083389]]),\n",
       "  matrix([[5.83359528],\n",
       "          [4.05547142]]),\n",
       "  matrix([[5.41702461],\n",
       "          [3.86099482]]),\n",
       "  matrix([[5.1087122 ],\n",
       "          [3.63043237]]),\n",
       "  matrix([[4.76684016],\n",
       "          [3.41105637]]),\n",
       "  matrix([[4.43632831],\n",
       "          [3.18789366]]),\n",
       "  matrix([[4.10205183],\n",
       "          [2.96598582]]),\n",
       "  matrix([[3.7690326 ],\n",
       "          [2.74365891]]),\n",
       "  matrix([[3.43559456],\n",
       "          [2.52147162]]),\n",
       "  matrix([[3.10229626],\n",
       "          [2.29923784]]),\n",
       "  matrix([[2.76895174],\n",
       "          [2.0770198 ]]),\n",
       "  matrix([[2.43562401],\n",
       "          [1.85479743]]),\n",
       "  matrix([[2.10229591],\n",
       "          [1.63257999]]),\n",
       "  matrix([[1.76898776],\n",
       "          [1.41037413]]),\n",
       "  matrix([[1.43574819],\n",
       "          [1.18821455]]),\n",
       "  matrix([[1.10277093],\n",
       "          [0.96622967]]),\n",
       "  matrix([[0.77078533],\n",
       "          [0.74490595]]),\n",
       "  matrix([[0.44252321],\n",
       "          [0.52606453]]),\n",
       "  matrix([[0.12784917],\n",
       "          [0.31628184]]),\n",
       "  matrix([[-0.14202662],\n",
       "          [ 0.13636465]]),\n",
       "  matrix([[-0.30440153],\n",
       "          [ 0.0281147 ]]),\n",
       "  matrix([[-0.34477525],\n",
       "          [ 0.00119889]]),\n",
       "  matrix([[-3.46570354e-01],\n",
       "          [ 2.15728916e-06]])])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newton(boyd_example_func, [[5.0],[5.0]],  0.4, 0.9, eps=1e-5, maximum_iterations=65536, linesearch=backtracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[-0.34683887],\n",
       "         [ 0.00040362]]),\n",
       " [438995622.7416436,\n",
       "  350639.9076837685,\n",
       "  2.738081662639273,\n",
       "  2.5636590971609303,\n",
       "  2.559267724540552],\n",
       " [0.0,\n",
       "  0.006981849670410156,\n",
       "  0.011935710906982422,\n",
       "  0.016921520233154297,\n",
       "  0.021909713745117188],\n",
       " [array([[5.],\n",
       "         [5.]]),\n",
       "  matrix([[12.03929433],\n",
       "          [ 0.17717763]]),\n",
       "  matrix([[-0.41410411],\n",
       "          [ 0.17717763]]),\n",
       "  matrix([[-0.29103812],\n",
       "          [ 0.00854753]]),\n",
       "  matrix([[-0.34683887],\n",
       "          [ 0.00040362]])])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newton(boyd_example_func, [[5.0],[5.0]], 10e-5, 1e-2, 2.0,  eps=1e-5, maximum_iterations=65536, linesearch=exact_line_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (2 pts) the following Python function `quadratic` (with different types of matrix $H$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic( H, b, x, order=0 ):\n",
    "    \"\"\" \n",
    "    Quadratic Objective\n",
    "    H:          the Hessian matrix\n",
    "    b:          the vector of linear coefficients\n",
    "    x:          the current iterate\n",
    "    order:      the order of the oracle. For example, order=1 returns the value of the function and its gradient while order=2 will also return the hessian\n",
    "    \"\"\"\n",
    "    H = np.asmatrix(H)\n",
    "    b = np.asmatrix(b)\n",
    "    x = np.asmatrix(x)\n",
    "    \n",
    "    value = 0.5 * x.T @ H @ x + b.T @ x\n",
    "\n",
    "    if order == 0:\n",
    "        return value\n",
    "    elif order == 1:\n",
    "        gradient = H @ x + b\n",
    "        return (value, gradient)\n",
    "    elif order == 2:\n",
    "        gradient = H @ x + b\n",
    "        hessian = H\n",
    "        return (value, gradient, hessian)\n",
    "    else:\n",
    "        raise ValueError(\"The argument \\\"order\\\" should be 0, 1 or 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quadratic_function(H, b):\n",
    "    return lambda x, order=0: quadratic(H, b, x, order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_definite(n=4):\n",
    "    A = np.random.rand(n,n)\n",
    "    M = A@A.T\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weakly_positive_definite(n=4):\n",
    "    eigenvalues = np.random.rand(n)\n",
    "    D = np.diag(eigenvalues)\n",
    "    D[0, 0] = 0.0\n",
    "    P = ortho_group.rvs(dim=n)\n",
    "    M = P @ D @ P.T\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-bad7b0023016>:13: RuntimeWarning: overflow encountered in matmul\n",
      "  value = 0.5 * x.T @ H @ x + b.T @ x\n",
      "<ipython-input-9-bad7b0023016>:13: RuntimeWarning: invalid value encountered in matmul\n",
      "  value = 0.5 * x.T @ H @ x + b.T @ x\n",
      "<ipython-input-3-aafbb3a96a19>:26: RuntimeWarning: overflow encountered in matmul\n",
      "  return f(x + t*direction, order=1)[1].T @ direction\n",
      "<ipython-input-3-aafbb3a96a19>:26: RuntimeWarning: invalid value encountered in matmul\n",
      "  return f(x + t*direction, order=1)[1].T @ direction\n",
      "<ipython-input-5-2c5e333e8707>:39: RuntimeWarning: invalid value encountered in matmul\n",
      "  direction = - np.linalg.inv(hessian) @ gradient\n",
      "<ipython-input-5-2c5e333e8707>:42: RuntimeWarning: invalid value encountered in matmul\n",
      "  newton_decrement = gradient.T @ np.linalg.inv(hessian) @ gradient\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-6ad66c087f63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mquad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_quadratic_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mnewton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10e-5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaximum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m65536\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinesearch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexact_line_search\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-2c5e333e8707>\u001b[0m in \u001b[0;36mnewton\u001b[1;34m(func, initial_x, eps, maximum_iterations, linesearch, print_decrement, *linesearch_args)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mruntimes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mxs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m### TODO: Compute the Newton update direction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "H = weakly_positive_definite(n=4)\n",
    "x0 = np.random.rand(4, 1)\n",
    "b = (H @ x0).reshape(-1, 1)\n",
    "quad = create_quadratic_function(H, b)\n",
    "newton(quad, [[5.0],[5.0],[5.0],[5.0]], 10e-5, 1e-2, 2.0,  eps=1e-5, maximum_iterations=65536, linesearch=exact_line_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[ 0.26461157],\n",
       "         [-1.25487787],\n",
       "         [-0.82585368],\n",
       "         [ 1.46799317]]),\n",
       " [237.56430402456004, -0.38835420476505966],\n",
       " [0.0, 0.0010111331939697266],\n",
       " [array([[5.],\n",
       "         [5.],\n",
       "         [5.],\n",
       "         [5.]]),\n",
       "  matrix([[ 0.26461157],\n",
       "          [-1.25487787],\n",
       "          [-0.82585368],\n",
       "          [ 1.46799317]])])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = positive_definite(n=4)\n",
    "b = np.random.rand(4, 1)\n",
    "quad = create_quadratic_function(H, b)\n",
    "newton(quad, [[5.0],[5.0],[5.0],[5.0]], 10e-5, 1e-2, 2.0,  eps=1e-5, maximum_iterations=65536, linesearch=exact_line_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[-65.00273438],\n",
       "         [ 18.00050781]]),\n",
       " [557.5, -84.99999901962293],\n",
       " [0.0, 0.0009975433349609375],\n",
       " [array([[5.],\n",
       "         [5.]]),\n",
       "  matrix([[-65.00273438],\n",
       "          [ 18.00050781]])])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = np.asmatrix([[1,1],[3,4]]) @ np.asmatrix([[1,1],[3,4]]).T\n",
    "b = np.asmatrix([[4],[5]])\n",
    "quad = create_quadratic_function(H, b)\n",
    "newton(quad, [[5.0],[5.0]], 10e-5, 1e-2, 2.0,  eps=1e-5, maximum_iterations=65536, linesearch=exact_line_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[-65.],\n",
       "         [ 18.]]),\n",
       " [557.5, -85.0],\n",
       " [0.0, 0.0],\n",
       " [array([[5.],\n",
       "         [5.]]),\n",
       "  matrix([[-65.],\n",
       "          [ 18.]])])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = np.asmatrix([[1,1],[3,4]]) @ np.asmatrix([[1,1],[3,4]]).T\n",
    "b = np.asmatrix([[4],[5]])\n",
    "quad = lambda x, order=2: quadratic(H, b, x, order=order)\n",
    "newton(quad, [[5.0],[5.0]],  0.4, 0.9, eps=1e-5, maximum_iterations=65536, linesearch=backtracking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton method finds solutions of convex functions very fast. There was no significant difference between using exact line search and backtracking. Newton finds optimum for strictly positive definite quadratic functions but fails to do so when matrix is weakly positive definite (we may encounter problems with inversions, nans or infinities, which goes on par with theory as hessian is not invertible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark. In `newton` function you should use both `exact_line_search` (1 pt) and `backtracking` (1 pt).\n",
    "\n",
    "<img width=\"80%\" src=\"https://i.ibb.co/1fQ0Nfs/Boyd-line-search.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5.2** (total 6 pts)\n",
    "\n",
    "(2 pts) Complete the implementation of Conjugate gradients method (see [Nocedal, Wright, *Numerical Optimization*, $\\S 5.2$])\n",
    "\n",
    "<img src=\"https://i.ibb.co/Hxn9PmM/Nocedal-Wright-CG-FR.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cg_fr( func, initial_x, *linesearch_args, eps=1e-5, maximum_iterations=65536, linesearch=backtracking  ):\n",
    "    \"\"\" \n",
    "    Conjugate Gradient\n",
    "    func:               the function to optimize It is called as \"value, gradient = func( x, 1 )\n",
    "    initial_x:          the starting point\n",
    "    eps:                the maximum allowed error in the resulting stepsize t\n",
    "    maximum_iterations: the maximum allowed number of iterations\n",
    "    linesearch:         the linesearch routine\n",
    "    *linesearch_args:   the extra arguments of linesearch routine\n",
    "    \"\"\"\n",
    "    \n",
    "    global COUNT_EVALS\n",
    "    global ITERS\n",
    "    if eps <= 0:\n",
    "        raise ValueError(\"Epsilon must be positive\")\n",
    "    x = np.asarray( initial_x.copy() )\n",
    "\n",
    "    # initialization\n",
    "    values = []\n",
    "    runtimes = []\n",
    "    xs = []\n",
    "    start_time = time.time()\n",
    "    m = len( initial_x )\n",
    "    iterations = 0\n",
    "    direction = np.asmatrix( np.zeros( x.shape ) )\n",
    "\n",
    "    # conjugate gradient updates\n",
    "    while True:\n",
    "        COUNT_EVALS += 1\n",
    "        value, gradient = func( x , 1 )\n",
    "        value = np.double( value )\n",
    "        gradient = np.asarray( gradient )\n",
    "        \n",
    "        # updating the logs\n",
    "        values.append( value )\n",
    "        runtimes.append( time.time() - start_time )\n",
    "        xs.append( x.copy() )\n",
    "\n",
    "        if (gradient.T @ gradient) < eps:  \n",
    "            break\n",
    "\n",
    "        # reset after #(dimensions) iterations\n",
    "        if iterations % m == 0:\n",
    "            beta = 0\n",
    "        else:\n",
    "            beta = ((gradient.T @ gradient) / (old_gradient.T @ old_gradient))[0,0]\n",
    "\n",
    "        direction = -gradient + beta * direction\n",
    "\n",
    "        t = linesearch(func, x, direction, iterations, *linesearch_args)\n",
    "\n",
    "        x += t * direction\n",
    "        \n",
    "        old_gradient = gradient\n",
    "\n",
    "        iterations += 1\n",
    "        if iterations >= maximum_iterations:\n",
    "            raise ValueError(\"Too many iterations\")\n",
    "    ITERS = iterations\n",
    "    return (x, values, runtimes, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2.32232979]]),\n",
       " [-186.00601956150342,\n",
       "  -192.1464665653188,\n",
       "  -192.6221279402406,\n",
       "  -192.62416255562982,\n",
       "  -192.62418162773025,\n",
       "  -192.62418182859264],\n",
       " [0.0,\n",
       "  0.003988504409790039,\n",
       "  0.007013559341430664,\n",
       "  0.009007930755615234,\n",
       "  0.011005401611328125,\n",
       "  0.013962745666503906],\n",
       " [array([[2.]]),\n",
       "  array([[2.39871719]]),\n",
       "  array([[2.31718294]]),\n",
       "  array([[2.32282178]]),\n",
       "  array([[2.32227355]]),\n",
       "  array([[2.32232979]])])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cg_fr(f_example, [[2.0]], 0.4, 0.9, eps=1e-5, maximum_iterations=65536, linesearch=backtracking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, copy the function above but use the following Polak-Riberie formulae:\n",
    "$$ \\beta_{k+1}^{\\mathtt{PR}} = \\frac{\\nabla f_{k+1}^T(\\nabla f_{k+1} - \\nabla f_k)}{\\|\\nabla f_k\\|^2}$$\n",
    "\n",
    "Observe that we applied the reset trick.\n",
    "It is worth reading more implementation hints in section [Nocedal, Wright, *Numerical Optimization*, $\\S 5.2$]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cg_pr( func, initial_x, *linesearch_args, eps=1e-5, maximum_iterations=65536, linesearch=backtracking  ):\n",
    "    \"\"\" \n",
    "    Conjugate Gradient\n",
    "    func:               the function to optimize It is called as \"value, gradient = func( x, 1 )\n",
    "    initial_x:          the starting point\n",
    "    eps:                the maximum allowed error in the resulting stepsize t\n",
    "    maximum_iterations: the maximum allowed number of iterations\n",
    "    linesearch:         the linesearch routine\n",
    "    *linesearch_args:   the extra arguments of linesearch routine\n",
    "    \"\"\"\n",
    "    \n",
    "    global COUNT_EVALS\n",
    "    global ITERS\n",
    "    if eps <= 0:\n",
    "        raise ValueError(\"Epsilon must be positive\")\n",
    "    x = np.asarray( initial_x.copy() )\n",
    "\n",
    "    # initialization\n",
    "    values = []\n",
    "    runtimes = []\n",
    "    xs = []\n",
    "    start_time = time.time()\n",
    "    m = len( initial_x )\n",
    "    iterations = 0\n",
    "    direction = np.asmatrix( np.zeros( x.shape ) )\n",
    "\n",
    "    # conjugate gradient updates\n",
    "    while True:\n",
    "        COUNT_EVALS += 1\n",
    "        value, gradient = func( x , 1 )\n",
    "        value = np.double( value )\n",
    "        gradient = np.asarray( gradient )\n",
    "\n",
    "        # updating the logs\n",
    "        values.append( value )\n",
    "        runtimes.append( time.time() - start_time )\n",
    "        xs.append( x.copy() )\n",
    "\n",
    "        if (gradient.T @ gradient) < eps:\n",
    "            break\n",
    "\n",
    "        # reset after #(dimensions) iterations\n",
    "        if iterations % m == 0:\n",
    "            beta = 0\n",
    "        else:\n",
    "            beta = ((gradient.T @ (gradient - old_gradient)) / (old_gradient.T @ old_gradient))[0,0]\n",
    "\n",
    "        direction = -gradient + beta * direction\n",
    "\n",
    "        t = linesearch(func, x, direction, iterations, *linesearch_args)\n",
    "\n",
    "        x += t * direction\n",
    "        \n",
    "        old_gradient = gradient\n",
    "        old_value = value\n",
    "\n",
    "        iterations += 1\n",
    "        if iterations >= maximum_iterations:\n",
    "            raise ValueError(\"Too many iterations\")\n",
    "    \n",
    "    ITERS = iterations\n",
    "    return (x, values, runtimes, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2.32232979]]),\n",
       " [-186.00601956150342,\n",
       "  -192.1464665653188,\n",
       "  -192.6221279402406,\n",
       "  -192.62416255562982,\n",
       "  -192.62418162773025,\n",
       "  -192.62418182859264],\n",
       " [0.0,\n",
       "  0.0019981861114501953,\n",
       "  0.0059850215911865234,\n",
       "  0.008975982666015625,\n",
       "  0.011969327926635742,\n",
       "  0.013965368270874023],\n",
       " [array([[2.]]),\n",
       "  array([[2.39871719]]),\n",
       "  array([[2.31718294]]),\n",
       "  array([[2.32282178]]),\n",
       "  array([[2.32227355]]),\n",
       "  array([[2.32232979]])])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cg_pr(f_example, [[2.0]],  0.4, 0.9, eps=1e-5, maximum_iterations=65536, linesearch=backtracking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(4 pts) Compare the efficiency (number of function/gradient evaluations) of FR and PR updates in CG method for Powell's optimization problem (PSF):\n",
    "$$ \\min_{-10 \\leq x_i \\leq 10} (x_1+10x_2)^2+5(x_3-x_4)^2+(x_2-2x_3)^4 + 10(x_1-x_4)^4,$$\n",
    "\n",
    "\n",
    "Observe that $f(X^*)=0$ for $X^*=0$.\n",
    "\n",
    "More info about PSF can be found, for example, here http://www.optimization-online.org/DB_FILE/2012/03/3382.pdf.\n",
    "\n",
    "Compare your results with [Nocedal, Wright, *Numerical Optimization*, Table 5.1] (row XPOWELL)\n",
    "<img width=50% src=\"https://i.ibb.co/6PVGJrS/Table51.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psf(x, order=1):\n",
    "    x = np.array(x)\n",
    "    x1,x2,x3,x4 = x[:,0]\n",
    "    value = (x1 + 10*x2)**2 + 5*(x3 - x4)**2 + (x2 - 2*x3)**4 + 10*(x1 - x4)**4\n",
    "    if order==0:\n",
    "        return value\n",
    "    elif order==1:\n",
    "        d1 = 2*(x1 + 10*x2) + 40*(x1 - x4)**3\n",
    "        d2 = 20*(x1 + 10*x2) + 4*(x2 - 2*x3)**3\n",
    "        d3 = 10*(x3 - x4) - 8*(x2 - 2*x3)**3\n",
    "        d4 = -10*(x3 - x4) - 40*(x1 - x4)**3\n",
    "        gradient = np.array([[d1], [d2], [d3], [d4]])\n",
    "        return value, gradient\n",
    "    else:\n",
    "        raise ValueError(\"The argument \\\"order\\\" should be 0 or 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_performance(n_trials=20):\n",
    "    global COUNT_EVALS\n",
    "    global ITERS\n",
    "    results = pd.DataFrame({\"Algorithm\": [], \"trial\": [], \"iterations\": [], \"evaluations\": []})\n",
    "    for i in range(n_trials):\n",
    "        startx = np.random.rand(4, 1)*10\n",
    "        COUNT_EVALS = 0\n",
    "        ITERS = 0\n",
    "        _ = cg_fr(psf, startx, 10e-5, 1e-2, 2.0,  eps=1e-5, maximum_iterations=65536, linesearch=exact_line_search)\n",
    "        results = results.append({\"Algorithm\": \"FR\", \"trial\": i, \"iterations\": ITERS, \"evaluations\": COUNT_EVALS},\n",
    "                                ignore_index=True)\n",
    "        \n",
    "        COUNT_EVALS = 0\n",
    "        ITERS = 0\n",
    "        _ = cg_pr(psf, startx, 10e-5, 1e-2, 2.0,  eps=1e-5, maximum_iterations=65536, linesearch=exact_line_search)\n",
    "        results = results.append({\"Algorithm\": \"PR\", \"trial\": i, \"iterations\": ITERS, \"evaluations\": COUNT_EVALS},\n",
    "                                ignore_index=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>iterations</th>\n",
       "      <th>evaluations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th>FR</th>\n",
       "      <td>26.0</td>\n",
       "      <td>365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR</th>\n",
       "      <td>38.0</td>\n",
       "      <td>515.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1.0</th>\n",
       "      <th>FR</th>\n",
       "      <td>40.0</td>\n",
       "      <td>575.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR</th>\n",
       "      <td>46.0</td>\n",
       "      <td>643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <th>FR</th>\n",
       "      <td>42.0</td>\n",
       "      <td>603.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97.0</th>\n",
       "      <th>PR</th>\n",
       "      <td>46.0</td>\n",
       "      <td>647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">98.0</th>\n",
       "      <th>FR</th>\n",
       "      <td>26.0</td>\n",
       "      <td>363.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR</th>\n",
       "      <td>40.0</td>\n",
       "      <td>583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">99.0</th>\n",
       "      <th>FR</th>\n",
       "      <td>25.0</td>\n",
       "      <td>358.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR</th>\n",
       "      <td>21.0</td>\n",
       "      <td>296.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 iterations  evaluations\n",
       "trial Algorithm                         \n",
       "0.0   FR               26.0        365.0\n",
       "      PR               38.0        515.0\n",
       "1.0   FR               40.0        575.0\n",
       "      PR               46.0        643.0\n",
       "2.0   FR               42.0        603.0\n",
       "...                     ...          ...\n",
       "97.0  PR               46.0        647.0\n",
       "98.0  FR               26.0        363.0\n",
       "      PR               40.0        583.0\n",
       "99.0  FR               25.0        358.0\n",
       "      PR               21.0        296.0\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_trials = 100\n",
    "df = compare_performance(n_trials=n_trials).set_index([\"trial\", \"Algorithm\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iterations</th>\n",
       "      <th>evaluations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algorithm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FR</th>\n",
       "      <td>35.60</td>\n",
       "      <td>502.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR</th>\n",
       "      <td>33.75</td>\n",
       "      <td>477.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           iterations  evaluations\n",
       "Algorithm                         \n",
       "FR              35.60       502.16\n",
       "PR              33.75       477.83"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(level=[1]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that PR version of CG on average requires a bit less iterations and function evaluations than FR version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5.3 (2 pts)**\n",
    "Show experimentally that affine invariance of Newton's method. \n",
    "\n",
    "Let $f:\\mathbb{R}^n\\to\\mathbb{R}$ be a convex function.\n",
    "Consider an affine transform $y\\mapsto Ay + b$, where $A \\in \\mathbb{R}^{n\\times n}$ is invertible and\n",
    "$b \\in \\mathbb R^n$.\n",
    "\n",
    "Define the function $g : \\mathbb R^n \\mapsto \\mathbb{R}$ by $g(y) = f(Ay + b)$.\n",
    "Denote by $x^{(k)}$ the k-th iterate of Newton‚Äôs method performed on $f$.\n",
    "Denote by $y^{(k)}$ the k-th iterate of Newton‚Äôs method performed on $g$.\n",
    "* Show that if $x^{(k)} = Ay^{(k)} + b$, then $x^{(k+1)} = Ay^{(k+1)} + b$.\n",
    "* Show that Newton's decrement does not depend on the coordinates, i.e., show that $Œª(x^{(k)}) = Œª(y^{(k)} ).$\n",
    "\n",
    "Together, this implies that Newton‚Äôs method is affine invariant. As an important consequence,\n",
    "Newton‚Äôs method cannot be improved by a change of coordinates, unlike gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Newton's decrement of f:\n",
      "[[3636996.24433504]]\n",
      "[[3451.02325969]]\n",
      "[[0.12596374]]\n",
      "[[0.00132684]]\n",
      "[[2.51737189e-08]]\n",
      "\n",
      "Newton's decrement of g:\n",
      "[[3636997.01281699]]\n",
      "[[3451.03206193]]\n",
      "[[0.12596359]]\n",
      "[[0.00132687]]\n",
      "[[2.5179149e-08]]\n",
      "\n",
      "xs:\n",
      "[array([[4.29672631],\n",
      "       [3.63664746]]), matrix([[7.505193 ],\n",
      "        [0.1042028]]), matrix([[-0.37062481],\n",
      "        [ 0.10420254]]), matrix([[-0.32428902],\n",
      "        [ 0.00220532]]), matrix([[-3.46592892e-01],\n",
      "        [ 4.58590938e-05]])]\n",
      "mapped ys (that is Ay+b):\n",
      "[array([[4.29672631],\n",
      "       [3.63664746]]), array([[7.50519549],\n",
      "       [0.10420286]]), array([[-0.37062232],\n",
      "       [ 0.10420261]]), array([[-0.3242889 ],\n",
      "       [ 0.00220557]]), array([[-3.46592889e-01],\n",
      "       [ 4.58644494e-05]])]\n"
     ]
    }
   ],
   "source": [
    "def transform_f(f, A, b):\n",
    "    def g(y, order=0):\n",
    "        if order==0:\n",
    "            return f(A@y+b, order=0)\n",
    "        elif order==1:\n",
    "            value, gradient = f(A@y+b, order=1)\n",
    "            gradient = A.T @ gradient\n",
    "            return (value, gradient)\n",
    "        elif order==2:\n",
    "            value, gradient, hessian = f(A@y+b, order=2)\n",
    "            gradient = A.T @ gradient\n",
    "            hessian = A.T @ hessian @ A\n",
    "            return (value, gradient, hessian)\n",
    "        else:\n",
    "            raise ValueError(\"The argument \\\"order\\\" should be 0, 1 or 2\")\n",
    "    return g\n",
    "\n",
    "def experiment(f, A, b, y0):\n",
    "    x0 = A @ y0 + b\n",
    "    g = transform_f(f, A, b)\n",
    "    print(\"\\nNewton's decrement of f:\")\n",
    "    x, x_values, x_runtimes, xs = newton(f, x0,  10e-5, 1e-2, 2.0,  eps=1e-5, \n",
    "                                         maximum_iterations=65536, linesearch=exact_line_search,\n",
    "                                         print_decrement=True)\n",
    "    print(\"\\nNewton's decrement of g:\")\n",
    "    y, y_values, y_runtimes, ys = newton(g, y0,  10e-5, 1e-2, 2.0,  eps=1e-5, \n",
    "                                         maximum_iterations=65536, linesearch=exact_line_search,\n",
    "                                         print_decrement=True)\n",
    "    return xs, ys\n",
    "\n",
    "y0 = np.array([[3.0], [2.0]])\n",
    "A = np.random.rand(2, 2)\n",
    "b = np.random.rand(2, 1)\n",
    "xs, ys = experiment(boyd_example_func, A, b, y0)\n",
    "mapped_ys = list(map(lambda x: A@np.array(x)+b, ys))\n",
    "print(f\"\\nxs:\\n{xs}\\nmapped ys (that is Ay+b):\\n{mapped_ys}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that obtained xs and mapped ys are almost equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5.4 (2 pts)**\n",
    "Show experimentally that conjugate gradient method is *not* affine invariant.\n",
    "\n",
    "\n",
    "For example consider the quadratic (convex) function $f:\\mathbb R^n \\to \\mathbb R$ as follows\n",
    "$$ f(x) = \\frac12 x^T H x - c^T x,$$\n",
    "where $H$ positive semi-definite.\n",
    "\n",
    "Consider an affine transformation $y\\mapsto Ay$, where  $A \\in \\mathbb{R}^{n\\times n}$ is invertible:\n",
    "* Denote by $x^{(0)} , x^{(1)} , x^{(2)}$ the first three iterates of conjugate gradient descent on $f(x)$ initialized at $x^{(0)}$.\n",
    "* Now, let $y^{(0)}$ be the point such that $x^{(0)} = Ay^{(0)}$. Denote by $y^{(0)} , y^{(1)} , y^{(2)}$ the first three iterates of conjugate gradient descent on $g(y) = f(Ay)$ initialized at $y^{(0)}$.\n",
    "* Provide an explicit example of $H, A$ and $x^{(0)}$ such that $x^{(1)} \\neq Ay^{(1)}$ and $x^{(2)} \\neq Ay^{(2)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "xs:\n",
      "[array([[11.5],\n",
      "       [10. ]]), array([[-4.10091668],\n",
      "       [ 1.32656618]]), array([[-1.00247217],\n",
      "       [ 2.8900455 ]])]\n",
      "mapped ys (that is Ay+b):\n",
      "[array([[11.5],\n",
      "       [10. ]]), array([[-0.98025973],\n",
      "       [-3.32415322]]), array([[ 0.90706505],\n",
      "       [-1.30900059]])]\n"
     ]
    }
   ],
   "source": [
    "def experiment2(A, b, y0, H=None):\n",
    "    if H is None:\n",
    "        H = positive_definite(y0.shape[0])\n",
    "    f = create_quadratic_function(H, b)\n",
    "    x0 = A @ y0\n",
    "    g = transform_f(f, A, np.zeros((A.shape[0], 1)))\n",
    "    x, x_values, x_runtimes, xs = cg_fr(f, x0, 0.4, 0.9, eps=1e-5, maximum_iterations=65536, linesearch=backtracking)\n",
    "    y, y_values, y_runtimes, ys = cg_fr(g, y0, 0.4, 0.9, eps=1e-5, maximum_iterations=65536, linesearch=backtracking)\n",
    "    xs, ys = xs[:3], ys[:3]\n",
    "    return xs, ys\n",
    "\n",
    "# n = 2\n",
    "# y0 = np.random.rand(n, 1)\n",
    "# A = np.random.rand(n, n)\n",
    "# b = np.random.rand(n, 1)\n",
    "y0 = np.array([[2.5], [1.0]])\n",
    "A = np.array([[3.0, 4.0], [2.0, 5.0]])\n",
    "b = np.array([[1.0], [2.0]])\n",
    "# H = np.array([[4.0, 3.0], [5.0, 2.0]]).T @ np.array([[4.0, 3.0], [5.0, 2.0]])\n",
    "H = np.array([[41.0, 22.0], [22.0, 13.0]])\n",
    "xs, ys = experiment2(A, b, y0, H=H)\n",
    "mapped_ys = list(map(lambda x: A@np.array(x), ys))\n",
    "print(f\"\\nxs:\\n{xs}\\nmapped ys (that is Ay+b):\\n{mapped_ys}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that xs and mapped ys are very different (except x0 and mapped y0 which by assumption are the same)."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "176.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
