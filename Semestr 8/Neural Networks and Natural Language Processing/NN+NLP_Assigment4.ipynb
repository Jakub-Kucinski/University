{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49DRaM7y1QxB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tqdm.notebook import tqdm\n",
        "from gensim.models import KeyedVectors\n",
        "import re\n",
        "from itertools import permutations\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "import gensim"
      ],
      "id": "49DRaM7y1QxB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETFQc3-p1QxE"
      },
      "outputs": [],
      "source": [
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "\n",
        "class EpochLogger(CallbackAny2Vec):\n",
        "    '''Callback to log information about training'''\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "    def on_epoch_begin(self, model):\n",
        "        print(\"Epoch #{} start\".format(self.epoch))\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "        print(\"Epoch #{} end\".format(self.epoch))\n",
        "        self.epoch += 1\n",
        "        \n",
        "epoch_logger = EpochLogger()"
      ],
      "id": "ETFQc3-p1QxE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUU07uUy1QxF"
      },
      "source": [
        "### Assigment 4\n",
        "\n",
        "**Submission deadlines**:\n",
        "\n",
        "* get at least 4 points by Tuesday, 12.05.2022\n",
        "* remaining points: last lab session before or on Tuesday, 19.05.2022\n",
        "\n",
        "**Points:** Aim to get 12 out of 15+ possible points\n",
        "\n",
        "All needed data files are on Drive: <https://drive.google.com/drive/folders/1HaMbhzaBxxNa_z_QJXSDCbv5VddmhVVZ?usp=sharing> (or will be soon :) )\n",
        "\n",
        "# Task 1 (5 points)\n",
        "\n",
        "Implement simplified word2vec with negative sampling from scratch (using pure numpy). Assume that in the training data objects and contexts are given explicitly, one pair per line, and objects are on the left. The result of the training should be object vectors. Please, write them to a file using *natural* text format, ie\n",
        "\n",
        "<pre>\n",
        "K N\n",
        "word1 x1_1 x1_2 ... x1_N \n",
        "word2 x2_1 x2_2 ... x2_N\n",
        "...\n",
        "wordK xK_1 xK_2 ... xk_N\n",
        "</pre>\n",
        "\n",
        "Use the loss from Slide 3 in Lecture NLP.2, compute the gradient manually. You can use some gradient clipping, or regularisation. \n",
        "\n",
        "**Remark**: the data is specially prepared to make the learning process easier. \n",
        "Present vectors using the code below. In this task we define success as 'obtaining a result which looks definitely not random'\n"
      ],
      "id": "DUU07uUy1QxF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0ZSEe6w1QxG"
      },
      "source": [
        "Usuwać pozytywny indeks z indeksów w negative samplingu. lr=0.1 bez zmniejszania lr. Sprawdzać loss. neg=3, u^3/4, dim=20. Można porównać gradienty z pytorchem."
      ],
      "id": "l0ZSEe6w1QxG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bBDpmPB1QxH"
      },
      "outputs": [],
      "source": [
        "def create_words_dicts(object_context_filename=\"task1_objects_contexts_polish.txt\"):\n",
        "    target_to_id = dict()\n",
        "    context_to_id = dict()\n",
        "    context_counts = defaultdict(int)\n",
        "    target_contex_pairs = []\n",
        "    total_contex_counts = 0\n",
        "    with open(object_context_filename, \"r\", encoding=\"utf8\") as object_context_file:\n",
        "        i_target = 0\n",
        "        i_context = 0\n",
        "        for line in tqdm(object_context_file):\n",
        "            line = line.strip()\n",
        "            target, context = line.split()\n",
        "            \n",
        "            if target not in target_to_id:\n",
        "                target_to_id[target] = i_target\n",
        "                i_target += 1\n",
        "                \n",
        "            if context not in context_to_id:\n",
        "                context_to_id[context] = i_context\n",
        "                i_context += 1\n",
        "            \n",
        "            context_counts[context] += 1\n",
        "            total_contex_counts += 1\n",
        "            \n",
        "            target_contex_pairs.append((target_to_id[target], context_to_id[context]))\n",
        "    return target_to_id, context_to_id, context_counts, total_contex_counts, target_contex_pairs\n",
        "\n",
        "def create_unigram_distribution(context_counts, total_contex_counts, context_to_id, power=0.75):\n",
        "    unigram_distribution = dict()\n",
        "    for k, v in context_counts.items():\n",
        "        unigram_distribution[k] = (v / total_contex_counts) ** power\n",
        "    Z = sum(unigram_distribution.values())\n",
        "    for k, v in unigram_distribution.items():\n",
        "        unigram_distribution[k] = v /Z\n",
        "    index_to_prob = dict()\n",
        "    for k, v in unigram_distribution.items():\n",
        "        index_to_prob[context_to_id[k]] = v\n",
        "    return np.array(list(index_to_prob.keys())), np.array(list(index_to_prob.values()))"
      ],
      "id": "9bBDpmPB1QxH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "0ae5acb14bcd4e3d84db9e43d2b32f26"
          ]
        },
        "id": "amrG0QNt1QxJ",
        "outputId": "3d74300a-a01d-4241-b8f6-a254fdf7ca2b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ae5acb14bcd4e3d84db9e43d2b32f26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "target_to_id, context_to_id, context_counts, total_contex_counts, target_contex_pairs = create_words_dicts(\n",
        "    object_context_filename=\"task1_objects_contexts_polish.txt\")\n",
        "unigram_indices, unigram_probs = create_unigram_distribution(\n",
        "    context_counts, total_contex_counts, context_to_id, power=0.75)"
      ],
      "id": "amrG0QNt1QxJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWYAtVjQ1QxJ"
      },
      "outputs": [],
      "source": [
        "class Word2Vec():\n",
        "    def __init__(self, target_to_id, context_to_id, unigram_distribution, target_contex_pairs,\n",
        "                 object_context_filename=\"task1_objects_contexts_polish.txt\"):\n",
        "        self.target_to_id = target_to_id\n",
        "        self.context_to_id = context_to_id\n",
        "        self.n_targets = len(target_to_id)\n",
        "        self.n_context = len(context_to_id)\n",
        "        self.unigram_indices = unigram_distribution[0]\n",
        "        self.unigram_probs = unigram_distribution[1]\n",
        "        self.cumulative = [self.unigram_probs[0]]\n",
        "        for pvalue in self.unigram_probs[1:]:\n",
        "            self.cumulative.append(self.cumulative[-1] + pvalue)\n",
        "        self.no_pvalues = len(self.unigram_probs)\n",
        "        self.target_contex_pairs = target_contex_pairs\n",
        "        self.object_context_filename = object_context_filename\n",
        "\n",
        "    def initialize_vectors(self, dim=312):\n",
        "        self.dim = dim\n",
        "        self.V = np.random.uniform(-0.5, 0.5, (self.n_targets, dim)) / dim\n",
        "        self.U = np.random.uniform(-0.5, 0.5, (self.n_context, dim)) / dim\n",
        "        \n",
        "    def read_weights_from_files(self, target_filename, context_filename):\n",
        "        pass\n",
        "    \n",
        "    def save_weights_to_files(self, target_filename, contex_filename):\n",
        "        with open(target_filename, \"w\", encoding=\"utf8\")as target_file:\n",
        "            target_file.write(f\"{self.n_targets} {self.dim}\\n\")\n",
        "            for word, word_id in self.target_to_id.items():\n",
        "                target_file.write(word)\n",
        "                for i in self.V[word_id]:\n",
        "                    target_file.write(\" \" + str(i))\n",
        "                target_file.write(\"\\n\")\n",
        "        with open(contex_filename, \"w\", encoding=\"utf8\")as context_file:\n",
        "            context_file.write(f\"{self.n_context} {self.dim}\\n\")\n",
        "            for word, word_id in self.context_to_id.items():\n",
        "                context_file.write(word)\n",
        "                for i in self.U[word_id]:\n",
        "                    context_file.write(\" \" + str(i))\n",
        "                context_file.write(\"\\n\")\n",
        "    \n",
        "    def sigmoid(self, v):\n",
        "        return 1/(1+np.exp(-v))\n",
        "    \n",
        "#     def choice(self):\n",
        "#         options = self.unigram_indices\n",
        "#         probs = self.unigram_probs\n",
        "#         x = np.random.rand()\n",
        "#         cum = 0\n",
        "#         for i,p in enumerate(probs):\n",
        "#             cum += p\n",
        "#             if x < cum:\n",
        "#                 break\n",
        "#         return options[i]\n",
        "    \n",
        "    def choice(self):\n",
        "        x = np.random.rand()\n",
        "        l = 0\n",
        "        r = self.no_pvalues\n",
        "        while l < r:\n",
        "            mid = (l + r) // 2\n",
        "            if self.cumulative[mid] < x:\n",
        "                l = mid\n",
        "            elif self.cumulative[mid] == x or (mid >= 1 and self.cumulative[mid - 1] < x):\n",
        "                return mid\n",
        "            else:\n",
        "                r = mid\n",
        "        return l\n",
        "    \n",
        "#     def multidimensional_shifting(self, num_samples, sample_size):\n",
        "#         # replicate probabilities as many times as `num_samples`\n",
        "#         elements = self.unigram_indices\n",
        "#         probabilities = self.unigram_probs\n",
        "#         replicated_probabilities = np.tile(probabilities, (num_samples, 1))    # get random shifting numbers & scale them correctly\n",
        "#         random_shifts = np.random.random(replicated_probabilities.shape)\n",
        "#         random_shifts /= random_shifts.sum(axis=1)[:, np.newaxis]    # shift by numbers & find largest (by finding the smallest of the negative)\n",
        "#         shifted_probabilities = random_shifts - replicated_probabilities\n",
        "#         return np.argpartition(shifted_probabilities, sample_size, axis=1)[:, :sample_size]\n",
        "\n",
        "    \n",
        "    def train(self, neg_samples=10, lr=0.003, epochs=6, const_lr=True):\n",
        "        n_of_pairs = len(self.target_contex_pairs)\n",
        "        if not const_lr:\n",
        "            lr_decrease = lr / (n_of_pairs * epochs)\n",
        "        else:\n",
        "            lr_decrease = 0\n",
        "        for i_epoch in tqdm(range(epochs)):\n",
        "            order = np.arange(n_of_pairs)\n",
        "            np.random.shuffle(order)\n",
        "            sampling_frequency = 10000\n",
        "            for i, pair_idx in enumerate(tqdm(order, leave=False)):\n",
        "                target_id, context_id = self.target_contex_pairs[pair_idx]\n",
        "                \n",
        "                if i % sampling_frequency == 0:\n",
        "                    many_negative_ids = np.random.choice(self.unigram_indices, size=(sampling_frequency, neg_samples),\n",
        "                                                         p=self.unigram_probs)\n",
        "                negative_ids = many_negative_ids[i%sampling_frequency]\n",
        "#                 negative_ids = np.random.choice(self.unigram_indices, size=neg_samples, p=self.unigram_probs)\n",
        "#                 negative_ids = [self.choice() for i in range(neg_samples)]\n",
        "                v = self.V[target_id]\n",
        "                pos_u = self.U[context_id]\n",
        "                neg_u = self.U[negative_ids]\n",
        "                \n",
        "                grad_v = (self.sigmoid(pos_u @ v) - 1) * pos_u + (\n",
        "                    self.sigmoid(neg_u @ v).reshape(-1, 1) * neg_u).sum(axis=0)\n",
        "                grad_pos_u = (self.sigmoid(pos_u @ v) - 1) * v\n",
        "#                 grad_neg_u = self.sigmoid(neg_u @ v).reshape(-1, 1) @ v.reshape(1, -1)\n",
        "                grad_neg_u = np.outer(self.sigmoid(neg_u @ v), v)\n",
        "                \n",
        "                self.V[target_id] -= lr * grad_v\n",
        "                self.U[context_id] -= lr * grad_pos_u\n",
        "                self.U[negative_ids] -= lr * grad_neg_u\n",
        "                lr -= lr_decrease\n",
        "        "
      ],
      "id": "AWYAtVjQ1QxJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bQc2L2P1QxK"
      },
      "outputs": [],
      "source": [
        "def multidimensional_shifting(num_samples, sample_size, elements, probabilities):\n",
        "    # replicate probabilities as many times as `num_samples`\n",
        "    replicated_probabilities = np.tile(probabilities, (num_samples, 1))    # get random shifting numbers & scale them correctly\n",
        "    random_shifts = np.random.random(replicated_probabilities.shape)\n",
        "    random_shifts /= random_shifts.sum(axis=1)[:, np.newaxis]    # shift by numbers & find largest (by finding the smallest of the negative)\n",
        "    shifted_probabilities = random_shifts - replicated_probabilities\n",
        "    return np.argpartition(shifted_probabilities, sample_size, axis=1)[:, :sample_size]"
      ],
      "id": "1bQc2L2P1QxK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raPyIUB-1QxL",
        "outputId": "16bd2e26-7a7f-4d99-9cff-54d0d432b5d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "88.2 ms ± 1.67 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit np.random.choice(unigram_indices, size=(100000, 5), p=unigram_probs)"
      ],
      "id": "raPyIUB-1QxL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7_--6Ij1QxM",
        "outputId": "db20b4d4-e1d8-4827-8996-e32cbf6e527d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([24301, 42289, 62610, 69562, 98935])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.random.choice(unigram_indices, size=(10, 5), p=unigram_probs)[0]"
      ],
      "id": "F7_--6Ij1QxM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4wlT1Ey1QxM"
      },
      "outputs": [],
      "source": [
        "w2v = Word2Vec(target_to_id, context_to_id, (unigram_indices, unigram_probs), target_contex_pairs)"
      ],
      "id": "T4wlT1Ey1QxM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "zghKKjvj1QxN"
      },
      "outputs": [],
      "source": [
        "w2v.initialize_vectors(dim=128)\n",
        "w2v.train(neg_samples=3, lr=0.003, epochs=6)"
      ],
      "id": "zghKKjvj1QxN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TV8srva-1QxN"
      },
      "outputs": [],
      "source": [
        "w2v = Word2Vec(target_to_id, context_to_id, (unigram_indices, unigram_probs), target_contex_pairs)\n",
        "w2v.initialize_vectors(dim=128)\n",
        "w2v.train(neg_samples=5, lr=0.003, epochs=6)"
      ],
      "id": "TV8srva-1QxN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI_42hzN1QxO"
      },
      "outputs": [],
      "source": [
        "w2v = Word2Vec(target_to_id, context_to_id, (unigram_indices, unigram_probs), target_contex_pairs)\n",
        "w2v.initialize_vectors(dim=128)\n",
        "w2v.train(neg_samples=5, lr=0.003, epochs=6)"
      ],
      "id": "vI_42hzN1QxO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLSxPxvk1QxO"
      },
      "outputs": [],
      "source": [
        "w2v = Word2Vec(target_to_id, context_to_id, (unigram_indices, unigram_probs), target_contex_pairs)\n",
        "w2v.initialize_vectors(dim=128)\n",
        "w2v.train(neg_samples=20, lr=0.003, epochs=6)"
      ],
      "id": "FLSxPxvk1QxO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKLxKaRK1QxP"
      },
      "outputs": [],
      "source": [
        "w2v = Word2Vec(target_to_id, context_to_id, (unigram_indices, unigram_probs), target_contex_pairs)\n",
        "w2v.initialize_vectors(dim=20)\n",
        "w2v.train(neg_samples=3, lr=0.1, epochs=6)"
      ],
      "id": "vKLxKaRK1QxP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iphVXCt-1QxP"
      },
      "outputs": [],
      "source": [
        "w2v = Word2Vec(target_to_id, context_to_id, (unigram_indices, unigram_probs), target_contex_pairs)\n",
        "w2v.initialize_vectors(dim=20)\n",
        "w2v.train(neg_samples=5, lr=0.1, epochs=6, const_lr=True)"
      ],
      "id": "iphVXCt-1QxP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEng_pHG1QxQ"
      },
      "outputs": [],
      "source": [
        "w2v.save_weights_to_files('task1_w2v_vectors_neg5_dim20_lr01.txt',\n",
        "                          'task1_w2v_contexts_neg5_dim20_lr01.txt')"
      ],
      "id": "BEng_pHG1QxQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "JpxED7JO1QxQ",
        "outputId": "4b0c9825-7bfa-4cf5-9fed-abc6f948265f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WORD: pies\n",
            "    krowa 0.9014444947242737\n",
            "    facet 0.8701364398002625\n",
            "    liliputka 0.8605725765228271\n",
            "    osesek 0.8509775400161743\n",
            "    kot 0.848791241645813\n",
            "    nordyk 0.8469008207321167\n",
            "    panna 0.8351026773452759\n",
            "    kura 0.826441764831543\n",
            "    predator 0.8199484944343567\n",
            "    wąż 0.8166723251342773\n",
            "\n",
            "WORD: smok\n",
            "    bogini 0.8793821930885315\n",
            "    topielica 0.8707863092422485\n",
            "    mrówkojad 0.8640605211257935\n",
            "    krasnolud 0.8577755689620972\n",
            "    straszydło 0.8460540771484375\n",
            "    stwór 0.8394477963447571\n",
            "    niedźwiedzica 0.8370437026023865\n",
            "    afrodyta 0.836599588394165\n",
            "    pitekantrop 0.8360117673873901\n",
            "    toreador 0.8353720903396606\n",
            "\n",
            "WORD: miłość\n",
            "    twórczość 0.8362767100334167\n",
            "    wiara 0.8238243460655212\n",
            "    człowieczeństwo 0.8071744441986084\n",
            "    uczucie 0.7987077236175537\n",
            "    tęsknota 0.7985913753509521\n",
            "    miłosierdzie 0.7920595407485962\n",
            "    synostwo 0.7905794978141785\n",
            "    uskrzydlenie 0.7892605066299438\n",
            "    sen 0.7874357104301453\n",
            "    konanie 0.7870968580245972\n",
            "\n",
            "WORD: rower\n",
            "    skuter 0.9266026020050049\n",
            "    motocykl 0.9092680811882019\n",
            "    furgonetka 0.875817060470581\n",
            "    kajak 0.8653139472007751\n",
            "    wózek 0.8600744605064392\n",
            "    jeep 0.857009768486023\n",
            "    sanki 0.8558916449546814\n",
            "    wóz 0.849555492401123\n",
            "    sanka 0.8478562831878662\n",
            "    narta 0.8409382104873657\n",
            "\n",
            "WORD: maraton\n",
            "    czempionat 0.8708891868591309\n",
            "    basket 0.8685712218284607\n",
            "    trial 0.8661448359489441\n",
            "    cross 0.8612272143363953\n",
            "    euroliga 0.8578544855117798\n",
            "    sprint 0.8577399253845215\n",
            "    piknik 0.8566814661026001\n",
            "    slalom 0.8454224467277527\n",
            "    wampiriada 0.8433250188827515\n",
            "    czasówka 0.8430479764938354\n",
            "\n",
            "WORD: logika\n",
            "    antropocentryzm 0.8924211859703064\n",
            "    kategoryczność 0.8752405047416687\n",
            "    pragmatyka 0.8729718327522278\n",
            "    spójność 0.8632803559303284\n",
            "    pluralizm 0.8627861142158508\n",
            "    trygonometria 0.8621761202812195\n",
            "    opozycyjność 0.8587630987167358\n",
            "    polityzacja 0.8575786352157593\n",
            "    chiromancja 0.8552240133285522\n",
            "    centralizm 0.8505282402038574\n",
            "\n",
            "WORD: motyl\n",
            "    żółw 0.8810234665870667\n",
            "    czerw 0.87406986951828\n",
            "    jaszczurka 0.8690976500511169\n",
            "    wąż 0.8670338988304138\n",
            "    stwór 0.8662228584289551\n",
            "    owad 0.8620920777320862\n",
            "    antylopa 0.861985445022583\n",
            "    bażant 0.860480010509491\n",
            "    ślimak 0.857162356376648\n",
            "    jedwabnik 0.8491615653038025\n",
            "\n"
          ]
        }
      ],
      "source": [
        "task1_wv = KeyedVectors.load_word2vec_format('task1_w2v_vectors_neg3_dim20_constlr01.txt', binary=False)\n",
        "example_english_words = ['dog', 'dragon', 'love', 'bicycle', 'marathon', 'logic', 'butterfly']  # replace, or add your own examples\n",
        "example_polish_words = ['pies', 'smok', 'miłość', 'rower', 'maraton', 'logika', 'motyl']\n",
        "\n",
        "example_words = example_polish_words\n",
        "\n",
        "for w0 in example_words:\n",
        "    print ('WORD:', w0)\n",
        "    for w, v in task1_wv.most_similar(w0, topn=10):\n",
        "        print ('   ', w, v)\n",
        "    print ()"
      ],
      "id": "JpxED7JO1QxQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Umt4DRtr1QxQ",
        "outputId": "48c0f81a-b84e-49ea-cc05-987a82b53692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #0 start\n",
            "Epoch #0 end\n",
            "Epoch #1 start\n",
            "Epoch #1 end\n",
            "Epoch #2 start\n",
            "Epoch #2 end\n",
            "Epoch #3 start\n",
            "Epoch #3 end\n",
            "Epoch #4 start\n",
            "Epoch #4 end\n"
          ]
        }
      ],
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "import gensim\n",
        "model = Word2Vec(corpus_file=\"task1_objects_contexts_polish.txt\", vector_size=100, window=5, min_count=1,\n",
        "                 workers=4, callbacks=[EpochLogger()])\n",
        "model.save(\"task1_gensim.model\")"
      ],
      "id": "Umt4DRtr1QxQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HM2Tt5Rq1QxR"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec.load(\"task1_gensim.model\")"
      ],
      "id": "HM2Tt5Rq1QxR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "aV4GAbhR1QxS",
        "outputId": "8525f7f2-a3aa-4078-e94a-0f9906058676"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WORD: pies\n",
            "    koń 0.9845697283744812\n",
            "    dziewczyna 0.9662677645683289\n",
            "    zwierzę 0.9631377458572388\n",
            "    chłopiec 0.9611811637878418\n",
            "    kot 0.9588338732719421\n",
            "    ptak 0.9534725546836853\n",
            "    chłopak 0.9449874758720398\n",
            "    facet 0.944753885269165\n",
            "    dziewczynka 0.9417502880096436\n",
            "    chłopek 0.9391786456108093\n",
            "\n",
            "WORD: smok\n",
            "    baba 0.9924331307411194\n",
            "    krowa 0.989021360874176\n",
            "    szczur 0.988284707069397\n",
            "    mucha 0.9871571063995361\n",
            "    pszczoła 0.9853836894035339\n",
            "    kota 0.9851759672164917\n",
            "    kura 0.9841569662094116\n",
            "    kamyk 0.9832960963249207\n",
            "    słoń 0.9817646741867065\n",
            "    niedźwiedź 0.981716513633728\n",
            "\n",
            "WORD: miłość\n",
            "    wiara 0.9597357511520386\n",
            "    wyobraźnia 0.9333335757255554\n",
            "    duch 0.932645857334137\n",
            "    nadzieja 0.932121753692627\n",
            "    radość 0.9288252592086792\n",
            "    marzenie 0.9262442588806152\n",
            "    umysł 0.9238408207893372\n",
            "    szczęście 0.9171186685562134\n",
            "    nienawiść 0.9145898818969727\n",
            "    dusza 0.9100762605667114\n",
            "\n",
            "WORD: rower\n",
            "    telewizor 0.9882622957229614\n",
            "    wózek 0.9849364161491394\n",
            "    pistolet 0.9822293519973755\n",
            "    fotel 0.9814831018447876\n",
            "    stolik 0.9795236587524414\n",
            "    karabin 0.9795141816139221\n",
            "    paczka 0.9780448079109192\n",
            "    krzesło 0.9778200387954712\n",
            "    zabawka 0.9768279194831848\n",
            "    kosz 0.9766708612442017\n",
            "\n",
            "WORD: maraton\n",
            "    jarmark 0.9952144026756287\n",
            "    mityng 0.9951916933059692\n",
            "    dożynki 0.9951515793800354\n",
            "    plener 0.9942498207092285\n",
            "    biesiada 0.9939975738525391\n",
            "    sympozjum 0.9939942359924316\n",
            "    piknik 0.9934744238853455\n",
            "    rejs 0.9934555292129517\n",
            "    zgrupowanie 0.9929692149162292\n",
            "    sparing 0.990733802318573\n",
            "\n",
            "WORD: logika\n",
            "    motywacja 0.9809197187423706\n",
            "    zależność 0.9806596040725708\n",
            "    fenomen 0.976975679397583\n",
            "    nawyk 0.976619303226471\n",
            "    powinność 0.9754154682159424\n",
            "    uwarunkowanie 0.9733779430389404\n",
            "    odrębność 0.9708852767944336\n",
            "    niuans 0.9698395729064941\n",
            "    preferencja 0.9698342680931091\n",
            "    wymowa 0.9697948098182678\n",
            "\n",
            "WORD: motyl\n",
            "    pająk 0.9894561171531677\n",
            "    wierzba 0.9891325831413269\n",
            "    gęś 0.9886512756347656\n",
            "    ptaszek 0.988226056098938\n",
            "    piesek 0.9881105422973633\n",
            "    konik 0.988014280796051\n",
            "    pal 0.9868509769439697\n",
            "    kur 0.9865700006484985\n",
            "    gad 0.9856919646263123\n",
            "    łabędź 0.9856831431388855\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# task1_wv = KeyedVectors.load_word2vec_format('task1_w2v_vectors_neg20_dim128_numpy_choice.txt', binary=False)\n",
        "example_english_words = ['dog', 'dragon', 'love', 'bicycle', 'marathon', 'logic', 'butterfly']  # replace, or add your own examples\n",
        "example_polish_words = ['pies', 'smok', 'miłość', 'rower', 'maraton', 'logika', 'motyl']\n",
        "\n",
        "example_words = example_polish_words\n",
        "\n",
        "for w0 in example_words:\n",
        "    print ('WORD:', w0)\n",
        "    for w, v in model.wv.most_similar(w0, topn=10):\n",
        "        print ('   ', w, v)\n",
        "    print ()"
      ],
      "id": "aV4GAbhR1QxS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR9dvhWF1QxS"
      },
      "source": [
        "# Task 2 (4 points)\n",
        "\n",
        "Your task is to train the embeddings for Simple Wikipedia titles, using gensim library. As the example below shows, training is really simple:\n",
        "\n",
        "```python\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "model.save(\"word2vec.model\")\n",
        "```\n",
        "*sentences* can be a list of list of tokens, you can also use *gensim.models.word2vec.LineSentence(source)* to create restartable iterator from file. At first, use [this file] containing such pairs of titles, that one article links to another.\n",
        "\n",
        "We say that two titles are *related* if they both contain a word (or a word bigram) which is not very popular (it occurs only in several titles). Make this definition more precise, and create the corpora which contains pairs of related titles. Make a mixture of the original corpora, and the new one, then train title vectors again.\n",
        "\n",
        "Compare these two approaches using similar code to the code from Task 1."
      ],
      "id": "NR9dvhWF1QxS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cclVaY5G1QxS"
      },
      "outputs": [],
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "import gensim\n",
        "# sentences = gensim.models.word2vec.LineSentence(\"simple.wiki.links.txt\")\n",
        "# model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "model = Word2Vec(corpus_file=\"simple.wiki.links.txt\", vector_size=100, window=5, min_count=1, workers=4) \n",
        "model.save(\"task2_links_only.model\")"
      ],
      "id": "cclVaY5G1QxS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeJ36rGT1QxT"
      },
      "outputs": [],
      "source": [
        "task2_wv = Word2Vec.load(\"task2_links_only.model\")"
      ],
      "id": "zeJ36rGT1QxT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "5rRohtyi1QxT",
        "outputId": "6f2f0429-5fb0-433d-c1a7-1f9242c6294e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WORD: statistics\n",
            "    machine 0.9958315491676331\n",
            "    idea 0.9952155947685242\n",
            "    vaccine 0.9942779541015625\n",
            "    health 0.9935333728790283\n",
            "    flight 0.993130087852478\n",
            "    clock 0.9928908348083496\n",
            "    communication 0.9923535585403442\n",
            "    gospel 0.9921176433563232\n",
            "    theory 0.991838812828064\n",
            "    meteor 0.9916765093803406\n",
            "\n",
            "WORD: harry_potter_and_the_order_of_the_phoenix\n",
            "    louisiana_blues 0.9936421513557434\n",
            "    parent 0.9931883811950684\n",
            "    category:british_military_people 0.9930567741394043\n",
            "    shirt 0.9930347800254822\n",
            "    albus_dumbledore 0.9928532242774963\n",
            "    low 0.9928193092346191\n",
            "    a_minor 0.9927289485931396\n",
            "    nine 0.9926779866218567\n",
            "    matrix_function 0.9926573634147644\n",
            "    interpretation 0.9926302433013916\n",
            "\n",
            "WORD: western_philosophy\n",
            "    unification_church 0.9958949685096741\n",
            "    history_of_indonesia 0.995868980884552\n",
            "    310_bc 0.9956936240196228\n",
            "    khmer_empire 0.9955422282218933\n",
            "    0s 0.9955031275749207\n",
            "    ablai_khan 0.9954162836074829\n",
            "    template:european_diasporas 0.9951825141906738\n",
            "    christian_vii_of_denmark 0.9951239824295044\n",
            "    496_bc 0.9950959086418152\n",
            "    oil_paint 0.995075523853302\n",
            "\n",
            "WORD: crops\n",
            "    macquarie_river 0.9975330829620361\n",
            "    list_of_stars_in_apus 0.9974477291107178\n",
            "    orion_arm 0.9973905682563782\n",
            "    angas_downs_indigenous_protected_area 0.997229278087616\n",
            "    largest_artiodactyls 0.9971519708633423\n",
            "    parables_of_jesus 0.9971413612365723\n",
            "    list_of_minerals 0.9971050024032593\n",
            "    wildlife_of_afghanistan 0.9970151782035828\n",
            "    category:symbols 0.9969925284385681\n",
            "    template:municipalities_of_iceland 0.9969574809074402\n",
            "\n",
            "WORD: acceleration\n",
            "    magnetism 0.9968989491462708\n",
            "    sphere 0.9968553781509399\n",
            "    template:solar_system 0.9967824220657349\n",
            "    rotation 0.9967520833015442\n",
            "    thermodynamics 0.9965111017227173\n",
            "    list_of_solar_system_objects 0.996445894241333\n",
            "    list_of_jupiter's_moons 0.9963183999061584\n",
            "    red_giant 0.9962056279182434\n",
            "    light_year 0.9960886240005493\n",
            "    planetary_nebula 0.9960557222366333\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example_titles = ['statistics', 'harry_potter_and_the_order_of_the_phoenix', 'western_philosophy', 'crops', 'acceleration']\n",
        "\n",
        "example_words = example_titles\n",
        "\n",
        "for w0 in example_words:\n",
        "    print ('WORD:', w0)\n",
        "    for w, v in task2_wv.wv.most_similar(w0, topn=10):\n",
        "        print ('   ', w, v)\n",
        "    print ()"
      ],
      "id": "5rRohtyi1QxT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tqFKdrS1QxT"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from itertools import permutations\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "import gensim\n",
        "\n",
        "def get_unique_titles(filename):\n",
        "    titles = set()\n",
        "    with open(filename, \"r\", encoding=\"utf8\") as file:\n",
        "        for line in tqdm(file):\n",
        "            line = line.strip()\n",
        "            many_titles = line.split()\n",
        "            for title in many_titles:\n",
        "                titles.add(title)\n",
        "    return list(titles)\n",
        "\n",
        "# def get_unigrams(filename):\n",
        "#     unique_unigrams = set()\n",
        "#     with open(filename, \"r\", encoding=\"utf8\") as file:\n",
        "#         for line in tqdm(file):\n",
        "#             line = line.strip()\n",
        "#             unigrams = re.split(\" |_|:\", line)\n",
        "#             for unigram in unigrams:\n",
        "#                 unique_unigrams.add(unigram)\n",
        "#     return unique_unigrams\n",
        "\n",
        "def count_unigrams(titles):\n",
        "    unigram_reverse_index = defaultdict(list)\n",
        "    for i, title in enumerate(tqdm(titles)):\n",
        "        unigrams = re.split(\"_|:\", title)\n",
        "        for unigram in unigrams:\n",
        "            unigram_reverse_index[unigram].append(i)\n",
        "    return unigram_reverse_index\n",
        "\n",
        "def get_unigrams_below_threshold(unigram_reverse_index, threshold=5):\n",
        "    unigram_reverse_index_thresholded = defaultdict(list)\n",
        "    for unigram, title_ids in tqdm(unigram_reverse_index.items()):\n",
        "        n_of_ids = len(set(title_ids))\n",
        "        if n_of_ids > 1 and n_of_ids <= threshold:\n",
        "            unigram_reverse_index_thresholded[unigram] = list(set(title_ids))\n",
        "    return unigram_reverse_index_thresholded\n",
        "\n",
        "def create_pairs(titles, unigram_reverse_index_thresholded, filename):\n",
        "    with open(filename, \"w\", encoding=\"utf8\") as file:\n",
        "        for unigram, title_ids in tqdm(unigram_reverse_index_thresholded.items()):\n",
        "            for id1, id2 in permutations(title_ids, 2):\n",
        "                file.write(titles[id1] + \" \" + titles[id2] + \"\\n\")\n",
        "# combinations\n",
        "# def write_unigram_counts_to_file(unigram_counts, filename):\n",
        "#     with open(filename, \"w\", encoding=\"utf8\") as file:\n",
        "#         for unigram, count in tqdm(unigram_counts.items()):\n",
        "#             file.write(unigram + \" \" + str(count) + \"\\n\")\n",
        "            \n",
        "def create_mixed_corpus(corpus1_filename, corpus2_filename):\n",
        "    corpus1 = [line.strip().split() for line in open(corpus1_filename, \"r\", encoding=\"utf8\")]\n",
        "    corpus2 = [line.strip().split() for line in open(corpus2_filename, \"r\", encoding=\"utf8\")]\n",
        "    corpus1.extend(corpus2)\n",
        "    np.random.shuffle(corpus1)\n",
        "    return corpus1"
      ],
      "id": "6tqFKdrS1QxT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQVj-i071QxU"
      },
      "source": [
        "Creating unigram pairs file"
      ],
      "id": "bQVj-i071QxU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT2Y4XSl1QxU"
      },
      "outputs": [],
      "source": [
        "titles = get_unique_titles(\"simple.wiki.links.txt\")\n",
        "unigram_reverse_index = count_unigrams(titles)\n",
        "unigram_reverse_index_thresholded = get_unigrams_below_threshold(unigram_reverse_index, threshold=5)\n",
        "create_pairs(titles, unigram_reverse_index_thresholded, \"task2_unigram_links_thr5.txt\")"
      ],
      "id": "bT2Y4XSl1QxU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGnt-4jh1QxU"
      },
      "outputs": [],
      "source": [
        "titles = get_unique_titles(\"simple.wiki.links.txt\")\n",
        "unigram_reverse_index = count_unigrams(titles)\n",
        "unigram_reverse_index_thresholded = get_unigrams_below_threshold(unigram_reverse_index, threshold=10)\n",
        "create_pairs(titles, unigram_reverse_index_thresholded, \"task2_unigram_links_thr10.txt\")"
      ],
      "id": "DGnt-4jh1QxU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_tiQoER1QxU"
      },
      "outputs": [],
      "source": [
        "titles = get_unique_titles(\"simple.wiki.links.txt\")\n",
        "unigram_reverse_index = count_unigrams(titles)\n",
        "unigram_reverse_index_thresholded = get_unigrams_below_threshold(unigram_reverse_index, threshold=15)\n",
        "create_pairs(titles, unigram_reverse_index_thresholded, \"task2_unigram_links_thr15.txt\")"
      ],
      "id": "d_tiQoER1QxU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M1sX4-r1QxV"
      },
      "outputs": [],
      "source": [
        "titles = get_unique_titles(\"simple.wiki.links.txt\")\n",
        "unigram_reverse_index = count_unigrams(titles)\n",
        "unigram_reverse_index_thresholded = get_unigrams_below_threshold(unigram_reverse_index, threshold=20)\n",
        "create_pairs(titles, unigram_reverse_index_thresholded, \"task2_unigram_links_thr20.txt\")"
      ],
      "id": "1M1sX4-r1QxV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAnniSWH1QxV"
      },
      "source": [
        "Creating corporas and training"
      ],
      "id": "DAnniSWH1QxV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxPfPkr81QxV"
      },
      "outputs": [],
      "source": [
        "sentences = create_mixed_corpus(\"simple.wiki.links.txt\", \"task2_unigram_links_thr5.txt\")"
      ],
      "id": "uxPfPkr81QxV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0X1z_X0J1QxW",
        "outputId": "058a2a1b-ac18-4931-a2b5-8cf1c134142f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #0 start\n",
            "Epoch #0 end\n",
            "Epoch #1 start\n",
            "Epoch #1 end\n",
            "Epoch #2 start\n",
            "Epoch #2 end\n",
            "Epoch #3 start\n",
            "Epoch #3 end\n",
            "Epoch #4 start\n",
            "Epoch #4 end\n"
          ]
        }
      ],
      "source": [
        "epoch_logger = EpochLogger()\n",
        "model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4, callbacks=[epoch_logger])\n",
        "model.save(\"task2_mixed_thr5.model\")"
      ],
      "id": "0X1z_X0J1QxW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uTYVbmH1QxW"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec.load(\"task2_mixed_thr5.model\")"
      ],
      "id": "6uTYVbmH1QxW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "_etVZr691QxX",
        "outputId": "1cc2d2ec-a165-4424-bff7-27f51743a6c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WORD: statistics\n",
            "    vaccine 0.9946828484535217\n",
            "    perihelion 0.9933329224586487\n",
            "    idea 0.9931748509407043\n",
            "    astrology 0.9928882122039795\n",
            "    camera 0.9925744533538818\n",
            "    communication 0.9924458265304565\n",
            "    1004 0.9924340844154358\n",
            "    night 0.9920859932899475\n",
            "    37 0.9920130372047424\n",
            "    47_bc 0.9919556379318237\n",
            "\n",
            "WORD: harry_potter_and_the_order_of_the_phoenix\n",
            "    harry_potter_and_the_half-blood_prince 0.9963700771331787\n",
            "    red_river 0.996210515499115\n",
            "    town_rights 0.9962059259414673\n",
            "    template:politics_of_iran 0.9960891604423523\n",
            "    template:ethnic_groups_of_russia 0.9960600137710571\n",
            "    bass 0.9959389567375183\n",
            "    henry_king 0.9958997368812561\n",
            "    category:brazilian_lawyers 0.995841383934021\n",
            "    template:campaignbox_korean_war 0.9958338141441345\n",
            "    düsseldorf_school_of_painting 0.9958128929138184\n",
            "\n",
            "WORD: western_philosophy\n",
            "    merchant_vessel 0.9962515830993652\n",
            "    holguín_province 0.9954962730407715\n",
            "    joongang_ilbo 0.9951223731040955\n",
            "    north_sulawesi 0.994918704032898\n",
            "    tanggu_truce 0.9947398900985718\n",
            "    horse_racing 0.9947186708450317\n",
            "    anglophone_crisis 0.9946839809417725\n",
            "    china_airlines_flight_611 0.9946569204330444\n",
            "    herald_sun 0.9946327209472656\n",
            "    calligraphy 0.9946072101593018\n",
            "\n",
            "WORD: crops\n",
            "    stain 0.9977034330368042\n",
            "    amun 0.9976099729537964\n",
            "    list_of_kings_of_sparta 0.9975506067276001\n",
            "    bottom_trawling 0.9975094795227051\n",
            "    nome_(egypt) 0.9974976181983948\n",
            "    hyaena 0.9974814057350159\n",
            "    gout 0.9973966479301453\n",
            "    ruminant 0.9973419904708862\n",
            "    strychnine 0.9973418116569519\n",
            "    pedophilia 0.997331440448761\n",
            "\n",
            "WORD: acceleration\n",
            "    weight 0.9969281554222107\n",
            "    julian_year_(astronomy) 0.9967513680458069\n",
            "    sphere 0.9967496991157532\n",
            "    ionizing_radiation 0.9965735673904419\n",
            "    circle 0.9965101480484009\n",
            "    general_relativity 0.9965047836303711\n",
            "    degree_(angle) 0.9963260889053345\n",
            "    vacuum 0.9962300062179565\n",
            "    list_of_adjectivals_and_demonyms_of_astronomical_bodies 0.9962205290794373\n",
            "    red_giant 0.9961674809455872\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example_titles = ['statistics', 'harry_potter_and_the_order_of_the_phoenix', 'western_philosophy', 'crops', 'acceleration']\n",
        "\n",
        "example_words = example_titles\n",
        "\n",
        "for w0 in example_words:\n",
        "    print ('WORD:', w0)\n",
        "    for w, v in model.wv.most_similar(w0, topn=10):\n",
        "        print ('   ', w, v)\n",
        "    print ()"
      ],
      "id": "_etVZr691QxX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3s3QmEt11QxX",
        "outputId": "b9dad5b3-4124-40d5-f384-cc13e44053d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #5 start\n",
            "Epoch #5 end\n",
            "Epoch #6 start\n",
            "Epoch #6 end\n",
            "Epoch #7 start\n",
            "Epoch #7 end\n",
            "Epoch #8 start\n",
            "Epoch #8 end\n",
            "Epoch #9 start\n",
            "Epoch #9 end\n"
          ]
        }
      ],
      "source": [
        "sentences = create_mixed_corpus(\"simple.wiki.links.txt\", \"task2_unigram_links_thr20.txt\")\n",
        "epoch_logger = EpochLogger()\n",
        "model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4, callbacks=[epoch_logger])\n",
        "model.save(\"task2_mixed_thr20.model\")"
      ],
      "id": "3s3QmEt11QxX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpiNe26c1QxX"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec.load(\"task2_links_only.model\")"
      ],
      "id": "QpiNe26c1QxX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YkvlPev1QxY",
        "outputId": "cbd46b75-70f0-4388-d5ad-a906d9394977"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WORD: statistics\n",
            "    communication 0.9970863461494446\n",
            "    night 0.9969813227653503\n",
            "    billion 0.9969518780708313\n",
            "    vaccine 0.9968886375427246\n",
            "    photography 0.9963081479072571\n",
            "    man 0.9961920380592346\n",
            "    translation 0.9960965514183044\n",
            "    poem 0.9960731863975525\n",
            "    995 0.9960111975669861\n",
            "    antioch 0.9958942532539368\n",
            "\n",
            "WORD: harry_potter_and_the_order_of_the_phoenix\n",
            "    bullying 0.9964322447776794\n",
            "    lie 0.99638432264328\n",
            "    doctor_who_companions 0.9960330724716187\n",
            "    eve 0.9958974719047546\n",
            "    recipe 0.9956963658332825\n",
            "    changsha 0.9956899285316467\n",
            "    rubik's_cube 0.9956322312355042\n",
            "    hello 0.9955694079399109\n",
            "    duck_family_(disney) 0.9955654740333557\n",
            "    harry_potter_and_the_goblet_of_fire 0.9955259561538696\n",
            "\n",
            "WORD: western_philosophy\n",
            "    hamad_bin_khalifa_al_thani 0.9969141483306885\n",
            "    jan_peter_balkenende 0.9963982105255127\n",
            "    tsering_woeser 0.9963878989219666\n",
            "    horse_racing 0.9963662028312683\n",
            "    pakistan_international_airlines 0.9961951971054077\n",
            "    kyshtym_disaster 0.9961144924163818\n",
            "    the_five 0.9960407614707947\n",
            "    jude_the_apostle 0.9959894418716431\n",
            "    tvxq 0.9959824085235596\n",
            "    pedro_álvares_cabral 0.995974600315094\n",
            "\n",
            "WORD: crops\n",
            "    silica 0.9980490207672119\n",
            "    phobia 0.9977456331253052\n",
            "    category:mammals_of_africa 0.997668445110321\n",
            "    mammals 0.9976035952568054\n",
            "    ruminantia 0.9975891709327698\n",
            "    polychaete 0.9975460767745972\n",
            "    template:navbox_with_columns/doc 0.9975419640541077\n",
            "    asteraceae 0.997515082359314\n",
            "    leaves 0.9974994659423828\n",
            "    starch 0.9974973797798157\n",
            "\n",
            "WORD: acceleration\n",
            "    general_relativity 0.9978737235069275\n",
            "    atmospheric_chemistry 0.9976301789283752\n",
            "    umbriel_(moon) 0.9975717067718506\n",
            "    special_relativity 0.9975581169128418\n",
            "    history_of_astronomy 0.9973951578140259\n",
            "    wikipedia:article_wizard/example 0.9973015189170837\n",
            "    tidal_locking 0.9970972537994385\n",
            "    julian_day 0.9970089197158813\n",
            "    axial_tilt 0.9968255162239075\n",
            "    giant_planet 0.9967917799949646\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example_titles = ['statistics', 'harry_potter_and_the_order_of_the_phoenix', 'western_philosophy', 'crops', 'acceleration']\n",
        "\n",
        "example_words = example_titles\n",
        "\n",
        "for w0 in example_words:\n",
        "    print ('WORD:', w0)\n",
        "    for w, v in model.wv.most_similar(w0, topn=10):\n",
        "        print ('   ', w, v)\n",
        "    print ()"
      ],
      "id": "1YkvlPev1QxY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUwWElp61QxZ"
      },
      "source": [
        "# Task 3 (4 points)\n",
        "\n",
        "Suppose that we have two languages: Upper and Lower. This is an example Upper sentence:\n",
        "\n",
        "<pre>\n",
        "THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG.\n",
        "</pre>\n",
        "\n",
        "And this is its translation into Lower:\n",
        "\n",
        "<pre>\n",
        "the quick brown fox jumps over the lazy dog\n",
        "</pre>\n",
        "\n",
        "You have two corpora for these languages (with different sentences). Your task is to train word embedings for both languages together, so as to make embeddings of the words which are its translations as close as possible. But unfortunately, you have the budget which allows you to prepare the translation only for 1000 words (we call it D, you have to deside which words you want to be in D)\n",
        "\n",
        "Prepare the corpora wich contains three kind of sentences:\n",
        "* Upper corpus sentences\n",
        "* Lower corpus sentences\n",
        "* sentences derived from Upper/Lower corpus, modified using D\n",
        "\n",
        "There are many possible ways of doing this, for instance this one (ROT13.COM: hfr rirel fragrapr sebz obgu pbecben gjvpr: jvgubhg nal zbqvsvpngvbaf, naq jvgu rirel jbeqf sebz Q ercynprq ol vgf genafyngvba)\n",
        "\n",
        "We define the score for an Upper WORD as  $\\frac{1}{p}$, where $p$ is a position of its translation in the list of **Lower** words most similar to WORD. For instance, when most similar words to DOG are:\n",
        "\n",
        "<pre>\n",
        "WOLF, CAT, WOLVES, LION, gopher, dog\n",
        "</pre>\n",
        "\n",
        "then the score for the word DOG is 0.5. Compute the average score separately for words from D, and for words out of D (hint: if the computation takes to much time do it for a random sample).\n"
      ],
      "id": "VUwWElp61QxZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR47rmb51QxZ"
      },
      "outputs": [],
      "source": [
        "import unicodedata"
      ],
      "id": "AR47rmb51QxZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzy_ZJAD1QxZ"
      },
      "outputs": [],
      "source": [
        "def count_bigrams(lower_file, upper_file):\n",
        "    word_to_count = defaultdict(int)\n",
        "    punctations = [\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"]\n",
        "    with open(lower_file, \"r\", encoding=\"utf8\") as lf, open(upper_file, \"r\", encoding=\"utf8\") as uf:\n",
        "        for line in tqdm(lf):\n",
        "            line = line.strip().split()\n",
        "            for word in line:\n",
        "                if all(unicodedata.category(c) in punctations for c in word) or word.isdigit():\n",
        "                    pass\n",
        "                else:\n",
        "                    word_to_count[word] += 1\n",
        "        for line in tqdm(uf):\n",
        "            line = line.strip().lower().split()\n",
        "            for word in line:\n",
        "                if all(unicodedata.category(c) in punctations for c in word) or word.isdigit():\n",
        "                    pass\n",
        "                else:\n",
        "                    word_to_count[word] += 1\n",
        "    word_counts = [(k, v) for k, v in word_to_count.items()]\n",
        "    word_counts = sorted(word_counts, key=lambda x: x[1], reverse=True)\n",
        "    words = [tup[0] for tup in word_counts]\n",
        "    return words\n",
        "\n",
        "def create_new_corpora(words, lower_filename, upper_filename, target_filename):\n",
        "    translatable_words = set(words)\n",
        "    corpora = []\n",
        "    with open(lower_filename, \"r\", encoding=\"utf8\") as lf, open(upper_filename, \"r\", encoding=\"utf8\") as uf,\\\n",
        "        open(target_filename, \"w\", encoding=\"utf8\") as target_file:\n",
        "        for line in tqdm(lf):\n",
        "            line = line.strip()\n",
        "            target_file.write(line+\"\\n\")\n",
        "#             corpora.append(line)\n",
        "            splitted_line = line.split()\n",
        "            translated_line = \" \".join(word.upper() if word in translatable_words else word for word in splitted_line)\n",
        "            corpora.append(translated_line)\n",
        "        for line in tqdm(uf):\n",
        "            line = line.strip()\n",
        "#             corpora.append(line)\n",
        "            target_file.write(line+\"\\n\")\n",
        "            splitted_line = line.split()\n",
        "            translated_line = \" \".join(word.lower() if word.lower() in translatable_words else word for word in splitted_line)\n",
        "            corpora.append(translated_line)\n",
        "#         np.random.shuffle(corpora)\n",
        "        for sentence in corpora:\n",
        "            target_file.write(sentence+\"\\n\")\n",
        "\n",
        "def calc_score_for_d(words, model, topn=1000):\n",
        "    scores = []\n",
        "    for word in tqdm(words):\n",
        "        upper_words_count = 0\n",
        "        found = False\n",
        "        for w, v in model.wv.most_similar(word, topn=topn):\n",
        "            if w.isupper():\n",
        "                upper_words_count+=1\n",
        "                if w.lower() == word:\n",
        "                    found = True\n",
        "                    break\n",
        "        scores.append(0 if not found else 1/upper_words_count)\n",
        "        \n",
        "        word = word.upper()\n",
        "        lower_words_count = 0\n",
        "        found = False\n",
        "        for w, v in model.wv.most_similar(word, topn=topn):\n",
        "            if w.islower():\n",
        "                lower_words_count+=1\n",
        "                if w.upper() == word:\n",
        "                    found = True\n",
        "                    break\n",
        "        scores.append(0 if not found else 1/upper_words_count)\n",
        "    return scores, sum(scores)/len(scores)\n",
        "\n",
        "def calc_score_for_outside_d(words, allwords, model, n_samples=1000, topn=1000):\n",
        "    words = set(words)\n",
        "    allwords = [word for word in allwords if word not in words and all(c == '_' or c.isalpha() for c in word)]\n",
        "    sampled_words = np.random.choice(allwords, n_samples, replace=False)\n",
        "    scores = []\n",
        "    for word in tqdm(sampled_words):\n",
        "        upper_words_count = 0\n",
        "        found = False\n",
        "        try:\n",
        "            for w, v in model.wv.most_similar(word, topn=topn):\n",
        "                if w.isupper():\n",
        "                    upper_words_count+=1\n",
        "                    if w.lower() == word:\n",
        "                        found = True\n",
        "                        break\n",
        "        except:\n",
        "            pass\n",
        "        scores.append(0 if not found else 1/upper_words_count)\n",
        "        \n",
        "        word = word.upper()\n",
        "        lower_words_count = 0\n",
        "        found = False\n",
        "        try:\n",
        "            for w, v in model.wv.most_similar(word, topn=topn):\n",
        "                if w.islower():\n",
        "                    lower_words_count+=1\n",
        "                    if w.upper() == word:\n",
        "                        found = True\n",
        "                        break\n",
        "        except:\n",
        "            pass\n",
        "        scores.append(0 if not found else 1/lower_words_count)\n",
        "    return scores, sum(scores)/len(scores)"
      ],
      "id": "lzy_ZJAD1QxZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "82aab73a7f974d2fb24ea6fdffab20a5",
            "de3aa8dfc31d400093df56d531c33ea6"
          ]
        },
        "id": "bqy5lB_-1Qxa",
        "outputId": "0f3aa59e-0ef2-4330-8d64-b8c0fa8eb608"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82aab73a7f974d2fb24ea6fdffab20a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de3aa8dfc31d400093df56d531c33ea6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "allwords = count_bigrams(\"task3_polish_lower.txt\", \"task3_polish_upper.txt\")\n",
        "words = allwords[:1000]"
      ],
      "id": "bqy5lB_-1Qxa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "amvXZukO1Qxa",
        "outputId": "0577714e-11d3-4e47-db86-4056494ad722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['w', 'ten', 'i', 'być', 'na', 'z', 'do', 'się', 'nie_on', 'on', 'o', 'że', 'który', 'a', 'przez', 'ustawa', 'od', 'r.', 'rok_rok_roki', 'oraz', 'za', 'dla', 'po', 'zostać', 'czy', 'jak', 'dzień', 'pan', 'praca', 'co', 'taki', 'projekt', 'państwo', 'osoba', 'minister', 'polski', 'ale', 'mieć_mój', 'wszystki', 'inny', 'również', 'tak', 'móc', 'zmiana', 'komisja', 'ze', 'także', 'sprawa', 'może_móc', 'poseł', 'swój', 'bardzo', 'być_były', 'bycie_być', 'tylko', 'czas', 'ten_tychy', 'chcieć', 'już', 'pierwszy', 'lub_lubić', 'lato_rok', 'zakres', 'tenże_też', 'nowy', 'jednak', 'pytanie', 'działanie', 'miejsce', 'nasz', 'środek', 'art.', 'przepis', 'jaki', 'polska_polski', 'rząd', 'poprawka', 'jako', 'sam', 'należeć', 'przy', 'związek', 'wiele', 'publiczny', 'sytuacja', 'nad', 'sejm', 'warszawa', 'aby', 'możliwość', 'społeczny', 'dotyczący', 'wniosek', 'cel', 'można_możny', 'system', 'powinien', 'europejski', 'strona', 'ministerstwo', 'bo', 'dwa', 'prawo', 'uwaga', 'każdy', 'informacja', 'problem', 'przypadek', 'pod', 'jeden_jedny', 'prosić', 'klub', 'kraj', 'prawo_prawy', 'dziecko', 'jeszcze', 'program', 'drugi', 'pani', 'zasada', 'zadanie', 'szkoła', 'rozwiązanie', 'mówić', 'jeśli', 'iż', 'polska', 'prawny', 'sposobić_sposób', 'działalność', 'pomoc', 'cały', 'decyzja', 'finansowy', 'jeżeli_jeżeć', 'związany', 'więc', 'głos', 'koszt', 'przed', 'raz', 'realizacja', 'ochrona', 'narodowy', 'powiedzieć', 'on_ony', 'okres', 'stanowisko', 'udział', 'zgodnie', 'podstawa', 'kwestia', 'chodzić_chód', 'warunek', 'samorząd', 'budżet', 'kolejny', 'maić_mieć', 'usta', 'mój', 'jednostka', 'gospodarczy', 'świadczenie', 'unia', 'duży', 'rozwój', 'natomiast', 'miasto', 'zdrowie', 'bez_beza', 'żeby', 'fundusz', 'by', 'człowiek', 'grupa', 'punkt', 'liczba', 'rad_rada', 'nizać_niż', 'mieć', 'gmin_gmina', 'wysoki', 'podatek', 'teren', 'wzgląd', 'zł', 'rodzina', 'sąd', 'obecnie', 'rama', 'przyjęcie', 'wynik', 'pracownik', 'koniec', 'podczas', 'rzecz', 'żaden', 'urząd', 'sprawa_sprawić', 'postępowanie', 'kto', 'kiedy', 'dlatego', 'obowiązek', 'organ', 'dane_dany', 'dotyczyć', 'różny', 'we', 'izba', 'pan_pani', 'służba', 'nr', 'mama_mamić_mieć', 'gospodarka', 'nawet', 'wiedzieć', 'organizacja', 'sobie', 'ostatni', 'tu', 'miał_mieć', 'spółka', 'określony', 'gdy', 'gdzie', 'powyższy', 'wysokość', 'potrzeba', 'poziom', 'firma', 'umowa', 'krajowy', 'państwowy', 'ważny', 'styczeń', 'kontrola', 'm.in.', 'musieć_musić_muszy', 'rynek', 'trzeba', 'niektóry', 'życie', 'pewny', 'kwota', 'temat', 'wiek_wieko', 'senat', 'obszar', 'chwila', 'forma', 'główny', 'usługa', 'proces', 'podmiot', 'wobec', 'brak', 'poseł_posły', 'znajdować_znajdywać', 'droga', 'większość', 'województwo', 'dzisiaj', 'finanse', 'sprawiedliwość', 'tytuł', 'zespół', 'świat', 'środowisko', 'zakład', 'wojna', 'cena', 'własny', 'mieszkaniec', 'jeden', 'częsty_część', 'obecny', 'zawodowy', 'jak_jaki', 'odpowiedź', 'wybór', 'grudzień', 'mama_mieć', 'tutaj', 'dobry', 'wynikać', 'imię', 'trzy', 'część', 'tama', 'bezpieczeństwo', 'wielki', 'władza', 'instytucja', 'propozycja', 'właściwy', 'zarówno', 'wprowadzenie', 'oczywiście', 'marzec', 'opinia', 'właśnie', 'budowa', 'mistrzostwo', 'prowadzić', 'ponieważ', 'nieruchomość', 'miesiąc', 'wrzesień', 'rozporządzenie', 'inwestycja', 'ocena', 'ponad', 'polityka_polityki', 'między', 'przyjąć', 'podstawowy', 'termin', 'podjąć', 'czerwiec', 'członek', 'lipiec', 'posiadać', 'plan', 'administracja', 'przede', 'jednocześnie', 'większy', 'zdrowotny', 'czyli', 'istotny', 'wzrost', 'jakiś', 'według', 'rodzaj', 'uważać', 'badanie', 'miąć_on', 'wydawać', 'stosunek', 'rządowy', 'maj', 'proponować', 'głosowanie', 'więcej', 'życie_żyto', 'stopień', 'znaleźć', 'zapis', 'ubezpieczenie', 'otrzymać', 'dodatkowy', 'kwiecień', 'tys.', 'fakt', 'ponadto', 'dyskusja', 'np.', 'wyższy', 'ręka', 'poszczególny', 'wpływ', 'granica', 'skutek', 'zawarty', 'często', 'przedsiębiorstwo', 'obowiązujący', 'stan', 'powód', 'mln', 'prowadzony', 'sprawozdanie', 'październik', 'charakter', 'bardziej', 'stan_stanie_stanąć_stać_stanie', 'ii', 'kilka', 'istnieć', 'zatem', 'skarb', 'dokument', 'europ_europa', 'dzięki', 'pieniądz', 'zdanie', 'polityczny', 'ww.', 'odpowiedni', 'kościół', 'dochód', 'listopad', 'później', 'opieka', 'wartość', 'jeść_on', 'rzeczpospolita', 'nauka', 'prezes', 'dalszy', 'początek', 'marszałek', 'parlamentarny', 'rozpocząć', 'uzyskać', 'bowiem', 'ruch', 'funkcjonowanie', 'obywatel', 'współpraca', 'występować', 'nowelizacja', 'poza', 'budynek', 'sierpień', 'myśleć', 'podatkowy', 'dobro_dobrze', 'jedynie', 'pracować', 'spotkanie', 'dom', 'mały', 'funkcja', 'ok.', 'element', 'wymagać', 'prowadzenie', 'luty', 'latać_lato_rok', 'regulacja', 'bank', 'terytorialny', 'produkt', 'u', 'teraz', 'możliwy', 'wojskowy', 'poprzez', 'wieś', 'prawić_prawo', 'opłata', 'stan_stanko', 'budżetowy', 'typ', 'stanowić', 'międzynarodowy', 'porządek', 'przykład', 'rolny', 'wsparcie', 'posiedzenie', 'przedmiot', 'odrzucenie', 'wnosić', 'następnie', 'produkcja', 'około', 'wtedy', 'albo', 'kultura', 'placówka', 'szczególnie', 'oddział', 'rolnictwo', 'zawód', 'oznaczać', 'ile_ił', 'kierunek', 'przyjęty', 'obywatelski', 'wydatek', 'zagraniczny', 'uchwała', 'zostanie_zostać', 'mieć_mieść', 'przewidywać', 'wszystek', 'oferta', 'coś', 'ani_ania', 'stwierdzać', 'liczyć', 'dlaczego', 'niestety', 'wraz', 'droga_drogi', 'wstrzymać', 'premier_premiera', 'temu_ten', 'pozwolić', 'ustawa_ustawić', 'stan_stanowić_stanowy', 'pragnąć', 'obiekt', 'historia', 'wewnętrzny', 'przedsiębiorca', 'wynagrodzenie', 'przedstawiciel', 'kobieta', 'najwyższy', 'wcześniej', 'studio_studium', 'przedmiotowy', 'zatrudnienie', 'konieczność', 'wojewódzki', 'cel_cela', 'przedstawić', 'zwrócić', 'prezydent', 'policja', 'podnieść', 'przedstawiony', 'najlepszy', 'ograniczenie', 'konstytucja', 'kodeks', 'polak', 'stać_stanie_stać_stoi', 'odbyć', 'głosować', 'największy', 'trakt', 'utrzymanie', 'wziąć', 'przeciw_przeciwić', 'wątpliwość', 'analiza', 'finansowanie', 'światowy', 'zawsze', 'zarząd', 'procedura', 'sprzedaż', 'konstytucyjny', 'zrobić', 'materiał', 'edukacja', 'lokalny', 'uprawnienie', 'informować', 'ciąg', 'medyczny', 'poselski', 'jakość', 'wystąpienie', 'efekt', 'język', 'nauczyciel', 'regionalny', 'pełny', 'skład', 'przecież', 'rada', 'akcja', 'ktoś', 'podkreślić', 'zechcieć', 'siła', 'nadzór', 'szczególny', 'naprawdę', 'agencja', 'mimo', 'wynosić', 'interes', 'akt', 'spowodować', 'przeznaczony', 'uczeń', 'resort', 'niezbędny', 'zacząć', 'uzyskanie', 'reforma', 'rola', 'warta_warto', 'dziś', 'wykonywanie', 'transport', 'źródło', 'powiat', 'uniwersytet', 'znaczenie', 'moment', 'dać', 'dany', 'igrzysko', 'przygotowanie', 'pkt', 'zająć', 'kredyt', 'gdyż', 'przycisk', 'infrastruktura', 'realizowany', 'kilku', 'wypadek', 'ośrodek', 'rp', 'zwiększenie', 'lekarz', 'brać', 'mieszkanie', 'struktura', 'wyłącznie', 'tzw.', 'dopiero', 'nastąpić', 'wspólny', 'dostęp', 'trybunał', 'musieć_musić', 'założenie', 'administracyjny', 'godzina', 'trzeci', 'fizyczny', 'zakończenie', 'głównie', 'proponowany', 'wśród', 'zakończyć', 'oba', 'następny', 'planowany', 'platforma', 'prawie_prawo', 'specjalny', 'najbardziej', 'trudny', 'zajmować', 'zagrożenie', 'dyrektor', 'mający', 'zdobyć', 'dawać', 'pracodawca', 'kolejowy', 'nacisnąć', 'gra', 'film', 'społeczeństwo', 'dzienny', 'mecz', 'ilość', 'odpowiedzialność', 'sa', 'cywilny', 'wynikający', 'wprowadzić', 'partia', 'zapewnienie', 'pozostały', 'niepełnosprawny', 'etap', 'sieć', 'pomiędzy', 'emerytura', 'debata', 'parlament', 'zawierać', 'towar', 'naukowy', 'prostu', 'pozycja', 'wolność', 'energia', 'wydział', 'nic', 'samochód', 'pojazd', 'ogólny', 'wystąpić', 'musieć', 'szansa', 'całość', 'służyć', 'zmienić', 'ogół', 'generalny', 'poseł_posłowie', 'robić', 'czytanie', 'nikt', 'biuro', 'młody', 'kryterium', 'wejście', 'cztery', 'szkolenie', 'ekonomiczny', 'trwać', 'konsekwencja', 'klient', 'poseł_posłać', 'dotychczasowy', 'doświadczenie', 'karny', 'szpital', 'region', 'znaczny', 'linia', 'demokratyczny', 'likwidacja', 'wykonanie', 'sposób', 'drogowy', 'wojsko', 'pacjent', 'ziemia', 'rodzinny', 'drużyna', 'kompetencja', 'sezon', 'ludowy', 'złoty', 'chyba', 'wówczas', 'centrum', 'obrona', 'nadziać_nadzieja', 'rzeczywiście', 'gospodarstwo', 'wiedza', 'techniczny', 'przyczyna', 'najważniejszy', 'wnioskodawca', 'odbywać', 'wskazywać', 'reprezentacja', 'zwłaszcza', 'zawodnik', 'konkretny', 'składać', 'tryb_tryba', 'zgoda', 'niemiecki', 'organizacyjny', 'poprawa', 'walka', 'rolnik', 'uznać', 'pozwalać', 'zaś', 'oświadczenie', 'wszelki', 'znany', 'być_bądź', 'tworzenie', 'olimpijski', 'jedyny', 'widzieć', 'indywidualny', 'sportowy', 'coraz', 'treść', 'powszechny', 'obrót', 'dzisiejszy', 'obejmować', 'dużo', 'prawda', 'inicjatywa', 'powodować', 'szczegółowy', 'konkurs', 'wykorzystanie', 'potrzebny', 'składka', 'on_ten', 'miejski', 'moc', 'mniejszość', 'ukończyć', 'siebie', 'tydzień', 'znacznie', 'pkp', 'zwracać', 'aż', 'pozostawać', 'zostany_zostać', 'liga', 'stwierdzić', 'lecz_leczo_leczyć', 'czynność', 'producent', 'stosowanie', 'młodzież', 'woda', 'dziedzina', 'otóż', 'zgłosić', 'własność', 'słowo', 'szczególność', 'korzystać', 'budowlany', 'dotacja', 'zobowiązanie', 'uczestniczyć', 'bieżący', 'uczelnia', 'sektor', 'co_czego', 'podjęcie', 'wymiar', 'pamiętać', 'wymieniony', 'podobny', 'mieszkaniowy', 'kategoria', 'uprzejmie', 'prywatny', 'klasa', 'odniesienie', 'właściciel', 'gdyby', 'liczny', 'grunt', 'artykuł', 'tyle_tył', 'przyszłość', 'podlegać', 'samorządowy', 'straż', 'zajęcie', 'urządzenie', 'dokonać', 'handlowy', 'poinformować', 'ogromny', 'określać', 'uzasadnienie', 'wyborczy', 'mowa', 'poz.', 'poprzedni', 'choroba', 'zbyt', 'pojawić', 'medal', 'mniej', 'nadal', 'powiatowy', 'zachowanie', 'będący', 'centralny', 'sala', 'pismo', 'choć', 'kadencja', 'kolej', 'połowa_połów', 'legislacyjny', 'armia', 'podatnik', 'konieczny', 'interpelacja', 'mld', 'odpowiadać', 'minuta', 'zastosowanie', 'szereg', 'wydanie', 'sukces', 'podejmować', 'dostępny', 'ustawowy', 'postać', 'lewica', 'ustalenie', 'rozumieć', 'zwolnienie', 'okazja', 'wolny', 'zabezpieczenie', 'doprowadzić', 'śmierć', 'zauważyć', 'raz_razem', 'działać', 'sport', 'miejscowość', 'lepszy', 'stały', 'pewno', 'sztuka', 'funkcjonować', 'instytut', 'niemcy_niemiec', 'zapewnić', 'zainteresowany', 'rozpatrzenie', 'tysiąc', 'porozumienie', 'komitet', 'wybrany', 'najbliższy', 'poważny', 'pełnia_pełnić_pełny', 'stosowany', 'turniej', 'rodzic', 'prowadzący', 'przygotowany', 'ponownie', 'objęty', 'mniejszy', 'dodatkowo', 'bezpośredni', 'rynek_rynka', 'wydany', 'celny', 'prywatyzacja', 'skarbowy', 'las', 'zamówienie', 'zamierzać', 'układ', 'euro', 'ewentualny', 'obsługa', 'złożyć', 'wielkość', 'umowa_umówić', 'zależeć', 'przeprowadzenie', 'tj.', 'omawiany', 'uczestnik', 'żołnierz', 'udać', 'lek', 'sądowy', 'przypomnieć', 'wodny', 'zarządzanie', 'przemysł', 'lokal', 'nigdy', 'pozytywny', 'stronnictwo', 'człowiek_lud_ludzie', 'bezpośrednio', 'otwarty', 'zjednoczony', 'dofinansowanie', 'emerytalny', 'określenie', 'łącznie', 'inwestycyjny', 'wyjaśnienie', 'wskazany', 'jakikolwiek', 'internetowy', 'unijny', 'szkolny', 'przedstawiać', 'odpowiedzieć', 'wersja', 'wymóg', 'dochodowy', 'wskazać', 'powołany', 'zasadniczy', 'tworzyć', 'powierzchnia', '1.', 'konsultacja', 'zjawisko', 'trudno', 'odcinek', 'standard', 'dość', 'dotychczas', 'lista', 'leczenie', 'myśl_myśleć', 'rolniczy', 'niewielki', 'góra', 'wyżej', 'sojusz', 'regulamin', 'kształcenie', 'średni', 'por_pora', 'najmniej', 'socjalny', 'nagroda', 'szkoda', 'list_lista', 'wprowadzony', 'sędzia']\n"
          ]
        }
      ],
      "source": [
        "print(words)"
      ],
      "id": "amvXZukO1Qxa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "8befda60dc9e42f1ac0d3c112d2627dc",
            "9774efbdb15c4679b3caa22090998030"
          ]
        },
        "id": "t_SMg8Jx1Qxa",
        "outputId": "01922b0d-53d1-423b-9956-17a8f891c6f5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8befda60dc9e42f1ac0d3c112d2627dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9774efbdb15c4679b3caa22090998030",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "create_new_corpora(words, \"task3_polish_lower.txt\", \"task3_polish_upper.txt\", \"task3_append_corpora.txt\")"
      ],
      "id": "t_SMg8Jx1Qxa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ611QqD1Qxb",
        "outputId": "375fb464-7c1a-4b5c-cee3-4f001b105011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #0 start\n",
            "Epoch #0 end\n",
            "Epoch #1 start\n",
            "Epoch #1 end\n",
            "Epoch #2 start\n",
            "Epoch #2 end\n",
            "Epoch #3 start\n",
            "Epoch #3 end\n",
            "Epoch #4 start\n",
            "Epoch #4 end\n"
          ]
        }
      ],
      "source": [
        "model = Word2Vec(corpus_file=\"task3_mixed_corpora.txt\", vector_size=100, window=5, min_count=1, workers=4, callbacks=[EpochLogger()])\n",
        "model.save(\"task3.model\")"
      ],
      "id": "IQ611QqD1Qxb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf9b5MAY1Qxb"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec.load(\"task3.model\")"
      ],
      "id": "hf9b5MAY1Qxb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2EOMb_a1Qxc",
        "outputId": "4baadb4a-5ace-418a-9f5c-508ae7de7c0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WORD: bo\n",
            "    ponieważ 0.8838642835617065\n",
            "    ale 0.8804209232330322\n",
            "    choć 0.8241131901741028\n",
            "    oczywiście 0.8039419651031494\n",
            "    natomiast 0.7966280579566956\n",
            "    gdyż 0.790541410446167\n",
            "    niestety 0.787319004535675\n",
            "    żeby 0.7857784628868103\n",
            "    że 0.7715550065040588\n",
            "    który 0.768311619758606\n",
            "\n",
            "WORD: dwa\n",
            "    trzy 0.9646552801132202\n",
            "    cztery 0.9296600222587585\n",
            "    kilka 0.8342490196228027\n",
            "    kilku 0.7948956489562988\n",
            "    kolejny 0.7837705612182617\n",
            "    jeden 0.7439212799072266\n",
            "    następny 0.7108233571052551\n",
            "    oba 0.6999877095222473\n",
            "    trzeci 0.6966206431388855\n",
            "    pierwszy 0.6923424601554871\n",
            "\n",
            "WORD: prawo\n",
            "    prawo_prawy 0.8459263443946838\n",
            "    uprawnienie 0.6964763402938843\n",
            "    prawić_prawo 0.6592034101486206\n",
            "    przepis 0.6454287767410278\n",
            "    zasada 0.6298079490661621\n",
            "    prawie_prawo 0.6043374538421631\n",
            "    konstytucja 0.5995500087738037\n",
            "    obowiązek 0.5932927131652832\n",
            "    możliwość 0.5709974765777588\n",
            "    wymiar 0.568674623966217\n",
            "\n",
            "WORD: uwaga\n",
            "    wzgląd 0.6882945895195007\n",
            "    wskazywać 0.5804901123046875\n",
            "    fakt 0.5510799884796143\n",
            "    przykład 0.5459020733833313\n",
            "    wskazać 0.5265064835548401\n",
            "    rozwaga 0.5186505913734436\n",
            "    wpływ 0.5181295871734619\n",
            "    wynikać 0.5172081589698792\n",
            "    podkreślić 0.5146933197975159\n",
            "    UWAGE 0.514495849609375\n",
            "\n",
            "WORD: każdy\n",
            "    wszystki 0.7042164206504822\n",
            "    zawsze 0.6943948864936829\n",
            "    dany 0.6933411359786987\n",
            "    ten 0.6877288818359375\n",
            "    cały 0.6699705123901367\n",
            "    jeden_jedny 0.6567447781562805\n",
            "    jeden 0.6487341523170471\n",
            "    sam 0.6443619728088379\n",
            "    dwa 0.6347349882125854\n",
            "    taki 0.6299235224723816\n",
            "\n",
            "WORD: informacja\n",
            "    dane_dany 0.822982668876648\n",
            "    wyjaśnienie 0.7311845421791077\n",
            "    wiedza 0.7099007368087769\n",
            "    dokument 0.6911982893943787\n",
            "    opinia 0.6702457070350647\n",
            "    materiał 0.6538897752761841\n",
            "    odpowiedź 0.6413608193397522\n",
            "    oświadczenie 0.6093002557754517\n",
            "    sprawozdanie 0.5894593000411987\n",
            "    wniosek 0.5704492330551147\n",
            "\n",
            "WORD: problem\n",
            "    kwestia 0.753607451915741\n",
            "    sytuacja 0.6378663182258606\n",
            "    zjawisko 0.6261897087097168\n",
            "    przyczyna 0.626012921333313\n",
            "    potrzeba 0.6249551177024841\n",
            "    sprawa 0.6231482028961182\n",
            "    wątpliwość 0.6097608208656311\n",
            "    zagrożenie 0.6000239253044128\n",
            "    konieczność 0.5912863612174988\n",
            "    możliwość 0.5807718634605408\n",
            "\n",
            "WORD: przypadek\n",
            "    wypadek 0.7993788719177246\n",
            "    sytuacja 0.6891156435012817\n",
            "    forma 0.6645535826683044\n",
            "    postać 0.6601422429084778\n",
            "    moment 0.653897225856781\n",
            "    polska 0.6302921175956726\n",
            "    konsekwencja 0.6285999417304993\n",
            "    sprawa 0.6168686747550964\n",
            "    szczególność 0.6156145930290222\n",
            "    ogół 0.6117269992828369\n",
            "\n",
            "WORD: pod\n",
            "    bez_beza 0.6122816205024719\n",
            "    na 0.6088423728942871\n",
            "    nad 0.550130307674408\n",
            "    w 0.5171324610710144\n",
            "    i 0.5141438245773315\n",
            "    z 0.5067969560623169\n",
            "    ze 0.4888175129890442\n",
            "    przykuwać 0.48613664507865906\n",
            "    wówczas 0.4821159541606903\n",
            "    poza 0.4744664430618286\n",
            "\n",
            "WORD: jeden_jedny\n",
            "    jeden 0.8839012980461121\n",
            "    dwa 0.680482804775238\n",
            "    drugi 0.6732490062713623\n",
            "    trzy 0.6683590412139893\n",
            "    każdy 0.6567448973655701\n",
            "    ten_tychy 0.6558781266212463\n",
            "    najważniejszy 0.6513598561286926\n",
            "    kolej 0.6507844924926758\n",
            "    ten 0.6360471844673157\n",
            "    trzeci 0.6202160120010376\n",
            "\n",
            "WORD: prosić\n",
            "    chcieć 0.5774087309837341\n",
            "    wnosić 0.5491566061973572\n",
            "    POPROSIĆ 0.5375527143478394\n",
            "    PROSZĄC 0.5311712622642517\n",
            "    PROSZE 0.5246458053588867\n",
            "    poprosić 0.520652711391449\n",
            "    PROSIĆ 0.5153210759162903\n",
            "    pytanie 0.5040512681007385\n",
            "    mówić 0.5020158886909485\n",
            "    poseł_posłowie 0.4985344409942627\n",
            "\n",
            "WORD: klub\n",
            "    parlamentarny 0.6792619228363037\n",
            "    stronnictwo 0.6521687507629395\n",
            "    lewica 0.6424173712730408\n",
            "    sojusz 0.6291405558586121\n",
            "    partia 0.6244415044784546\n",
            "    poselski 0.6193670034408569\n",
            "    poseł_posłowie 0.6026397347450256\n",
            "    rząd 0.5869213938713074\n",
            "    liga 0.5807839035987854\n",
            "    skl 0.5798102617263794\n",
            "\n",
            "WORD: kraj\n",
            "    region 0.7852795124053955\n",
            "    polska 0.7744179368019104\n",
            "    europ_europa 0.7681384682655334\n",
            "    polska_polski 0.6706784963607788\n",
            "    świat 0.666368305683136\n",
            "    państwo 0.6618809700012207\n",
            "    województwo 0.6088120341300964\n",
            "    obszar 0.6005127429962158\n",
            "    miasto 0.596979558467865\n",
            "    dziedzina 0.5829102396965027\n",
            "\n",
            "WORD: prawo_prawy\n",
            "    prawo 0.8459262251853943\n",
            "    prawić_prawo 0.7537063360214233\n",
            "    uprawnienie 0.7030707001686096\n",
            "    kodeks 0.6608678698539734\n",
            "    zasada 0.6546062231063843\n",
            "    prawny 0.6501159071922302\n",
            "    przepis 0.6458958387374878\n",
            "    konstytucja 0.6212753653526306\n",
            "    wymiar 0.6084909439086914\n",
            "    prawie_prawo 0.6033256649971008\n",
            "\n",
            "WORD: dziecko\n",
            "    uczeń 0.8279701471328735\n",
            "    rodzic 0.8075916171073914\n",
            "    kobieta 0.7787886261940002\n",
            "    pacjent 0.7472794055938721\n",
            "    osoba 0.7354639768600464\n",
            "    rodzina 0.7156305909156799\n",
            "    człowiek 0.6818110942840576\n",
            "    młodzież 0.6799629926681519\n",
            "    nauczyciel 0.6695115566253662\n",
            "    polak 0.6406400203704834\n",
            "\n",
            "WORD: jeszcze\n",
            "    już 0.8083193898200989\n",
            "    teraz 0.7502706050872803\n",
            "    kolejny 0.689173698425293\n",
            "    tylko 0.6766611933708191\n",
            "    jednak 0.6749487519264221\n",
            "    ten 0.6617899537086487\n",
            "    nawet 0.646861732006073\n",
            "    oczywiście 0.6436496376991272\n",
            "    dzisiaj 0.6434656381607056\n",
            "    kilka 0.6342188715934753\n",
            "\n",
            "WORD: program\n",
            "    plan 0.7837169170379639\n",
            "    inwestycja 0.6967125535011292\n",
            "    system 0.6492828130722046\n",
            "    zadanie 0.6431535482406616\n",
            "    projekt 0.6426019668579102\n",
            "    założenie 0.6133991479873657\n",
            "    proces 0.6126094460487366\n",
            "    sieć 0.6119979023933411\n",
            "    inicjatywa 0.5955522656440735\n",
            "    budżet 0.5951470732688904\n",
            "\n",
            "WORD: drugi\n",
            "    trzeci 0.8688045144081116\n",
            "    pierwszy 0.8340761065483093\n",
            "    kolejny 0.739925742149353\n",
            "    dwa 0.6911385655403137\n",
            "    jeden_jedny 0.6732491850852966\n",
            "    trzy 0.6549890041351318\n",
            "    ostatni 0.6500986814498901\n",
            "    następny 0.6481701731681824\n",
            "    ii 0.645820677280426\n",
            "    oba 0.6427094340324402\n",
            "\n",
            "WORD: pani\n",
            "    pan 0.8228870034217834\n",
            "    pan_pani 0.7226663827896118\n",
            "    sprawiedliwość 0.6104394793510437\n",
            "    wnioskodawca 0.5704825520515442\n",
            "    finanse 0.5680046081542969\n",
            "    kaczmarek_kaczmarka 0.5455788373947144\n",
            "    tutaj 0.540267288684845\n",
            "    ziobro 0.5225992798805237\n",
            "    BALICKI 0.5214112997055054\n",
            "    WĄSACZ 0.5210253000259399\n",
            "\n",
            "WORD: zasada\n",
            "    przepis 0.7417954206466675\n",
            "    warunek 0.6932066082954407\n",
            "    procedura 0.6876888871192932\n",
            "    sposób 0.6858596801757812\n",
            "    zapis 0.6703615188598633\n",
            "    kryterium 0.665814995765686\n",
            "    wymóg 0.6654350757598877\n",
            "    prawo_prawy 0.6546063423156738\n",
            "    system 0.6466869115829468\n",
            "    forma 0.6412786841392517\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example_words = words[100:120]\n",
        "\n",
        "for w0 in example_words:\n",
        "    print ('WORD:', w0)\n",
        "    for w, v in model.wv.most_similar(w0, topn=10):\n",
        "        print ('   ', w, v)\n",
        "    print ()"
      ],
      "id": "k2EOMb_a1Qxc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "357a8eac9137461b9ef3ef1c8dec8a9c"
          ]
        },
        "id": "MgvjWPyZ1Qxc",
        "outputId": "1b6b4e66-e198-40b0-d80c-6f5f4f52e7f7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "357a8eac9137461b9ef3ef1c8dec8a9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.29150051446646585"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_, v = calc_score_for_d(words, model, topn=1000)\n",
        "v"
      ],
      "id": "MgvjWPyZ1Qxc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "394af87ac3794fdc8d5e27d31ed0fab4"
          ]
        },
        "id": "PRdqLcfJ1Qxd",
        "outputId": "2354dcea-9bff-46f5-a26f-2b19720c47fe"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "394af87ac3794fdc8d5e27d31ed0fab4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.42028420553812745"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_, v = calc_score_for_outside_d(words, allwords, model, n_samples=1000, topn=1000)\n",
        "v"
      ],
      "id": "PRdqLcfJ1Qxd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKxtYtpF1Qxd",
        "outputId": "66261e42-fa57-4b51-8f5b-bc8fafd9f3dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #0 start\n",
            "Epoch #0 end\n",
            "Epoch #1 start\n",
            "Epoch #1 end\n",
            "Epoch #2 start\n",
            "Epoch #2 end\n",
            "Epoch #3 start\n",
            "Epoch #3 end\n",
            "Epoch #4 start\n",
            "Epoch #4 end\n"
          ]
        }
      ],
      "source": [
        "model = Word2Vec(corpus_file=\"task3_append_corpora.txt\", vector_size=100, window=5, min_count=1, workers=4, callbacks=[EpochLogger()])\n",
        "model.save(\"task3_append.model\")"
      ],
      "id": "zKxtYtpF1Qxd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YluwZILG1Qxe"
      },
      "outputs": [],
      "source": [
        "# model = Word2Vec.load(\"task3_append.model\")"
      ],
      "id": "YluwZILG1Qxe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-7VpnsG1Qxe",
        "outputId": "d4cb709d-bddd-448a-ab36-7f8c0f91d5b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WORD: bo\n",
            "    ponieważ 0.8948083519935608\n",
            "    ale 0.8685998320579529\n",
            "    choć 0.8286816477775574\n",
            "    oczywiście 0.8150435090065002\n",
            "    niestety 0.8033995032310486\n",
            "    natomiast 0.7966384291648865\n",
            "    gdyż 0.793933629989624\n",
            "    że 0.779845118522644\n",
            "    który 0.7764931917190552\n",
            "    otóż 0.7750731706619263\n",
            "\n",
            "WORD: dwa\n",
            "    trzy 0.9663321375846863\n",
            "    cztery 0.9291338324546814\n",
            "    kilka 0.8316941857337952\n",
            "    kilku 0.8042236566543579\n",
            "    kolejny 0.759882390499115\n",
            "    jeden 0.7408120036125183\n",
            "    następny 0.7153948545455933\n",
            "    jeden_jedny 0.7129083871841431\n",
            "    ostatni 0.6951188445091248\n",
            "    oba 0.6892820000648499\n",
            "\n",
            "WORD: prawo\n",
            "    prawo_prawy 0.852893054485321\n",
            "    uprawnienie 0.7194205522537231\n",
            "    prawić_prawo 0.6411972045898438\n",
            "    przepis 0.6359310746192932\n",
            "    zasada 0.617060661315918\n",
            "    obowiązek 0.6042078137397766\n",
            "    kodeks 0.5901337265968323\n",
            "    konstytucja 0.5839157104492188\n",
            "    prawie_prawo 0.5768797993659973\n",
            "    możliwość 0.5655156970024109\n",
            "\n",
            "WORD: uwaga\n",
            "    wzgląd 0.6878319978713989\n",
            "    UWAGE 0.5942088961601257\n",
            "    wskazywać 0.5817933678627014\n",
            "    powód 0.5628733038902283\n",
            "    przykład 0.5587363243103027\n",
            "    fakt 0.5527440309524536\n",
            "    wynikać 0.5487743616104126\n",
            "    wskazać 0.5302592515945435\n",
            "    podkreślić 0.5249068140983582\n",
            "    zauważyć 0.5143110752105713\n",
            "\n",
            "WORD: każdy\n",
            "    zawsze 0.7183135151863098\n",
            "    dany 0.7004624605178833\n",
            "    ten 0.7003849744796753\n",
            "    wszystki 0.6937048435211182\n",
            "    cały 0.684838593006134\n",
            "    jeden_jedny 0.6699782609939575\n",
            "    sam 0.6552816033363342\n",
            "    jeden 0.6503344774246216\n",
            "    taki 0.6394153237342834\n",
            "    dwa 0.6349827647209167\n",
            "\n",
            "WORD: informacja\n",
            "    dane_dany 0.8279491662979126\n",
            "    wyjaśnienie 0.7353707551956177\n",
            "    wiedza 0.691446840763092\n",
            "    dokument 0.6857324838638306\n",
            "    opinia 0.6783943176269531\n",
            "    materiał 0.6671974658966064\n",
            "    odpowiedź 0.6582292914390564\n",
            "    sprawozdanie 0.6444183588027954\n",
            "    oświadczenie 0.6145936250686646\n",
            "    ustalenie 0.5853110551834106\n",
            "\n",
            "WORD: problem\n",
            "    kwestia 0.7542146444320679\n",
            "    zjawisko 0.6852061748504639\n",
            "    sytuacja 0.6428499221801758\n",
            "    przyczyna 0.6353718638420105\n",
            "    potrzeba 0.6267878413200378\n",
            "    wątpliwość 0.6151977777481079\n",
            "    sprawa 0.6138078570365906\n",
            "    zagrożenie 0.6116186380386353\n",
            "    element 0.5766951441764832\n",
            "    konieczność 0.5712259411811829\n",
            "\n",
            "WORD: przypadek\n",
            "    wypadek 0.7950063943862915\n",
            "    forma 0.6826463937759399\n",
            "    postać 0.6787354946136475\n",
            "    sytuacja 0.6474156379699707\n",
            "    moment 0.6471642851829529\n",
            "    polska 0.6420437097549438\n",
            "    sposobić_sposób 0.6376385688781738\n",
            "    zakres 0.6284871101379395\n",
            "    konsekwencja 0.6241018176078796\n",
            "    tryb_tryba 0.623742938041687\n",
            "\n",
            "WORD: pod\n",
            "    bez_beza 0.6094173789024353\n",
            "    na 0.5914760828018188\n",
            "    nad 0.5493177771568298\n",
            "    ze 0.5394630432128906\n",
            "    POD 0.5163994431495667\n",
            "    w 0.4869197607040405\n",
            "    i 0.48512542247772217\n",
            "    do 0.46166157722473145\n",
            "    jako 0.45490360260009766\n",
            "    z 0.4531927704811096\n",
            "\n",
            "WORD: jeden_jedny\n",
            "    jeden 0.8930580615997314\n",
            "    dwa 0.7129084467887878\n",
            "    trzy 0.6998858451843262\n",
            "    każdy 0.6699782609939575\n",
            "    drugi 0.6627345085144043\n",
            "    cztery 0.6523984670639038\n",
            "    ten 0.6257614493370056\n",
            "    najważniejszy 0.6241065859794617\n",
            "    trzeci 0.622199296951294\n",
            "    kilku 0.6207767128944397\n",
            "\n",
            "WORD: prosić\n",
            "    chcieć 0.6045763492584229\n",
            "    wnosić 0.5878780484199524\n",
            "    POPROSIĆ 0.5574423670768738\n",
            "    PROSZE 0.5543602705001831\n",
            "    poprosić 0.5523955225944519\n",
            "    pytanie 0.5430213809013367\n",
            "    PROSIĆ 0.5419682860374451\n",
            "    mówić 0.5302252769470215\n",
            "    PROSZĄC 0.515296459197998\n",
            "    trudno 0.5135126709938049\n",
            "\n",
            "WORD: klub\n",
            "    parlamentarny 0.6704272031784058\n",
            "    stronnictwo 0.6451823115348816\n",
            "    lewica 0.6095809936523438\n",
            "    partia 0.6093363761901855\n",
            "    sojusz 0.6091204881668091\n",
            "    skl 0.6010851860046387\n",
            "    poselski 0.5937977433204651\n",
            "    poseł_posłowie 0.5931819677352905\n",
            "    liga 0.5747127532958984\n",
            "    rząd 0.5641749501228333\n",
            "\n",
            "WORD: kraj\n",
            "    region 0.7720900774002075\n",
            "    polska 0.7684341669082642\n",
            "    europ_europa 0.7548875212669373\n",
            "    polska_polski 0.6834557056427002\n",
            "    świat 0.6601403951644897\n",
            "    państwo 0.6570306420326233\n",
            "    miasto 0.634522557258606\n",
            "    obszar 0.6030528545379639\n",
            "    dziedzina 0.5921079516410828\n",
            "    niemcy_niemiec 0.5824493765830994\n",
            "\n",
            "WORD: prawo_prawy\n",
            "    prawo 0.8528929948806763\n",
            "    prawić_prawo 0.7752546668052673\n",
            "    uprawnienie 0.7164943814277649\n",
            "    prawny 0.686119019985199\n",
            "    przepis 0.6635832786560059\n",
            "    kodeks 0.6607034802436829\n",
            "    zasada 0.6376098990440369\n",
            "    konstytucja 0.6266560554504395\n",
            "    ustawowy 0.6207004189491272\n",
            "    prawie_prawo 0.6027849316596985\n",
            "\n",
            "WORD: dziecko\n",
            "    uczeń 0.8258121609687805\n",
            "    rodzic 0.8095129132270813\n",
            "    kobieta 0.7729438543319702\n",
            "    pacjent 0.7528409361839294\n",
            "    osoba 0.7418556213378906\n",
            "    rodzina 0.7134936451911926\n",
            "    człowiek 0.7016161680221558\n",
            "    młodzież 0.6901442408561707\n",
            "    nauczyciel 0.6838085055351257\n",
            "    polak 0.6767835021018982\n",
            "\n",
            "WORD: jeszcze\n",
            "    już 0.8146871328353882\n",
            "    tylko 0.7234514951705933\n",
            "    teraz 0.7053623199462891\n",
            "    kolejny 0.6638228893280029\n",
            "    nawet 0.6627641916275024\n",
            "    jednak 0.6528173089027405\n",
            "    ten 0.6495412588119507\n",
            "    tenże_też 0.6418676972389221\n",
            "    rzeczywiście 0.6349982023239136\n",
            "    niestety 0.6318802833557129\n",
            "\n",
            "WORD: program\n",
            "    plan 0.7582883238792419\n",
            "    inwestycja 0.6888070106506348\n",
            "    projekt 0.6526266932487488\n",
            "    system 0.651794970035553\n",
            "    zadanie 0.6425261497497559\n",
            "    proces 0.6359697580337524\n",
            "    sieć 0.6119815111160278\n",
            "    założenie 0.6013762950897217\n",
            "    potrzeba 0.5988085865974426\n",
            "    działanie 0.5908627510070801\n",
            "\n",
            "WORD: drugi\n",
            "    trzeci 0.8634014129638672\n",
            "    pierwszy 0.8328108787536621\n",
            "    kolejny 0.7510152459144592\n",
            "    dwa 0.6890010237693787\n",
            "    jeden_jedny 0.6627344489097595\n",
            "    następny 0.661506175994873\n",
            "    trzy 0.6594985723495483\n",
            "    oba 0.657863199710846\n",
            "    ostatni 0.6502799391746521\n",
            "    ii 0.6305122375488281\n",
            "\n",
            "WORD: pani\n",
            "    pan 0.8494632244110107\n",
            "    pan_pani 0.7028950452804565\n",
            "    sprawiedliwość 0.6314049959182739\n",
            "    finanse 0.57611483335495\n",
            "    zdrojewski 0.5443395376205444\n",
            "    wnioskodawca 0.5377974510192871\n",
            "    miąć_on 0.5364229083061218\n",
            "    mój 0.5348051190376282\n",
            "    KACZMAREK_KACZMARKA 0.5298054218292236\n",
            "    tutaj 0.5284971594810486\n",
            "\n",
            "WORD: zasada\n",
            "    przepis 0.7409439086914062\n",
            "    sposób 0.6950505375862122\n",
            "    warunek 0.6908465027809143\n",
            "    procedura 0.686846137046814\n",
            "    kryterium 0.6781522631645203\n",
            "    wymóg 0.6680372953414917\n",
            "    standard 0.6644245982170105\n",
            "    zapis 0.6594333052635193\n",
            "    system 0.6473671793937683\n",
            "    prawo_prawy 0.6376097798347473\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example_words = words[100:120]\n",
        "\n",
        "for w0 in example_words:\n",
        "    print ('WORD:', w0)\n",
        "    for w, v in model.wv.most_similar(w0, topn=10):\n",
        "        print ('   ', w, v)\n",
        "    print ()"
      ],
      "id": "9-7VpnsG1Qxe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "0d4a69d5da984262b6b696fb49882fc9"
          ]
        },
        "id": "FFAJEzF31Qxe",
        "outputId": "305bc511-0cb3-42de-c250-2ce6c15c1ef7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d4a69d5da984262b6b696fb49882fc9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.33330897538816273"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_, v = calc_score_for_d(words, model, topn=1000)\n",
        "v"
      ],
      "id": "FFAJEzF31Qxe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "d625c4a8b1ef479b9e9ec7e73b801120"
          ]
        },
        "id": "mQQLx__S1Qxe",
        "outputId": "1ca3ff7a-3309-4298-c2d6-02611527ede9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d625c4a8b1ef479b9e9ec7e73b801120",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.4152741299820696"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_, v = calc_score_for_outside_d(words, allwords, model, n_samples=1000, topn=1000)\n",
        "v"
      ],
      "id": "mQQLx__S1Qxe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR6e40df1Qxf"
      },
      "source": [
        "# Task 4 (4 points)\n",
        "\n",
        "In this task you are asked to do two things:\n",
        "1. compare the embeddings computed on small corpus (like Brown Corpus , see: <https://en.wikipedia.org/wiki/Brown_Corpus>) with the ones coming from Google News Corpus\n",
        "2. Try to use other resourses like WordNet to enrich to corpus, and obtain better embeddings\n",
        "\n",
        "You can use the following code snippets:\n",
        "\n",
        "```python\n",
        "# printing tokenized Brown Corpora\n",
        "from nltk.corpus import brown\n",
        "for s in brown.sents():\n",
        "    print(*s)\n",
        "    \n",
        "#iterating over all synsets in WordNet\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "for synset_type in 'avrns': # n == noun, v == verb, ...\n",
        "    for synset in list(wn.all_synsets(synset_type)))[:10]:\n",
        "        print (synset.definition())\n",
        "        print (synset.examples())\n",
        "        print ([lem.name() for lem in synset.lemmas()])\n",
        "        print (synset.hyperonims()) # nodes 1 level up in ontology\n",
        "        \n",
        "# loading model and compute cosine similarity between words\n",
        "\n",
        "model = Word2Vec.load('models/w2v.wordnet5.model') \n",
        "print (model.wv.similarity('dog', 'cat'))\n",
        "```\n",
        "\n",
        "Embeddings will be tested using WordSim-353 dataset, the code showing the quality is in the cell below. Prepare the following corpora:\n",
        "1. Tokenized Brown Corpora\n",
        "2. Definitions and examples from Princeton WordNet\n",
        "3. (1) and (2) together\n",
        "4. (3) enriched with pseudosentences containing (a subset) of WordNet knowledge (such as 'tiger is a carnivore')\n",
        "\n",
        "Train 4 Word2Vec models, and raport Spearman correletion between similarities based on your vectors, and similarities based on human judgements.\n",
        "\n"
      ],
      "id": "RR6e40df1Qxf"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k77hbFaJpMAp",
        "outputId": "af2c3d6c-ba36-48fc-d465-6a97180c3e0a"
      },
      "id": "k77hbFaJpMAp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (6.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google News Corpus"
      ],
      "metadata": {
        "id": "TVpiPAkHkdPk"
      },
      "id": "TVpiPAkHkdPk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "W3SEWlon1Qxf"
      },
      "outputs": [],
      "source": [
        "# Code for computing correlation between W2V similarity, and human judgements\n",
        "\n",
        "import gensim.downloader\n",
        "from scipy.stats import spearmanr\n",
        "gn = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "id": "W3SEWlon1Qxf"
    },
    {
      "cell_type": "code",
      "source": [
        "for similarity_type in ['relatedness', 'similarity']:\n",
        "    ws353 = []\n",
        "    vals = []\n",
        "    ys = []\n",
        "    for x in open(f'task4_wordsim_{similarity_type}_goldstandard.txt'): \n",
        "        a,b,val = x.split()\n",
        "        val = float(val)\n",
        "        ws353.append( (a,b,val))\n",
        "        vals.append(val)\n",
        "        ys.append(gn.similarity(a, b))\n",
        "    # spearmanr returns 2 vallues: correlation and pval. pval should be close to zero\n",
        "    \n",
        "    print(similarity_type + ':', spearmanr(vals, ys))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luqg707vmXAB",
        "outputId": "01c9567f-4a2f-40c9-9244-9ba8c516138d"
      },
      "id": "luqg707vmXAB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relatedness: SpearmanrResult(correlation=0.6354514099606292, pvalue=6.623359420760354e-30)\n",
            "similarity: SpearmanrResult(correlation=0.7717239276951675, pvalue=2.2385731613138314e-41)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Brown Corpora"
      ],
      "metadata": {
        "id": "wfMQwS6tktUL"
      },
      "id": "wfMQwS6tktUL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53FUkqKU1Qxf",
        "outputId": "d2531c5d-7edf-4986-930e-c150f6aff6e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "# for s in brown.sents()[:10]:\n",
        "#     print(s)\n",
        "\n",
        "with open(\"brown_sentences.txt\", \"w\", encoding=\"utf8\") as brown_file:\n",
        "    for s in brown.sents():\n",
        "        brown_file.write(\" \".join(s)+\"\\n\")"
      ],
      "id": "53FUkqKU1Qxf"
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(corpus_file=\"brown_sentences.txt\", vector_size=100, window=5, min_count=1, workers=4, callbacks=[EpochLogger()])\n",
        "model.save(\"brown_sentences.model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keyXTv3anxac",
        "outputId": "cc78ef7e-0d05-4b69-f9c9-ce866663dd14"
      },
      "id": "keyXTv3anxac",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #0 start\n",
            "Epoch #0 end\n",
            "Epoch #1 start\n",
            "Epoch #1 end\n",
            "Epoch #2 start\n",
            "Epoch #2 end\n",
            "Epoch #3 start\n",
            "Epoch #3 end\n",
            "Epoch #4 start\n",
            "Epoch #4 end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec.load(\"brown_sentences.model\")"
      ],
      "metadata": {
        "id": "y05eweakn2FW"
      },
      "id": "y05eweakn2FW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What should we return then? I decided that we will skip examples with words unknown to our model\n",
        "model.wv.similarity('OPEC', 'country')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "J7W5rdEwrqvZ",
        "outputId": "944016ee-612a-4754-85bc-0c49a3742947"
      },
      "id": "J7W5rdEwrqvZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-22816fe0d7c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# What should we return then? I decided that we will skip examples with words unknown to our model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'OPEC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'country'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \"\"\"\n\u001b[0;32m-> 1238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \"\"\"\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \"\"\"\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'OPEC' not present\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "for similarity_type in ['relatedness', 'similarity']:\n",
        "    ws353 = []\n",
        "    vals = []\n",
        "    ys = []\n",
        "    for x in open(f'task4_wordsim_{similarity_type}_goldstandard.txt'): \n",
        "        a,b,val = x.split()\n",
        "        val = float(val)\n",
        "        ws353.append( (a,b,val))\n",
        "        try:\n",
        "            ys.append(model.wv.similarity(a, b))\n",
        "            vals.append(val)\n",
        "        except KeyError:\n",
        "            pass\n",
        "    # spearmanr returns 2 vallues: correlation and pval. pval should be close to zero\n",
        "    \n",
        "    print(similarity_type + ':', spearmanr(vals, ys))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPKhek3wn2Ip",
        "outputId": "5e81f0bd-14cc-43f3-c0d9-2249a130ba0b"
      },
      "id": "jPKhek3wn2Ip",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relatedness: SpearmanrResult(correlation=0.08412994886846215, pvalue=0.20168258853600937)\n",
            "similarity: SpearmanrResult(correlation=0.10166031487280731, pvalue=0.17207591679340606)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wordnet"
      ],
      "metadata": {
        "id": "egYtoeaBkxTr"
      },
      "id": "egYtoeaBkxTr"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')\n",
        "\n",
        "with open(\"wordnet_sentences.txt\", \"w\", encoding=\"utf8\") as wordnet_file:\n",
        "    for synset_type in 'avrns': # n == noun, v == verb, ...\n",
        "        for synset in wn.all_synsets(synset_type):\n",
        "            wordnet_file.write(synset.definition()+\"\\n\")\n",
        "            for example in synset.examples():\n",
        "                wordnet_file.write(example+\" \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNO-LgRM2TSs",
        "outputId": "d1323e9a-76e5-4d65-bd66-422b7b23e4a4"
      },
      "id": "XNO-LgRM2TSs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(corpus_file=\"wordnet_sentences.txt\", vector_size=100, window=5, min_count=1, workers=4, callbacks=[EpochLogger()])\n",
        "model.save(\"wordnet_sentences.model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEpWu9xIsoBJ",
        "outputId": "c1fb976a-6d93-42c9-ec86-b3a5d7614fd2"
      },
      "id": "KEpWu9xIsoBJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #0 start\n",
            "Epoch #0 end\n",
            "Epoch #1 start\n",
            "Epoch #1 end\n",
            "Epoch #2 start\n",
            "Epoch #2 end\n",
            "Epoch #3 start\n",
            "Epoch #3 end\n",
            "Epoch #4 start\n",
            "Epoch #4 end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec.load(\"wordnet_sentences.model\")"
      ],
      "metadata": {
        "id": "yFFaCy5Bspuk"
      },
      "id": "yFFaCy5Bspuk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "for similarity_type in ['relatedness', 'similarity']:\n",
        "    ws353 = []\n",
        "    vals = []\n",
        "    ys = []\n",
        "    for x in open(f'task4_wordsim_{similarity_type}_goldstandard.txt'): \n",
        "        a,b,val = x.split()\n",
        "        val = float(val)\n",
        "        ws353.append( (a,b,val))\n",
        "        try:\n",
        "            ys.append(model.wv.similarity(a, b))\n",
        "            vals.append(val)\n",
        "        except KeyError:\n",
        "            pass\n",
        "    # spearmanr returns 2 vallues: correlation and pval. pval should be close to zero\n",
        "    \n",
        "    print(similarity_type + ':', spearmanr(vals, ys))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BSW8jVUssMu",
        "outputId": "d68a1948-6e3a-41a1-b730-fafa2b8de746"
      },
      "id": "4BSW8jVUssMu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relatedness: SpearmanrResult(correlation=0.30314952838601944, pvalue=1.0894969307572987e-06)\n",
            "similarity: SpearmanrResult(correlation=0.39139285389932144, pvalue=1.0894677233239208e-08)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Brown Corpora and Wordnet"
      ],
      "metadata": {
        "id": "3RROCTNak0GE"
      },
      "id": "3RROCTNak0GE"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "sentences = []\n",
        "with open(\"wordnet_sentences.txt\", \"r\", encoding=\"utf8\") as wordnet_file:\n",
        "    for line in wordnet_file:\n",
        "        sentences.append(line)\n",
        "\n",
        "with open(\"brown_sentences.txt\", \"r\", encoding=\"utf8\") as brown_file:\n",
        "    for line in brown_file:\n",
        "        sentences.append(line)\n",
        "\n",
        "np.random.shuffle(sentences)\n",
        "with open(\"brown_wordnet_sentences.txt\", \"w\", encoding=\"utf8\") as brown_wordnet_file:\n",
        "    for line in sentences:\n",
        "        brown_wordnet_file.write(line)"
      ],
      "metadata": {
        "id": "Puw4aB-bkLaO"
      },
      "id": "Puw4aB-bkLaO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(corpus_file=\"brown_wordnet_sentences.txt\", vector_size=100, window=5, min_count=1, workers=4, callbacks=[EpochLogger()])\n",
        "model.save(\"brown_wordnet_sentences.model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQUHAZC7xX1U",
        "outputId": "d6aaf1e6-da22-4213-b383-639df9735463"
      },
      "id": "IQUHAZC7xX1U",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #0 start\n",
            "Epoch #0 end\n",
            "Epoch #1 start\n",
            "Epoch #1 end\n",
            "Epoch #2 start\n",
            "Epoch #2 end\n",
            "Epoch #3 start\n",
            "Epoch #3 end\n",
            "Epoch #4 start\n",
            "Epoch #4 end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec.load(\"brown_wordnet_sentences.model\")"
      ],
      "metadata": {
        "id": "ZGplTwx4xkYU"
      },
      "id": "ZGplTwx4xkYU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "for similarity_type in ['relatedness', 'similarity']:\n",
        "    ws353 = []\n",
        "    vals = []\n",
        "    ys = []\n",
        "    for x in open(f'task4_wordsim_{similarity_type}_goldstandard.txt'): \n",
        "        a,b,val = x.split()\n",
        "        val = float(val)\n",
        "        ws353.append( (a,b,val))\n",
        "        try:\n",
        "            ys.append(model.wv.similarity(a, b))\n",
        "            vals.append(val)\n",
        "        except KeyError:\n",
        "            pass\n",
        "    # spearmanr returns 2 vallues: correlation and pval. pval should be close to zero\n",
        "    \n",
        "    print(similarity_type + ':', spearmanr(vals, ys))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9nm6P0Nxka1",
        "outputId": "907201ad-0c2b-4e32-947e-70c68c014697"
      },
      "id": "a9nm6P0Nxka1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relatedness: SpearmanrResult(correlation=0.25765754255958223, pvalue=3.6015207263190124e-05)\n",
            "similarity: SpearmanrResult(correlation=0.4496753545181397, pvalue=1.9004113566906157e-11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Brown Corpora and Wordnet enriched with pseudosentences containing (a subset) of WordNet knowledge (such as 'tiger is a carnivore')"
      ],
      "metadata": {
        "id": "xC56BxgQk4Op"
      },
      "id": "xC56BxgQk4Op"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')\n",
        "\n",
        "with open(\"wordnet_pseudosentences.txt\", \"w\", encoding=\"utf8\") as wordnet_file:\n",
        "    for synset_type in 'avrns': # n == noun, v == verb, ...\n",
        "        for synset in wn.all_synsets(synset_type):\n",
        "            lemmas = [lem.name() for lem in synset.lemmas()]\n",
        "            hypernyms = [str(lemma.name()) for hypernym in synset.hypernyms() for lemma in hypernym.lemmas()]\n",
        "            for lemma in lemmas:\n",
        "                for hypernym in hypernyms:\n",
        "                    wordnet_file.write(lemma + \" is \" + hypernym + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYSzTHwix-GV",
        "outputId": "dff064d1-3f00-4f01-faa8-fde0ea55a437"
      },
      "id": "JYSzTHwix-GV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')\n",
        "\n",
        "with open(\"wordnet_pseudosentences.txt\", \"w\", encoding=\"utf8\") as wordnet_file:\n",
        "    for synset_type in 'avrns': # n == noun, v == verb, ...\n",
        "        for synset in wn.all_synsets(synset_type):\n",
        "            lemmas = [lem.name() for lem in synset.lemmas()]\n",
        "            hypernyms = [str(lemma.name()) for hypernym in synset.hypernyms() for lemma in hypernym.lemmas()]\n",
        "            for lemma in lemmas:\n",
        "                for hypernym in hypernyms:\n",
        "                    wordnet_file.write(lemma + \" \" + hypernym + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqyM9hsY3KP1",
        "outputId": "4a8d46c1-2fdd-4f6d-e295-8dae46f83807"
      },
      "id": "DqyM9hsY3KP1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "sentences = []\n",
        "with open(\"wordnet_sentences.txt\", \"r\", encoding=\"utf8\") as wordnet_file:\n",
        "    for line in wordnet_file:\n",
        "        sentences.append(line)\n",
        "\n",
        "with open(\"brown_sentences.txt\", \"r\", encoding=\"utf8\") as brown_file:\n",
        "    for line in brown_file:\n",
        "        sentences.append(line)\n",
        "\n",
        "with open(\"wordnet_pseudosentences.txt\", \"r\", encoding=\"utf8\") as wordnet_pseudosentences_file:\n",
        "    for line in wordnet_pseudosentences_file:\n",
        "        sentences.append(line)\n",
        "\n",
        "np.random.shuffle(sentences)\n",
        "with open(\"brown_wordnet_pseudosentences.txt\", \"w\", encoding=\"utf8\") as brown_wordnet_file:\n",
        "    for line in sentences:\n",
        "        brown_wordnet_file.write(line)"
      ],
      "metadata": {
        "id": "yDC8Ss4zx-Is"
      },
      "id": "yDC8Ss4zx-Is",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(corpus_file=\"brown_wordnet_pseudosentences.txt\", vector_size=100, window=5, min_count=1, workers=4, callbacks=[EpochLogger()])\n",
        "model.save(\"brown_wordnet_pseudosentences.model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1O14L9X2oqE",
        "outputId": "24b0efd6-822e-4b23-e7f7-43b1561c20ae"
      },
      "id": "j1O14L9X2oqE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #0 start\n",
            "Epoch #0 end\n",
            "Epoch #1 start\n",
            "Epoch #1 end\n",
            "Epoch #2 start\n",
            "Epoch #2 end\n",
            "Epoch #3 start\n",
            "Epoch #3 end\n",
            "Epoch #4 start\n",
            "Epoch #4 end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec.load(\"brown_wordnet_pseudosentences.model\")"
      ],
      "metadata": {
        "id": "ffbcO5ap2qaO"
      },
      "id": "ffbcO5ap2qaO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "for similarity_type in ['relatedness', 'similarity']:\n",
        "    ws353 = []\n",
        "    vals = []\n",
        "    ys = []\n",
        "    for x in open(f'task4_wordsim_{similarity_type}_goldstandard.txt'): \n",
        "        a,b,val = x.split()\n",
        "        val = float(val)\n",
        "        ws353.append( (a,b,val))\n",
        "        try:\n",
        "            ys.append(model.wv.similarity(a, b))\n",
        "            vals.append(val)\n",
        "        except KeyError:\n",
        "            pass\n",
        "    # spearmanr returns 2 vallues: correlation and pval. pval should be close to zero\n",
        "    \n",
        "    print(similarity_type + ':', spearmanr(vals, ys))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD9Ujefp3Rb0",
        "outputId": "b1d56982-1a77-4f8a-8728-3ae3c9223404"
      },
      "id": "RD9Ujefp3Rb0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relatedness: SpearmanrResult(correlation=0.25167330737989807, pvalue=5.5198604348450965e-05)\n",
            "similarity: SpearmanrResult(correlation=0.47384148410064225, pvalue=9.30227478109151e-13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random code"
      ],
      "metadata": {
        "id": "QMPIAXpxx-gA"
      },
      "id": "QMPIAXpxx-gA"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')\n",
        "\n",
        "for synset_type in 'avrns': # n == noun, v == verb, ...\n",
        "    for synset in list(wn.all_synsets(synset_type))[:10]:\n",
        "        print(synset.definition())\n",
        "        print(synset.examples())\n",
        "        print([lem.name() for lem in synset.lemmas()])\n",
        "        # print([str(lemma.name()) for synset_lemma in synset.lemmas() for hypernym in synset_lemma.hypernyms() for lemma in hypernym.lemmas()])\n",
        "        print([str(lemma.name()) for hypernym in synset.hypernyms() for lemma in hypernym.lemmas()]) # nodes 1 level up in ontology\n",
        "        print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnzSrgP3GpMI",
        "outputId": "dace73af-2753-4dbe-bc56-e478b39d335d"
      },
      "id": "AnzSrgP3GpMI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "(usually followed by `to') having the necessary means or skill or know-how or authority to do something\n",
            "['able to swim', 'she was able to program her computer', 'we were at last able to buy a car', 'able to get a grant for the project']\n",
            "['able']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "(usually followed by `to') not having the necessary means or skill or know-how\n",
            "['unable to get to town without a car', 'unable to obtain funds']\n",
            "['unable']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "facing away from the axis of an organ or organism\n",
            "['the abaxial surface of a leaf is the underside or side facing away from the stem']\n",
            "['abaxial', 'dorsal']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "nearest to or facing toward the axis of an organ or organism\n",
            "['the upper side of a leaf is known as the adaxial surface']\n",
            "['adaxial', 'ventral']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "facing or on the side toward the apex\n",
            "[]\n",
            "['acroscopic']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "facing or on the side toward the base\n",
            "[]\n",
            "['basiscopic']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "especially of muscles; drawing away from the midline of the body or from an adjacent part\n",
            "[]\n",
            "['abducent', 'abducting']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "especially of muscles; bringing together or drawing toward the midline of the body or toward an adjacent part\n",
            "[]\n",
            "['adducent', 'adductive', 'adducting']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "being born or beginning\n",
            "['the nascent chicks', 'a nascent insurgency']\n",
            "['nascent']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "coming into existence\n",
            "['an emergent republic']\n",
            "['emergent', 'emerging']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "draw air into, and expel out of, the lungs\n",
            "['I can breathe better when the air is clean', 'The patient is respiring']\n",
            "['breathe', 'take_a_breath', 'respire', 'suspire']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "undergo the biomedical and metabolic processes of respiration by taking up oxygen and producing carbon monoxide\n",
            "[]\n",
            "['respire']\n",
            "['undergo']\n",
            "--------------------------------------------------\n",
            "breathe easily again, as after exertion or anxiety\n",
            "[]\n",
            "['respire']\n",
            "['breathe', 'take_a_breath', 'respire', 'suspire']\n",
            "--------------------------------------------------\n",
            "breathe with great difficulty, as when experiencing a strong emotion\n",
            "['She choked with emotion when she spoke about her deceased husband']\n",
            "['choke']\n",
            "['breathe', 'take_a_breath', 'respire', 'suspire']\n",
            "--------------------------------------------------\n",
            "breathe excessively hard and fast\n",
            "['The mountain climber started to hyperventilate']\n",
            "['hyperventilate']\n",
            "['breathe', 'take_a_breath', 'respire', 'suspire']\n",
            "--------------------------------------------------\n",
            "produce hyperventilation in\n",
            "['The nurses had to hyperventilate the patient']\n",
            "['hyperventilate']\n",
            "['treat', 'care_for']\n",
            "--------------------------------------------------\n",
            "suck in (air)\n",
            "[]\n",
            "['aspirate']\n",
            "['inhale', 'inspire', 'breathe_in']\n",
            "--------------------------------------------------\n",
            "expel gas from the stomach\n",
            "['In China it is polite to burp at the table']\n",
            "['burp', 'bubble', 'belch', 'eruct']\n",
            "['emit', 'breathe', 'pass_off']\n",
            "--------------------------------------------------\n",
            "emit or cause to move with force of effort\n",
            "['force out the air', 'force out the splinter']\n",
            "['force_out']\n",
            "['emit', 'breathe', 'pass_off']\n",
            "--------------------------------------------------\n",
            "breathe spasmodically, and make a sound\n",
            "['When you have to hiccup, drink a glass of cold water']\n",
            "['hiccup', 'hiccough']\n",
            "['breathe', 'take_a_breath', 'respire', 'suspire']\n",
            "--------------------------------------------------\n",
            "without musical accompaniment\n",
            "['they performed a cappella']\n",
            "['a_cappella']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "in the Christian era; used before dates after the supposed year Christ was born\n",
            "['in AD 200']\n",
            "['AD', 'A.D.', 'anno_Domini']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "of the period coinciding with the Christian era; preferred by some writers who are not Christians\n",
            "['in 200 CE']\n",
            "['CE', 'C.E.', 'Common_Era']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "before the Christian era; used following dates before the supposed year Christ was born\n",
            "['in 200 BC']\n",
            "['BC', 'B.C.', 'before_Christ']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "of the period before the Common Era; preferred by some writers who are not Christians\n",
            "['in 200 BCE']\n",
            "['BCE', 'B.C.E.']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "on the back of a horse\n",
            "['he rode horseback to town', 'managed to escape ahorse', 'policeman patrolled the streets ahorseback']\n",
            "['horseback', 'ahorse', 'ahorseback']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "only a very short time before\n",
            "['they could barely hear the speaker', 'we hardly knew them', 'just missed being hit', 'had scarcely rung the bell when the door flew open', 'would have scarce arrived before she would have found some excuse to leave\"- W.B.Yeats']\n",
            "['barely', 'hardly', 'just', 'scarcely', 'scarce']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "exactly at this moment or the moment described; \n",
            "[\"we've just finished painting the walls, so don't touch them\"]\n",
            "['just']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "almost not\n",
            "['he hardly ever goes fishing', 'he was hardly more than sixteen years old', 'they scarcely ever used the emergency generator']\n",
            "['hardly', 'scarcely']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "in an anisotropic manner\n",
            "[]\n",
            "['anisotropically']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "that which is perceived or known or inferred to have its own distinct existence (living or nonliving)\n",
            "[]\n",
            "['entity']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "an entity that has physical existence\n",
            "[]\n",
            "['physical_entity']\n",
            "['entity']\n",
            "--------------------------------------------------\n",
            "a general concept formed by extracting common features from specific examples\n",
            "[]\n",
            "['abstraction', 'abstract_entity']\n",
            "['entity']\n",
            "--------------------------------------------------\n",
            "a separate and self-contained entity\n",
            "[]\n",
            "['thing']\n",
            "['physical_entity']\n",
            "--------------------------------------------------\n",
            "a tangible and visible entity; an entity that can cast a shadow\n",
            "['it was full of rackets, balls and other objects']\n",
            "['object', 'physical_object']\n",
            "['physical_entity']\n",
            "--------------------------------------------------\n",
            "an assemblage of parts that is regarded as a single entity\n",
            "['how big is that part compared to the whole?', 'the team is a unit']\n",
            "['whole', 'unit']\n",
            "['object', 'physical_object']\n",
            "--------------------------------------------------\n",
            "a whole (a thing or person) of the same kind or category as another\n",
            "['lard was also used, though its congener, butter, was more frequently employed', 'the American shopkeeper differs from his European congener']\n",
            "['congener']\n",
            "['whole', 'unit']\n",
            "--------------------------------------------------\n",
            "a living (or once living) entity\n",
            "[]\n",
            "['living_thing', 'animate_thing']\n",
            "['whole', 'unit']\n",
            "--------------------------------------------------\n",
            "a living thing that has (or can develop) the ability to act or function independently\n",
            "[]\n",
            "['organism', 'being']\n",
            "['living_thing', 'animate_thing']\n",
            "--------------------------------------------------\n",
            "organisms (plants and animals) that live at or near the bottom of a sea\n",
            "[]\n",
            "['benthos']\n",
            "['organism', 'being']\n",
            "--------------------------------------------------\n",
            "(usually followed by `to') having the necessary means or skill or know-how or authority to do something\n",
            "['able to swim', 'she was able to program her computer', 'we were at last able to buy a car', 'able to get a grant for the project']\n",
            "['able']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "(usually followed by `to') not having the necessary means or skill or know-how\n",
            "['unable to get to town without a car', 'unable to obtain funds']\n",
            "['unable']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "facing away from the axis of an organ or organism\n",
            "['the abaxial surface of a leaf is the underside or side facing away from the stem']\n",
            "['abaxial', 'dorsal']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "nearest to or facing toward the axis of an organ or organism\n",
            "['the upper side of a leaf is known as the adaxial surface']\n",
            "['adaxial', 'ventral']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "facing or on the side toward the apex\n",
            "[]\n",
            "['acroscopic']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "facing or on the side toward the base\n",
            "[]\n",
            "['basiscopic']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "especially of muscles; drawing away from the midline of the body or from an adjacent part\n",
            "[]\n",
            "['abducent', 'abducting']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "especially of muscles; bringing together or drawing toward the midline of the body or toward an adjacent part\n",
            "[]\n",
            "['adducent', 'adductive', 'adducting']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "being born or beginning\n",
            "['the nascent chicks', 'a nascent insurgency']\n",
            "['nascent']\n",
            "[]\n",
            "--------------------------------------------------\n",
            "coming into existence\n",
            "['an emergent republic']\n",
            "['emergent', 'emerging']\n",
            "[]\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"wordnet_pseudosentences.txt\", \"r\", encoding=\"utf8\") as wordnet_file:\n",
        "    line = wordnet_file.readline()\n",
        "    line2 = wordnet_file.readline()\n",
        "    line3 = wordnet_file.readline()\n",
        "line, line2, line3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLsedMcA_nPx",
        "outputId": "4fa0e443-28df-4bbd-a618-bacdd082512c"
      },
      "id": "GLsedMcA_nPx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('respire is undergo\\n', 'respire is breathe\\n', 'respire is take_a_breath\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8gnp72d1Qxk"
      },
      "outputs": [],
      "source": [],
      "id": "v8gnp72d1Qxk"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NR9dvhWF1QxS"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}