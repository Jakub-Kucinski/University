{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "a4dYmQp691wJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install recommenders recommenders[examples] tf_slim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c0dBEgQ-Fxer",
        "outputId": "aa9258d4-1f3f-4b96-b04d-4eb81294c581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting recommenders\n",
            "  Downloading recommenders-1.1.0-py3-none-manylinux1_x86_64.whl (335 kB)\n",
            "\u001b[K     |████████████████████████████████| 335 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting tf_slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 26.5 MB/s \n",
            "\u001b[?25hCollecting category-encoders<2,>=1.3.0\n",
            "  Downloading category_encoders-1.3.0-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lightgbm>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.2.3)\n",
            "Requirement already satisfied: scipy<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.4.1)\n",
            "Collecting transformers<5,>=2.5.0\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 38.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: bottleneck<2,>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.3.4)\n",
            "Requirement already satisfied: scikit-learn<1.0.3,>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.21.6)\n",
            "Collecting nltk<4,>=3.4\n",
            "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 47.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib<4,>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (3.2.2)\n",
            "Collecting retrying>=1.3.3\n",
            "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
            "Requirement already satisfied: pandas<2,>1.0.3 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.3.5)\n",
            "Collecting pandera[strategies]>=0.6.5\n",
            "  Downloading pandera-0.9.0-py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 51.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.23.0)\n",
            "Collecting memory-profiler<1,>=0.54.0\n",
            "  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n",
            "Collecting pyyaml<6,>=5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 49.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numba<1,>=0.38.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.51.2)\n",
            "Collecting cornac<2,>=1.1.2\n",
            "  Downloading cornac-1.14.2-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4 MB 40.4 MB/s \n",
            "\u001b[?25hCollecting lightfm<2,>=1.15\n",
            "  Downloading lightfm-1.16.tar.gz (310 kB)\n",
            "\u001b[K     |████████████████████████████████| 310 kB 49.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2<3.1,>=2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.11.3)\n",
            "Requirement already satisfied: tqdm<5,>=4.31.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (4.64.0)\n",
            "Collecting scikit-surprise>=1.0.6\n",
            "  Downloading scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn<1,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.11.2)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders) (0.5.2)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders) (0.10.2)\n",
            "Collecting powerlaw\n",
            "  Downloading powerlaw-1.5-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<3.1,>=2->recommenders) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<4,>=2.2.2->recommenders) (4.2.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler<1,>=0.54.0->recommenders) (5.4.8)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2022.6.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 43.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>1.0.3->recommenders) (2022.1)\n",
            "Collecting pydantic\n",
            "  Downloading pydantic-1.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1 MB 29.8 MB/s \n",
            "\u001b[?25hCollecting typing-inspect>=0.6.0\n",
            "  Downloading typing_inspect-0.7.1-py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (21.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (1.14.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (6.0.1)\n",
            "Collecting hypothesis>=5.41.1\n",
            "  Downloading hypothesis-6.47.0-py3-none-any.whl (387 kB)\n",
            "\u001b[K     |████████████████████████████████| 387 kB 56.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (21.4.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (2.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->category-encoders<2,>=1.3.0->recommenders) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.0.3,>=0.22.1->recommenders) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (3.7.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (4.11.4)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 38.1 MB/s \n",
            "\u001b[?25hCollecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf_slim) (1.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5,>=2.5.0->recommenders) (3.8.0)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.7/dist-packages (from powerlaw->cornac<2,>=1.1.2->recommenders) (1.2.1)\n",
            "Requirement already satisfied: ipykernel<7,>=4.6.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (4.10.1)\n",
            "Collecting azure.mgmt.cosmosdb<1,>=0.8.0\n",
            "  Downloading azure_mgmt_cosmosdb-0.16.0-py2.py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 51.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter<2,>=1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.0.0)\n",
            "Collecting scrapbook<1.0.0,>=0.5.0\n",
            "  Downloading scrapbook-0.5.0-py3-none-any.whl (34 kB)\n",
            "Collecting locust<2,>=1\n",
            "  Downloading locust-1.6.0-py3-none-any.whl (766 kB)\n",
            "\u001b[K     |████████████████████████████████| 766 kB 44.9 MB/s \n",
            "\u001b[?25hCollecting papermill<3,>=2.1.2\n",
            "  Downloading papermill-2.3.4-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: hyperopt<1,>=0.1.2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.1.2)\n",
            "Collecting msrest>=0.5.0\n",
            "  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting msrestazure<2.0.0,>=0.4.32\n",
            "  Downloading msrestazure-0.6.4-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.7 MB/s \n",
            "\u001b[?25hCollecting azure-common~=1.1\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt<1,>=0.1.2->recommenders) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt<1,>=0.1.2->recommenders) (2.6.3)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt<1,>=0.1.2->recommenders) (4.1.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel<7,>=4.6.1->recommenders) (5.5.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel<7,>=4.6.1->recommenders) (5.1.1)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel<7,>=4.6.1->recommenders) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel<7,>=4.6.1->recommenders) (5.3.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel<7,>=4.6.1->recommenders) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel<7,>=4.6.1->recommenders) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel<7,>=4.6.1->recommenders) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel<7,>=4.6.1->recommenders) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel<7,>=4.6.1->recommenders) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel<7,>=4.6.1->recommenders) (2.6.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter<2,>=1->recommenders) (5.2.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter<2,>=1->recommenders) (7.7.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter<2,>=1->recommenders) (5.3.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter<2,>=1->recommenders) (5.3.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter<2,>=1->recommenders) (5.6.1)\n",
            "Collecting flask==1.1.2\n",
            "  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting Flask-Cors>=3.0.10\n",
            "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
            "Collecting geventhttpclient>=1.4.4\n",
            "  Downloading geventhttpclient-1.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting psutil\n",
            "  Downloading psutil-5.9.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "\u001b[K     |████████████████████████████████| 281 kB 65.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from locust<2,>=1->recommenders) (1.0.1)\n",
            "Collecting ConfigArgParse>=1.0\n",
            "  Downloading ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: pyzmq>=16.0.2 in /usr/local/lib/python3.7/dist-packages (from locust<2,>=1->recommenders) (23.0.0)\n",
            "Requirement already satisfied: msgpack>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from locust<2,>=1->recommenders) (1.0.3)\n",
            "Collecting Flask-BasicAuth>=0.2.0\n",
            "  Downloading Flask-BasicAuth-0.2.0.tar.gz (16 kB)\n",
            "Collecting gevent>=20.9.0\n",
            "  Downloading gevent-21.12.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 44.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask==1.1.2->locust<2,>=1->recommenders) (1.1.0)\n",
            "Collecting zope.interface\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 64.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent>=20.9.0->locust<2,>=1->recommenders) (1.1.2)\n",
            "Collecting zope.event\n",
            "  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
            "Collecting brotli\n",
            "  Downloading Brotli-1.0.9-cp37-cp37m-manylinux1_x86_64.whl (357 kB)\n",
            "\u001b[K     |████████████████████████████████| 357 kB 54.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.5.0->azure.mgmt.cosmosdb<1,>=0.8.0->recommenders) (1.3.1)\n",
            "Collecting isodate>=0.6.0\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 674 kB/s \n",
            "\u001b[?25hCollecting adal<2.0.0,>=0.6.0\n",
            "  Downloading adal-1.2.7-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting cryptography>=1.1.0\n",
            "  Downloading cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 39.3 MB/s \n",
            "\u001b[?25hCollecting PyJWT<3,>=1.0.0\n",
            "  Downloading PyJWT-2.4.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.1.0->adal<2.0.0,>=0.6.0->msrestazure<2.0.0,>=0.4.32->azure.mgmt.cosmosdb<1,>=0.8.0->recommenders) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.1.0->adal<2.0.0,>=0.6.0->msrestazure<2.0.0,>=0.4.32->azure.mgmt.cosmosdb<1,>=0.8.0->recommenders) (2.21)\n",
            "Requirement already satisfied: nbclient>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from papermill<3,>=2.1.2->recommenders) (0.6.4)\n",
            "Requirement already satisfied: nbformat>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from papermill<3,>=2.1.2->recommenders) (5.4.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from papermill<3,>=2.1.2->recommenders) (0.4)\n",
            "Collecting ansiwrap\n",
            "  Downloading ansiwrap-0.8.4-py2.py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.7/dist-packages (from papermill<3,>=2.1.2->recommenders) (8.0.1)\n",
            "Collecting traitlets>=4.1.0\n",
            "  Downloading traitlets-5.2.2.post1-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 54.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from nbclient>=0.2.0->papermill<3,>=2.1.2->recommenders) (1.5.5)\n",
            "Collecting jupyter-client\n",
            "  Downloading jupyter_client-7.3.3-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 47.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel<7,>=4.6.1->recommenders) (4.10.0)\n",
            "Collecting tornado>=4.0\n",
            "  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n",
            "\u001b[K     |████████████████████████████████| 428 kB 62.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.2->papermill<3,>=2.1.2->recommenders) (2.15.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.2->papermill<3,>=2.1.2->recommenders) (4.3.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill<3,>=2.1.2->recommenders) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill<3,>=2.1.2->recommenders) (5.7.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel<7,>=4.6.1->recommenders) (0.2.5)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.0->azure.mgmt.cosmosdb<1,>=0.8.0->recommenders) (3.2.0)\n",
            "Collecting textwrap3>=0.9.2\n",
            "  Downloading textwrap3-0.9.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter<2,>=1->recommenders) (1.1.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter<2,>=1->recommenders) (3.6.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter<2,>=1->recommenders) (0.2.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter<2,>=1->recommenders) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter<2,>=1->recommenders) (0.13.3)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter<2,>=1->recommenders) (0.7.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter<2,>=1->recommenders) (5.0.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter<2,>=1->recommenders) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter<2,>=1->recommenders) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter<2,>=1->recommenders) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter<2,>=1->recommenders) (0.6.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter<2,>=1->recommenders) (0.5.1)\n",
            "Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter<2,>=1->recommenders) (2.1.0)\n",
            "Building wheels for collected packages: lightfm, memory-profiler, retrying, scikit-surprise, Flask-BasicAuth\n",
            "  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightfm: filename=lightfm-1.16-cp37-cp37m-linux_x86_64.whl size=705374 sha256=708cdcbe9255cb952bd974d343710cdac7e8e5983afecd9aa9e90c35aa0c49c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/56/28/5772a3bd3413d65f03aa452190b00898b680b10028a1021914\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=b6864653a8bbaf4a7eecd3ae8ea3788ceca13621edcc91be9f5688b1e37f589c\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n",
            "  Building wheel for retrying (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11447 sha256=0b8afbb3df56f33890f2a4dd0d672530a748b01917b696ddabc027949b42935d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/8d/8d/f6af3f7f9eea3553bc2fe6d53e4b287dad18b06a861ac56ddf\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1633731 sha256=42918b6206e7bb3f93f9334a2521a62bfe62a10232a469ef5acac31c83227458\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/44/74/b498c42be47b2406bd27994e16c5188e337c657025ab400c1c\n",
            "  Building wheel for Flask-BasicAuth (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Flask-BasicAuth: filename=Flask_BasicAuth-0.2.0-py3-none-any.whl size=4243 sha256=49f87c77e70228a9c8ec9f7a8572794c485cfb6d5b82820226e66bd5d56028b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/08/a3/19638d90fdf01258ede772449bcbde424839459749acb977b6\n",
            "Successfully built lightfm memory-profiler retrying scikit-surprise Flask-BasicAuth\n",
            "Installing collected packages: traitlets, tornado, jupyter-client, mypy-extensions, zope.interface, zope.event, typing-inspect, textwrap3, pyyaml, PyJWT, pydantic, isodate, cryptography, tokenizers, regex, psutil, powerlaw, pandera, msrest, hypothesis, huggingface-hub, gevent, flask, brotli, ansiwrap, adal, transformers, scikit-surprise, retrying, papermill, nltk, msrestazure, memory-profiler, lightfm, geventhttpclient, Flask-Cors, Flask-BasicAuth, cornac, ConfigArgParse, category-encoders, azure-common, scrapbook, recommenders, locust, azure.mgmt.cosmosdb, tf-slim\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.1.1\n",
            "    Uninstalling traitlets-5.1.1:\n",
            "      Successfully uninstalled traitlets-5.1.1\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 5.3.5\n",
            "    Uninstalling jupyter-client-5.3.5:\n",
            "      Successfully uninstalled jupyter-client-5.3.5\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Attempting uninstall: flask\n",
            "    Found existing installation: Flask 1.1.4\n",
            "    Uninstalling Flask-1.1.4:\n",
            "      Successfully uninstalled Flask-1.1.4\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.1 which is incompatible.\u001b[0m\n",
            "Successfully installed ConfigArgParse-1.5.3 Flask-BasicAuth-0.2.0 Flask-Cors-3.0.10 PyJWT-2.4.0 adal-1.2.7 ansiwrap-0.8.4 azure-common-1.1.28 azure.mgmt.cosmosdb-0.16.0 brotli-1.0.9 category-encoders-1.3.0 cornac-1.14.2 cryptography-37.0.2 flask-1.1.2 gevent-21.12.0 geventhttpclient-1.5.3 huggingface-hub-0.7.0 hypothesis-6.47.0 isodate-0.6.1 jupyter-client-7.3.3 lightfm-1.16 locust-1.6.0 memory-profiler-0.60.0 msrest-0.6.21 msrestazure-0.6.4 mypy-extensions-0.4.3 nltk-3.7 pandera-0.9.0 papermill-2.3.4 powerlaw-1.5 psutil-5.9.1 pydantic-1.9.1 pyyaml-5.4.1 recommenders-1.1.0 regex-2022.6.2 retrying-1.3.3 scikit-surprise-1.1.1 scrapbook-0.5.0 textwrap3-0.9.2 tf-slim-1.1.0 tokenizers-0.12.1 tornado-6.1 traitlets-5.2.2.post1 transformers-4.19.2 typing-inspect-0.7.1 zope.event-4.5.0 zope.interface-5.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt6AA_mu5tY9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import random\n",
        "import sklearn\n",
        "\n",
        "import recommenders\n",
        "import recommenders.datasets.movielens\n",
        "from recommenders.datasets.python_splitters import python_chrono_split\n",
        "from recommenders.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset creation"
      ],
      "metadata": {
        "id": "vFP6lFQf77S3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random split\n",
        "RandomDatasetCreator ensures that every item and user is present at least once in the training set."
      ],
      "metadata": {
        "id": "scdKj_gR8DGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomDatasetCreator:\n",
        "    def __init__(self, df, user_id=\"userID\", item_id=\"itemID\", rating=\"rating\", shuffle=True, copy=False, seed=42):\n",
        "        self.seed = seed\n",
        "        if seed is not None:\n",
        "            random.seed(self.seed)\n",
        "            np.random.seed(seed=self.seed)\n",
        "        if shuffle:\n",
        "            self.df = sklearn.utils.shuffle(df, random_state=self.seed)\n",
        "        else:\n",
        "            self.df = df\n",
        "        if copy:\n",
        "            self.df = self.df.copy()\n",
        "        self.user_id = user_id\n",
        "        self.item_id = item_id\n",
        "        self.rating = rating\n",
        "        self.n_users, self.n_items = self.df[self.user_id].nunique(), self.df[self.item_id].nunique()\n",
        "        self.n_max_ratings = self.n_users * self.n_items\n",
        "        self.n_ratings = self.df.shape[0]\n",
        "        self.max_sparsity = self.n_ratings / self.n_max_ratings\n",
        "        self.minimal_indexes = self.__get_minimal_indexes()\n",
        "        self.min_sparsity = self.minimal_indexes.shape[0] / self.n_max_ratings\n",
        "        self.minimal_df = self.df.loc[self.minimal_indexes]\n",
        "        self.remaining_df = self.df[~self.df.index.isin(self.minimal_indexes)]\n",
        "\n",
        "    def __get_minimal_indexes(self):\n",
        "        if \"index\" in self.df.columns:\n",
        "            raise Exception(\"Dataframe can not contain 'index' column name.\")\n",
        "        df_with_index_as_column = self.df.reset_index()\n",
        "        numpy_array_of_indexes = pd.concat([df_with_index_as_column.groupby(self.user_id).first()[\"index\"],\n",
        "                   df_with_index_as_column.groupby(self.item_id).first()[\"index\"]]).unique()\n",
        "        return numpy_array_of_indexes\n",
        "\n",
        "    def train_test_split(self, train_size=0.8):\n",
        "        sparsity = self.max_sparsity * train_size\n",
        "        if sparsity <= self.min_sparsity:\n",
        "            return self.minimal_df.copy()\n",
        "        if sparsity >= self.max_sparsity:\n",
        "            return self.df\n",
        "        num_of_additional_ratings = max(int(train_size * self.n_ratings) - self.minimal_indexes.shape[0], 0)\n",
        "        additional_df = self.remaining_df.sample(n=num_of_additional_ratings)\n",
        "        train_df = pd.concat([self.minimal_df, additional_df])\n",
        "        test_df = self.df[~self.df.index.isin(train_df.index)]\n",
        "        return train_df, test_df"
      ],
      "metadata": {
        "id": "n9Jy4I8J7_J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = recommenders.datasets.movielens.load_pandas_df('100k', ('userID', 'itemID', 'rating'))\n",
        "dataset_generator = RandomDatasetCreator(df)\n",
        "train_df, test_df = dataset_generator.train_test_split(0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3OL0DRB8in7",
        "outputId": "8d211cbb-0b89-40de-d8a3-9733f39fe668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.81k/4.81k [00:00<00:00, 8.96kKB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time split\n",
        "Some of the users or items might not be present in the training set."
      ],
      "metadata": {
        "id": "9VX4JKYL8kD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = recommenders.datasets.movielens.load_pandas_df('100k', ('userID', 'itemID', 'ratings', 'timestamp'))\n",
        "random.seed(42)\n",
        "np.random.seed(seed=42)\n",
        "train_df, test_df = python_chrono_split(df, ratio=0.8, col_user='userID', col_item='itemID', col_timestamp='timestamp')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7drFKaf19UYq",
        "outputId": "a1d150d7-79fb-4f2a-fe47-083aa1b915cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.81k/4.81k [00:00<00:00, 9.69kKB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Leave-one-out user\n",
        "LeaveOneOutUser ensures that every item and user is present at least once in the training set."
      ],
      "metadata": {
        "id": "iRHvsTtW9evU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeaveOneOutUser:\n",
        "    def __init__(self, df, user_id=\"userID\", item_id=\"itemID\", rating=\"rating\", timestamp=\"timestamp\", shuffle=True, copy=False, seed=42):\n",
        "        self.seed = seed\n",
        "        if seed is not None:\n",
        "            random.seed(self.seed)\n",
        "            np.random.seed(seed=self.seed)\n",
        "        if shuffle:\n",
        "            self.df = sklearn.utils.shuffle(df, random_state=self.seed)\n",
        "        else:\n",
        "            self.df = df\n",
        "        self.df = self.df.sort_values(timestamp)\n",
        "        if copy:\n",
        "            self.df = self.df.copy()\n",
        "        self.user_id = user_id\n",
        "        self.item_id = item_id\n",
        "        self.rating = rating\n",
        "        self.timestamp = timestamp\n",
        "        self.n_users, self.n_items = self.df[self.user_id].nunique(), self.df[self.item_id].nunique()\n",
        "        self.n_max_ratings = self.n_users * self.n_items\n",
        "        self.n_ratings = self.df.shape[0]\n",
        "        self.max_sparsity = self.n_ratings / self.n_max_ratings\n",
        "        self.minimal_indexes = self.__get_minimal_indexes()\n",
        "        self.min_sparsity = self.minimal_indexes.shape[0] / self.n_max_ratings\n",
        "        self.minimal_df = self.df.loc[self.minimal_indexes]\n",
        "        self.remaining_df = self.df[~self.df.index.isin(self.minimal_indexes)]\n",
        "\n",
        "    def __get_minimal_indexes(self):\n",
        "        if \"index\" in self.df.columns:\n",
        "            raise Exception(\"Dataframe can not contain 'index' column name.\")\n",
        "        df_with_index_as_column = self.df.reset_index()\n",
        "        numpy_array_of_indexes = pd.concat([df_with_index_as_column.groupby(self.user_id).first()[\"index\"],\n",
        "                   df_with_index_as_column.groupby(self.item_id).first()[\"index\"]]).unique()\n",
        "        return numpy_array_of_indexes\n",
        "\n",
        "    def train_test_split(self):\n",
        "        test_df = self.remaining_df.sort_values(self.timestamp).reset_index().groupby(self.user_id).last().reset_index()\n",
        "        test_df = test_df.set_index('index')\n",
        "        test_df.index.name = None\n",
        "        train_df = self.df[~self.df.index.isin(test_df.index)]\n",
        "        return train_df, test_df"
      ],
      "metadata": {
        "id": "Cfu59U6L9cJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = recommenders.datasets.movielens.load_pandas_df('100k', ('userID', 'itemID', 'rating', 'timestamp'))\n",
        "dataset_generator = LeaveOneOutUser(df)\n",
        "train_df, test_df = dataset_generator.train_test_split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foqVP2wm9jVi",
        "outputId": "27ae19bc-1d89-4c4b-c641-508d4de10792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.81k/4.81k [00:00<00:00, 25.9kKB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Leave-one-out item\n",
        "LeaveOneOutUser ensures that every item and user is present at least once in the training set."
      ],
      "metadata": {
        "id": "94j3XxV59odo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeaveOneOutItem:\n",
        "    def __init__(self, df, user_id=\"userID\", item_id=\"itemID\", rating=\"rating\", timestamp=\"timestamp\", shuffle=True, copy=False, seed=42):\n",
        "        self.seed = seed\n",
        "        if seed is not None:\n",
        "            random.seed(self.seed)\n",
        "            np.random.seed(seed=self.seed)\n",
        "        if shuffle:\n",
        "            self.df = sklearn.utils.shuffle(df, random_state=self.seed)\n",
        "        else:\n",
        "            self.df = df\n",
        "        self.df = self.df.sort_values(timestamp)\n",
        "        if copy:\n",
        "            self.df = self.df.copy()\n",
        "        self.user_id = user_id\n",
        "        self.item_id = item_id\n",
        "        self.rating = rating\n",
        "        self.timestamp = timestamp\n",
        "        self.n_users, self.n_items = self.df[self.user_id].nunique(), self.df[self.item_id].nunique()\n",
        "        self.n_max_ratings = self.n_users * self.n_items\n",
        "        self.n_ratings = self.df.shape[0]\n",
        "        self.max_sparsity = self.n_ratings / self.n_max_ratings\n",
        "        self.minimal_indexes = self.__get_minimal_indexes()\n",
        "        self.min_sparsity = self.minimal_indexes.shape[0] / self.n_max_ratings\n",
        "        self.minimal_df = self.df.loc[self.minimal_indexes]\n",
        "        self.remaining_df = self.df[~self.df.index.isin(self.minimal_indexes)]\n",
        "\n",
        "    def __get_minimal_indexes(self):\n",
        "        if \"index\" in self.df.columns:\n",
        "            raise Exception(\"Dataframe can not contain 'index' column name.\")\n",
        "        df_with_index_as_column = self.df.reset_index()\n",
        "        numpy_array_of_indexes = pd.concat([df_with_index_as_column.groupby(self.user_id).first()[\"index\"],\n",
        "                   df_with_index_as_column.groupby(self.item_id).first()[\"index\"]]).unique()\n",
        "        return numpy_array_of_indexes\n",
        "\n",
        "    def train_test_split(self):\n",
        "        test_df = self.remaining_df.sort_values(self.timestamp).reset_index().groupby(self.item_id).last().reset_index()\n",
        "        test_df = test_df.set_index('index')\n",
        "        test_df.index.name = None\n",
        "        train_df = self.df[~self.df.index.isin(test_df.index)]\n",
        "        return train_df, test_df"
      ],
      "metadata": {
        "id": "Z0QcVeNE9pW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = recommenders.datasets.movielens.load_pandas_df('100k', ('userID', 'itemID', 'rating', 'timestamp'))\n",
        "dataset_generator = LeaveOneOutItem(df)\n",
        "train_df, test_df = dataset_generator.train_test_split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiDU8goK9rDd",
        "outputId": "c61cb369-0340-47ee-d345-535055dfe2da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.81k/4.81k [00:00<00:00, 21.8kKB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving files"
      ],
      "metadata": {
        "id": "Hcy1dl8W9vIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_FILE_PATH = \"./train.csv\"\n",
        "TEST_FILE_PATH = \"./test.csv\"\n",
        "\n",
        "train_df.sort_values(\"userID\").to_csv(TRAIN_FILE_PATH, index=False)\n",
        "test_df.sort_values(\"userID\").to_csv(TEST_FILE_PATH, index=False)"
      ],
      "metadata": {
        "id": "lvUs3m_B9woC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libffm format\n",
        "If your model requires input in Libffm format you can use [recommenders LibffmConverter](https://microsoft-recommenders.readthedocs.io/en/latest/datasets.html#recommenders.datasets.pandas_df_utils.LibffmConverter):"
      ],
      "metadata": {
        "id": "a4dYmQp691wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from recommenders.datasets.pandas_df_utils import LibffmConverter\n",
        "\n",
        "converter = LibffmConverter()\n",
        "converter = converter.fit(train_df, col_rating='rating')\n",
        "limffm_train_df = converter.transform(train_df)\n",
        "\n",
        "converter = converter.fit(test_df, col_rating='rating')\n",
        "limffm_test_df = converter.transform(test_df)\n",
        "limffm_test_df"
      ],
      "metadata": {
        "id": "DudzRRge924F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training\n",
        "\n",
        "Here is the part that needs to be custom written for every model"
      ],
      "metadata": {
        "id": "qxZmKOYF-Az2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import SVD, SVDpp, NMF, Dataset\n",
        "from surprise.model_selection import cross_validate\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "from surprise import Reader\n",
        "from surprise.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "dgSVOUGeqrYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader = Reader(rating_scale=(1, 5))\n",
        "train_data = Dataset.load_from_df(train_df[['userID', 'itemID', 'rating']], reader)\n",
        "test_data = Dataset.load_from_df(test_df[['userID', 'itemID', 'rating']], reader)"
      ],
      "metadata": {
        "id": "uwAXDq_bqzD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svd = SVD(lr_all=.001, reg_all=0.005, n_epochs=20, n_factors=15, verbose=True)\n",
        "svd = SVD()\n",
        "svd.fit(train_data.build_full_trainset())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOfVULjzrTQD",
        "outputId": "51171c8f-eb40-468f-abc2-31e5178dedb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7f3a03844490>"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svdpp = SVDpp(lr_all=.001, reg_all=0.005, n_epochs=20, n_factors=15, verbose=True)\n",
        "svdpp.fit(train_data.build_full_trainset())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68ppY8_TxIDK",
        "outputId": "d139a8ce-144b-4def-8018-956c6df08968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processing epoch 0\n",
            " processing epoch 1\n",
            " processing epoch 2\n",
            " processing epoch 3\n",
            " processing epoch 4\n",
            " processing epoch 5\n",
            " processing epoch 6\n",
            " processing epoch 7\n",
            " processing epoch 8\n",
            " processing epoch 9\n",
            " processing epoch 10\n",
            " processing epoch 11\n",
            " processing epoch 12\n",
            " processing epoch 13\n",
            " processing epoch 14\n",
            " processing epoch 15\n",
            " processing epoch 16\n",
            " processing epoch 17\n",
            " processing epoch 18\n",
            " processing epoch 19\n",
            " processing epoch 20\n",
            " processing epoch 21\n",
            " processing epoch 22\n",
            " processing epoch 23\n",
            " processing epoch 24\n",
            " processing epoch 25\n",
            " processing epoch 26\n",
            " processing epoch 27\n",
            " processing epoch 28\n",
            " processing epoch 29\n",
            " processing epoch 30\n",
            " processing epoch 31\n",
            " processing epoch 32\n",
            " processing epoch 33\n",
            " processing epoch 34\n",
            " processing epoch 35\n",
            " processing epoch 36\n",
            " processing epoch 37\n",
            " processing epoch 38\n",
            " processing epoch 39\n",
            " processing epoch 40\n",
            " processing epoch 41\n",
            " processing epoch 42\n",
            " processing epoch 43\n",
            " processing epoch 44\n",
            " processing epoch 45\n",
            " processing epoch 46\n",
            " processing epoch 47\n",
            " processing epoch 48\n",
            " processing epoch 49\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<surprise.prediction_algorithms.matrix_factorization.SVDpp at 0x7f2c85af6990>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from lightfm import LightFM\n",
        "from lightfm.data import Dataset\n",
        "from lightfm.datasets import fetch_movielens"
      ],
      "metadata": {
        "id": "7MIib95n10Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset()\n",
        "data.fit(train_df.userID.unique(), train_df.itemID.unique())\n",
        "user_id_mapping, _, item_id_mapping, _ = data.mapping()\n",
        "interactions_matrix, weights_matrix = data.build_interactions([tuple(i) for i in train_df.drop(['timestamp'], axis = 1).values])\n",
        "lightfm = LightFM(loss='warp')\n",
        "lightfm.fit(interactions_matrix, sample_weight = weights_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuQnmO2J19I1",
        "outputId": "db067fc6-50c3-4c85-f4a1-47f9d8b76b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<lightfm.lightfm.LightFM at 0x7f3a09239a90>"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "riJmqBv6YhrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model wrapper for custom model\n",
        "\n",
        "class ModelWrapper:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, users, items):\n",
        "        # Place to implement creating predictions with your model\n",
        "\n",
        "        # Surprise models:\n",
        "        # return [self.model.predict(str(user), str(item)).est for user, item in zip(users, items)]\n",
        "\n",
        "        # LightFM\n",
        "        return self.model.predict(user_id_mapping[int(users[0])], np.array([item_id_mapping[item] for item in items]))"
      ],
      "metadata": {
        "id": "WgqP246jYjXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example implementation\n",
        "\n",
        "class ModelWrapper:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, users, items):\n",
        "        self.model.eval()\n",
        "        return [self.model.train_step(user, n_users+item).cpu().detach().numpy()[0] for user, item in zip(users, items)]"
      ],
      "metadata": {
        "id": "Jw6RAYnxLkBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelwrapper = ModelWrapper(lightfm)"
      ],
      "metadata": {
        "id": "gqKrNDQzgd4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(TEST_FILE_PATH)\n",
        "train = pd.read_csv(TRAIN_FILE_PATH)"
      ],
      "metadata": {
        "id": "fw3qxO2hY9v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users, items, preds = [], [], []\n",
        "all_items = list(test.itemID.unique())\n",
        "train_item_user_pairs = set([tuple(arr) for arr in train[[\"itemID\", \"userID\"]].values])\n",
        "\n",
        "for user in test.userID.unique():\n",
        "    item_arr = [i for i in all_items if (i, user) not in train_item_user_pairs]\n",
        "    user_arr = [user] * len(item_arr)\n",
        "    users.extend(user_arr)\n",
        "    items.extend(item_arr)\n",
        "    preds.extend(list(modelwrapper.predict(user_arr, item_arr)))\n",
        "\n",
        "all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\":items, \"prediction\":preds})\n",
        "all_predictions"
      ],
      "metadata": {
        "id": "i1tivZXvMSxi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "95874728-ac11-4379-ccba-77f8f4fce516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        userID  itemID  prediction\n",
              "0            1      74   -3.064796\n",
              "1            1     281   -0.692533\n",
              "2            1     317   -1.029688\n",
              "3            1     457   -2.835075\n",
              "4            1     341   -3.283792\n",
              "...        ...     ...         ...\n",
              "435087     943     458   -1.407171\n",
              "435088     943     246   -1.386418\n",
              "435089     943     863   -2.491826\n",
              "435090     943     788   -3.056770\n",
              "435091     943     662   -1.198668\n",
              "\n",
              "[435092 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-27e8fe55-a451-402c-a51c-c3e593e5c32a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>itemID</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>74</td>\n",
              "      <td>-3.064796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>281</td>\n",
              "      <td>-0.692533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>317</td>\n",
              "      <td>-1.029688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>457</td>\n",
              "      <td>-2.835075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>341</td>\n",
              "      <td>-3.283792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435087</th>\n",
              "      <td>943</td>\n",
              "      <td>458</td>\n",
              "      <td>-1.407171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435088</th>\n",
              "      <td>943</td>\n",
              "      <td>246</td>\n",
              "      <td>-1.386418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435089</th>\n",
              "      <td>943</td>\n",
              "      <td>863</td>\n",
              "      <td>-2.491826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435090</th>\n",
              "      <td>943</td>\n",
              "      <td>788</td>\n",
              "      <td>-3.056770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435091</th>\n",
              "      <td>943</td>\n",
              "      <td>662</td>\n",
              "      <td>-1.198668</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>435092 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27e8fe55-a451-402c-a51c-c3e593e5c32a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-27e8fe55-a451-402c-a51c-c3e593e5c32a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-27e8fe55-a451-402c-a51c-c3e593e5c32a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = {\n",
        "    \"map@5\": map_at_k(test, all_predictions, col_prediction='prediction', k=5),\n",
        "    \"ndcg@5\": ndcg_at_k(test, all_predictions, col_prediction='prediction', k=5),\n",
        "    \"precision@5\": precision_at_k(test, all_predictions, col_prediction='prediction', k=5),\n",
        "    \"recall@5\": recall_at_k(test, all_predictions, col_prediction='prediction', k=5),\n",
        "\n",
        "    \"map@20\": map_at_k(test, all_predictions, col_prediction='prediction', k=20),\n",
        "    \"ndcg@20\": ndcg_at_k(test, all_predictions, col_prediction='prediction', k=20),\n",
        "    \"precision@20\": precision_at_k(test, all_predictions, col_prediction='prediction', k=20),\n",
        "    \"recall@20\": recall_at_k(test, all_predictions, col_prediction='prediction', k=20)\n",
        "}"
      ],
      "metadata": {
        "id": "_FGDKbPNYdIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('metrics.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(metrics, f, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "_0Y8mePjY375"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics"
      ],
      "metadata": {
        "id": "UYS3ZjGPZSsz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99cb16f3-ae55-4c06-af18-52fd595d05b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'map@20': 0.040046829953187375,\n",
              " 'map@5': 0.030858960763520675,\n",
              " 'ndcg@20': 0.06544530590240354,\n",
              " 'ndcg@5': 0.03774510732147239,\n",
              " 'precision@20': 0.007900318133616116,\n",
              " 'precision@5': 0.011664899257688223,\n",
              " 'recall@20': 0.15800636267232238,\n",
              " 'recall@5': 0.05832449628844114}"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    }
  ]
}