\documentclass{article}
\usepackage[margin=1.25in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[polish]{babel}
\usepackage{titlesec}
\titlelabel{\thetitle.\quad}

\let\babellll\lll
\let\lll\relax

\usepackage{amsthm} %proof
\usepackage{amsmath}
\usepackage{amssymb}

\title{Rozkład SVD macierzy \\ Singular Value Decomposition}
\author{Karol Kuczmarz \and Jakub Kuciński}
\date{\today}

\newtheorem{theorem}{Twierdzenie}[section]
\newtheorem{concl}{Wniosek}[theorem]
\newtheorem{att}[theorem]{Uwaga}
\newtheorem{lemma}[theorem]{Lemat}
\newtheorem{fact}[theorem]{Fakt}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definicja}

\usepackage{colortbl}
\usepackage{tikz}
\usetikzlibrary{matrix}
\newcommand{\szary}{\cellcolor{gray}}

\usepackage{array,tikz}
\usetikzlibrary{tikzmark,arrows.meta}

\begin{document}

\maketitle
\section{Wprowadzenie}

\begin{definition}
Niech $A \in M_{n \times n}(\mathbb{R})$. Wektor $v \in \mathbb{R}^n$ nazywamy wektorem własnym $A$, jeżeli
$$ (\exists \lambda \in \mathbb{R}) (Av = \lambda v) $$
\end{definition}

\begin{definition}
Wartością własną macierzy $A \in M_{m \times n}(\mathbb{R})$ nazywamy $\lambda \in \mathbb{R}$ takie, że istnieje niezerowy wektor własny dla $\lambda$ (tzn. $(\exists v \in \mathbb{R}^n \setminus \{ 0 \} ) (Av=\lambda v) $). 
\end{definition}

\begin{theorem}[Diagonalizacja macierzy]
Jeżeli macierz $A \in M_{n \times n} (\mathbb{R})$ ma $n$ liniowo niezależnych wektorów własnych $v_1$, \ldots, $v_n$ $\in \mathbb{R}^n $ i $\lambda_1$, \ldots, $\lambda_n$ to odpowiadające im wartości własne, to macierz $A$ możemy przedstawić jako
$$ A = PDP^{-1} $$
gdzie:
\begin{itemize}
    \item $P = (v_1, \ldots, v_n)$
    \item $D = \begin{pmatrix} \lambda_1 & & \\ & \ddots & \\
     & & \lambda_n \end{pmatrix} $ diagonalna.
\end{itemize}
\end{theorem}

\begin{att}
Nie wszystkie macierze można diagonalizować. Można przejść w liczby zespolone, ale dalej możemy mieć wielokrotne wartości własne. Uogólnienie -- \textit{Twierdzenie Jordana}.
\end{att}

Do rozkładu SVD potrzebne jest przypomnienie własności i nazewnictwa dla macierzy symetrycznych ($A^T = A$). Będziemy posługiwać się standardowym iloczynem skalarnym w $\mathbb{R}^n$, tzn. dla $v,u \in \mathbb{R}^n$ mamy $u \circ v = \sum_{i=1}^n u_i \cdot v_i = u^Tv$. Mówimy, że wektory $u$ i $v$ są prostopadłe, jeżeli $u \circ v = 0$.

\begin{definition}
Macierz kwadratową $A \in M_{n \times n} (\mathbb{R}) $ nazywamy macierzą ortogonalną, jeżeli zachodzi $AA^T = A^TA = I$.
\end{definition}

\begin{att}
Iloczyn macierzy ortogonalnych $A$, $B \in M_{m \times n} (\mathbb{R})$ jest macierzą ortogonalną.
\end{att}

\begin{proof}
$(AB)(AB)^T = ABB^TA^T = AA^T=I$ \\ $(AB)^T(AB) = B^TA^TAB = B^TB=I$
\end{proof}

\begin{definition}
Zbiór wektorów będący bazą przestrzeni $\mathbb{R}^n$ taki, że wektory są parami prostopadłe i mają długość $1$ nazywamy bazą ortonormalną.
\end{definition}

\newpage

\begin{att}
Ortogonalności macierzy są równoważne warunki:
\begin{itemize}
    \item kolumny $A$ stanowią bazę ortonormalną $\mathbb{R}^n$
    \item wiersze $A$ stanowią bazę ortonormalną $\mathbb{R}^n$
\end{itemize}
\end{att}
\begin{theorem}[Twierdzenie spektralne]
Każda symetryczna macierz $A \in M_{n \times n} (\mathbb{R} )$ diagonalizuje się w pewnej bazie ortonormalnej  i wszystkie jej wartości własne są rzeczywiste, tzn. $$A = PDP^T $$
gdzie:
\begin{itemize}
    \item $D$ jest diagonalna z rzeczywistymi wyrazami na przekątnej
    \item $P$ jest macierzą ortogonalną.
\end{itemize}
\end{theorem}
\begin{definition}
Macierz symetryczna $A \in M_{n \times n} (\mathbb{R})$ jest dodatnio określona, jeżeli dla każdego $v \in \mathbb{R}^n \setminus \{ 0 \}$ zachodzi
$$ v \circ Av > 0 $$
Jeżeli nierówność jest słaba, to mówimy, że macierz jest dodatnio półokreślona.
\end{definition}
\begin{att}
Dodatnia określoność (półokreśloność) jest równoznaczna ze wszystkimi dodatnimi (nieujemnymi) wartościami własnymi macierzy.
\end{att}

\begin{proof}
Z twierdzenia spektralnego mamy $A = PDP^T$. Wtedy 
$$ v \circ Av = v \circ PDP^Tv = (P^Tv) \circ D(P^Tv) > 0 $$
Skoro $P^T$ jest ortogonalna, to jest macierzą zmiany bazy, czyli powyższa równość jest tożsama z
$$ v \circ Dv > 0 $$
$$ v \circ Dv = \sum_{i = 1}^n \lambda_i \cdot v_i^2 > 0 $$
Powyższa nierówność zachodzi dla dowolnego $v$ wtedy i tylko wtedy, gdy $(\forall \: 1 \leq i \leq n) (\lambda_i > 0)$.
Podobnie pokazujemy dla dodatniej półokreśloności.
\end{proof}

\begin{lemma}
Dla dowolnej macierzy $A \in M_{m \times n} (\mathbb{R})$ macierze $AA^T$ i $A^TA$ są symetryczne i dodatnio półokreślone.
\end{lemma}
\begin{proof}
Macierze $A^TA$ i $AA^T$ są odpowiednio wymiaru $n \times n$ i $m \times m$.

Najpierw pokażemy symetryczność.
$$ (AA^T)^T = (A^T)^T  A^T = AA^T $$
$$ (A^TA)^T = A^T (A^T)^T = A^TA $$

Teraz pokażemy dodatnią półokreśloność $A^TA$, dowód dla macierzy $AA^T$ jest analogiczny. \\ Weźmy dowolny wektor $v \in \mathbb{R}^n \setminus \{ 0\}$.
$$ v \circ A^TA v = Av \circ Av \geq 0 $$
z definicji iloczynu skalarnego.
\end{proof}
\section{Główne twierdzenie}

Zanim podamy twierdzenie najpierw sformułujemy i udowodnimy pomocniczy lemat.
\begin{lemma}
Dla $A \in M_{m \times n} (\mathbb{R})$ zachodzi $rank(A^TA) \leq \min(m,n)$.
\end{lemma}
\begin{proof}
Wiemy, że dla przekształcenia liniowego o macierzy $B$ mamy $rank(B) = ImF_B$, czyli rząd macierzy $B$ jest równy wymiarowi obrazu przekształcenia zadanego tą macierzą.

Przekształcenie zadane macierzą $A^TA$ jest pomiędzy następującymi przestrzeniami: \\$F_{A^TA}: \mathbb{R}^n \rightarrow \mathbb{R}^m \rightarrow \mathbb{R}^n$. Stąd  $rank(A^TA) = Im F_{A^TA} \leq \min(m,n)$.
\end{proof}

\begin{theorem}[Rozkład SVD]
Każdą macierz $A \in M_{m \times n} (\mathbb{R})$ można przedstawić w postaci iloczynu $A = U \Sigma V^T$, gdzie:
\begin{itemize}
    \item $U \in M_{m \times m} (\mathbb{R})$ jest macierzą ortogonalną
    \item $\Sigma \in M_{m \times n} (\mathbb{R})$ jest macierzą postaci $\begin{pmatrix} D & 0 \end{pmatrix} lub \begin{pmatrix} D \\ 0 \end{pmatrix}$ dla $D$ diagonalnej z nieujemnymi wyrazami na przekątnej uporządkowanymi nierosnąco
    \item $V \in M_{n \times n} (\mathbb{R})$ jest macierzą ortogonalną.
\end{itemize}
Niezerowe elementy w macierzy $\Sigma$ nazywane są wartościami szczególnymi macierzy $A$.
\end{theorem}

\begin{proof}
Wiemy, że macierz $A^TA$ jest symetryczna i dodatnio półokreślona. Z twierdzenia spektralnego macierz $A^TA$ możemy zdiagonalizować w bazie ortonormalnej, czyli $A^TA = V \bar{D} V^T$, gdzie $V$ jest ortogonalna a $\bar{D}$ jest diagonalna. 

Możemy bez straty ogólności założyć, że macierz $\bar{D} = \begin{pmatrix} D & 0 \\ 0 & 0 \end{pmatrix}$, gdzie $D$ jest diagonalna z dodatnimi uporządkowanymi nierosnąco wyrazami na przekątnej. $D \in M_{l \times l} (\mathbb{R})$ dla $l \leq \min (m,n)$, ponieważ $l = rank(A^TA)$ (później \textit{Lemat 2.1}).

Niech $V = \begin{pmatrix} V_1 & V_2 \end{pmatrix}$, gdzie $V_1$ to macierz wektorów odpowiadających niezerowym wartościom własnym natomiast $V_2$ to macierz wektorów odpowiadających zerowym wartościom własnym.

Mamy $$ A^TA = V \bar{D} V^T $$
$$ V^T A^TA V = \bar{D} = \begin{pmatrix} D & 0 \\ 0 & 0 \end{pmatrix} $$
$$ \begin{pmatrix} V_1^T \\ V_2^T \end{pmatrix} A^TA \begin{pmatrix} V_1 & V_2 \end{pmatrix} = \begin{pmatrix} V_1^T A^T A \\ V_2^T A^TA \end{pmatrix} \begin{pmatrix} V_1 & V_2 \end{pmatrix} = \begin{pmatrix} V_1^T A^TA V_1 & V_1^T A^TA V_2 \\ V_2^T A^TA V_1 & V_2^T A^TA V_2 \end{pmatrix}$$
Zatem $D =  V_1^T A^TA V_1$.

Niech $U_1 = AV_1D^{-\frac{1}{2}}$ (odwracamy i pierwiastkujemy wyrazy z przekątnej macierzy).\\Zauważmy, że
$$ U_1 D^{\frac{1}{2}} V_1^T = AV_1 D^{-\frac{1}{2}} D^{\frac{1}{2}} V_1^T = A V_1 V_1^T = A $$

Pokażemy, że wektory macierzy $U_1$ są ortogonalne i długości 1.
Rozważmy iloczyn $U_1^T U_1$.
$$ U_1^T U_1 = (D^{-\frac{1}{2}})^T V_1^T A^T A V^1 D^{-\frac{1}{2}} $$
Mamy $D =  V_1^T A^TA V_1$. Stąd
$$ U_1^T U_1 = D^{-\frac{1}{2}} D D^{-\frac{1}{2}} = I $$
Macierz $U_1$ możemy też zapisać jako $U_1 = \begin{pmatrix} u_1 & u_2 & \ldots & u_l \end{pmatrix}$.
Wtedy $$ U_1^T U_1 = \begin{pmatrix} u_1^T \\ \vdots \\ u_l^T \end{pmatrix} \begin{pmatrix} u_1 & \ldots & u_l \end{pmatrix} = \begin{pmatrix} <u_1,u_1> & \ldots & <u_1, u_l> \\ \vdots & \ddots & \vdots \\ <u_l, u_1> & \ldots & <u_l, u_l>  \end{pmatrix} = \begin{pmatrix} 1 & & \\ & \ddots & \\ & & 1 \end{pmatrix}$$
Czyli wektory $u_1$, \ldots, $u_n$ są parami prostopadłe i długości $1$, zatem możemy uzupełnić je do bazy ortonormalnej przestrzeni $\mathbb{R}^m$ wektorami będącymi kolumnami macierzy $U_2$. Wtedy macierz $U = \begin{pmatrix} U_1 & U_2 \end{pmatrix} $ jest macierzą ortogonalną.

Niech $\Sigma = \begin{pmatrix} D^{\frac{1}{2}} & 0 \\ 0 & 0 \end{pmatrix}$. Uzupełniamy macierz $D$ zerami z prawej i z dołu tak, by była wymiaru $m \times n$. Wtedy
$$ U \Sigma V^T = \begin{pmatrix} U_1 & U_2 \end{pmatrix} \begin{pmatrix} D^{\frac{1}{2}} & 0 \\ 0 & 0 \end{pmatrix} \begin{pmatrix} V_1^T \\ V_2^T \end{pmatrix} = \begin{pmatrix} U_1 D^{\frac{1}{2}} & 0 \end{pmatrix} \begin{pmatrix} V_1^T \\ V_2^T \end{pmatrix} = U_1 D^{\frac{1}{2}} V_1^T = A $$
\end{proof}

\begin{att}
Macierz $U$ jest ortogonalną macierzą wektorów własnych macierzy $AA^T$.
\end{att}
\begin{proof}
Wystarczy pokazać, że macierz $U_1$ jest macierzą wektorów własnych $AA^T$, ponieważ $U_2$ dobieramy dowolnie tak, by stworzyć bazę ortonormalną (możemy zatem wykorzystać do tego wektory własne). Już wiemy, że te wektory są wzajemnie prostopadłe i niezerowe, więc są liniowo niezależne. Weźmy dowolny wektor z macierzy $U_1$. Niech $u_i = Av_i \frac{1}{\lambda_i}$.
$$ AA^T(Av_i) \frac{1}{\lambda_i} = \frac{1}{\lambda_i} A(A^TA)v_i $$
Wektor $v_i$ jest wektorem własnym macierzy $A^TA$ dla wartości własnej $\lambda_i$, zatem
$$ \frac{1}{\lambda_i} A(A^TA)v_i = \frac{1}{\lambda_i}A \lambda_i v_i = \sqrt{\lambda_i} Av_i $$
Stąd wektor $u_i$ jest wektorem własnym macierzy $AA^T$ dla wartości własnej $\lambda_i$.
\end{proof}

\section{Numeryczne wyznaczanie rozkładu SVD}
Rozkład SVD wykonujemy w dwóch krokach. Najpierw sprowadzamy macierz do postaci bidiagonalnej, tzn. do postaci, w której macierz ma wszystkie wyrazy zerowe oprócz tych na przekątnej i tuż nad przekątną.
$$ \begin{pmatrix} \ast & \ast & & & \\ & \ast & \ast & & \\ & & \ast & \ddots & \\ & & & \ddots & \ast \\ & & & & \ast \end{pmatrix} $$
Następnie dokonujemy rozkładu SVD na macierzy bidiagonalnej.

Oba kroki będą polegały na tym, że będziemy mnożyć naszą wyjściową macierz przez macierze ortogonalne o odpowiednim wymiarze, w taki sposób, by na końcu otrzymać macierz diagonalną.

Nasza wyjściowa macierz to $A \in M_{m \times n} (\mathbb{R})$. Zakładamy, że $m \geq n$, ponieważ przeciwny przypadek możemy rozwiązać po prostu przez transpozycję macierzy A, policzenie jej rozkładu SVD i ponowną transpozycję otrzymanego wyniku.

Opisywana metoda jest w uproszczeniu tą przedstawioną w artykule [1]. Pełna wersja została zaimplementowana w \textit{GNU Scientific Library} w funkcji \textit{gsl\_linalg\_SV\_decomp}. Programy przedstawiające zastosowania rozkładu SVD korzystają z tej właśnie funkcji.
\subsection{Bidiagonalizacja}
Do bidiagonalizacji będziemy wykorzystywać macierze transformacji Householdera. Wyraża się je wzorem
$$A = I - 2vv^t $$
gdzie $v$ jest wektorem długości $1$. Można pokazać, że macierz $A$ jest macierzą ortogonalną.

Metoda polega na tym, że po kolei będziemy zerować fragmenty odpowiednich kolumn i wierszy. W pierwszym kroku mnożymy naszą macierz przez taką macierz $P_1$, że 
$$ P_1 \cdot \begin{pmatrix} \ast & \ast & \ldots & \ast \\
\szary \ast & \ast & \ldots & \ast \\
\szary \vdots & \vdots & \ddots & \vdots \\
\szary \ast & \ast & \ldots & \ast
\end{pmatrix}
=
\begin{pmatrix} \ast & \ast & \ldots & \ast \\
\szary 0 & \ast & \ldots & \ast \\
 \szary \vdots & \vdots & \ddots & \vdots \\
\szary 0 & \ast & \ldots & \ast
\end{pmatrix}$$
Następnie z prawej strony mnożymy przez macierz $Q_1$ taką, że 
$$ \begin{pmatrix} \ast & \ast & \szary \ast & \szary \ldots & \szary \ast \\
0 & \ast & \ast & \ldots & \ast \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & \ast & \ast & \ldots & \ast
\end{pmatrix}
\cdot Q_1 = 
\begin{pmatrix} \ast & \ast & \szary 0 & \szary \ldots & \szary 0 \\
0 & \ast & \ast & \ldots & \ast \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & \ast & \ast & \ldots & \ast
\end{pmatrix}$$
Gdy chcemy wyzerować k-tą kolumnę ($P_k$), k-ty wiersz ($Q_k$), to macierze Householdera wyrażają się wzorem
$$ P_k = I - 2x^{(k)}{x^{(k)}}^T $$
$$ Q_k = I - 2{y^{(k)}y^{(k)}}^T $$
dla wektorów $x^{(k)}$, $y^{(k)} \in \mathbb{R}^n$.

Nie chcemy ingerować w już wyzerowane wyrazy, dlatego 
$$ x^{(k)}_i = 0 \text{ dla } i=1,\ldots,k-1$$
$$ y^{(k)}_j = 0 \text{ dla } j=1,\ldots,k$$

Dokładniejsze opisanie wyznaczania pozostałych wyrazów nie wnosiłoby niczego szczególnego do idei algorytmu, więc można prześledzić je w programie lub w pracy, w której została przedstawiona ta metoda.

Niech
$$ P = P_n \ldots P_1 $$
$$ Q = Q_1 \ldots Q_n $$

Zatem $PAQ = J$, gdzie $J$ bidiagonalna.
\subsection{Rozkład SVD macierzy bidiagonalnej}
Jeżeli otrzymamy rozkład SVD macierzy $J$, czyli $J=S\Sigma T$, to wtedy $A = (P^T S)\Sigma (TQ^T) $ będzie rozkładem macierzy $A$.

\subsubsection{Macierze obrotu Givensa}
Ustalmy, że jesteśmy w przestrzeni $\mathbb{R}^n$. Macierzą obrotu Givensa nazywamy macierz \\$G(i,j,\theta) \in M_{n \times n}(\mathbb{R})$ (dla $i<j$ i $\theta \in [0,2\pi ]$) w następującej postaci:
$$ G(i,j,\theta) = \begin{pmatrix}
1 & \ldots & 0 & \ldots & 0 & \ldots & 0 \\
\vdots & \ddots & \vdots & & \vdots & & \vdots \\
0 & \ldots & c & \ldots & -s & \ldots & 0 \\
\vdots & & \vdots & \ddots & \vdots & & \vdots \\
0 & \ldots & s & \ldots & c & \ldots & 0 \\
\vdots & & \vdots & & \vdots & \ddots & \vdots \\
0 & \ldots & 0 & \ldots & 0 & \ldots & 1
\end{pmatrix} $$
gdzie $c = \cos( \theta)$ i $ s = \sin (\theta)$.

Dokładniej, $G(i,j,\theta)$ jest macierzą identycznościową z wyjątkiem wyrazów
$$ g_{i,i} = g_{j,j} = c $$
$$ g_{i,j} = -g_{j,i} = s $$
Zauważmy, że ta macierz jest ortogonalna. 

Będziemy korzystać z macierzy obrotu Givensa, gdy będziemy chcieli wyzerować konkretny wyraz w macierzy. Taki sposób jest atrakcyjny, ponieważ macierz ta jest ortogonalna oraz skoro jest bliska macierzy identycznościowej, to w wyniku mnożenia macierzy przez macierz obrotu Givensa będziemy mogli większą część wyrazów po prostu przepisać.

Przemnożenie przez macierz obrotu Givensa z lewej strony modyfikuje tylko i-ty i j-ty wiersz, zaś z prawej -- i-tą i j-tą kolumnę.

\subsubsection{Algorytm}
Nasza metoda będzie teoretycznie nieskończona, tzn. po wykonaniu nieskończenie wielu iteracji otrzymalibyśmy dokładny wynik, ale na komputerze możemy przerwać obliczenia, gdy otrzymamy satysfakcjonującą nas dokładność bądź gdy nie będziemy już w stanie osiągnąć lepszej (z powodu precyzji arytmetyki na liczbach zmiennoprzecinkowych). Nie udało znaleźć się dowodu zbieżności tej metody.

Opiszemy jedną iterację algorytmu. Po każdej iteracji powinniśmy dalej mieć macierz bidiagonalną. Będziemy mnożyć macierz $J$ naprzemiennie z lewej i prawej strony przez macierze obrotu Givensa. 

Najpierw mnożymy z prawej strony przez macierz $T_2 = G(1,2, \theta)$. Sposób wybierania wartości $\theta$ może przyspieszać zbieżność ([1]), ale można przyjąć, że kąt $\theta$ jest dowolny.  Przemnożenie $J$ przez $T_2$ generuje wartość na miejscu $(2,1)$ macierzy.
$$ \begin{pmatrix}
\ast & \ast & 0 & \ldots & 0 \\
0 & \ast & \ast & \ldots & 0 \\
0 & 0 & \ast & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & \ast 
\end{pmatrix} \cdot T_2 = \begin{pmatrix}
\ast & \ast & 0 & \ldots & 0 \\
\szary \ast & \ast & \ast & \ldots & 0 \\
0 & 0 & \ast & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & \ast 
\end{pmatrix} $$

Następnie dobieramy macierz obrotu Givensa $S_2$ tak, by po przemnożeniu z lewej strony wyzerować element $(2,1)$. Jednak takie działanie zmieni wartość elementu na miejscu $(2, 4)$ i ten element wyzerujemy macierzą $T_3$, itd.
$$ S_2 \cdot \begin{pmatrix}
\ast & \ast & 0 & \ldots & 0 \\
\szary \ast & \ast & \ast & \ldots & 0 \\
0 & 0 & \ast & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & \ast 
\end{pmatrix} =
\begin{pmatrix}
\ast & \ast & \szary \ast & \ldots & 0 \\
0 & \ast & \ast & \ldots & 0 \\
0 & 0 & \ast & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & \ast 
\end{pmatrix}$$

Można to zobrazować następująco:
$$\begin{pmatrix}
\ast & \ast  & \szary \tikzmark{13} & & & & \\
\szary \tikzmark{21} & \ast & \ast & \szary \; \: \tikzmark{24} & & & \\
 & \szary \tikzmark{32} & \ast & & & &  \\
  & & &  & \ddots &  & \\
  & & & & & & \szary \tikzmark{56} \\
  & & & & & \ast & \ast \\
 & & & & & \szary \tikzmark{65} & \ast 
\end{pmatrix} $$

 \begin{tikzpicture}
    [
      remember picture,
      overlay,
      -latex,
      color=black,
      yshift=1ex,
      shorten >=1pt,
      shorten <=1pt,
    ]
    \draw ({pic cs:21}) -- ({pic cs:13});
    \draw ({pic cs:13}) -- ({pic cs:32});
    \draw ({pic cs:32}) -- ({pic cs:24});
    \draw ({pic cs:56}) -- ({pic cs:65});
  \end{tikzpicture}
  
Przy mnożeniu przez macierz obrotu Givensa kasujemy jeden wyraz spoza dwóch przekątnych, ale tworzymy następny. ''Gonimy'' niezerowy wyraz tak długo, aż nie znajdzie się ''poza macierzą''.

Zatem w jednej iteracji macierz $J$ zamieniamy na 
$$(S_n \ldots S_2) J (T_2 \ldots T_n)$$

Gdy wyrazy poza przekątną będą wystarczająco małe, to możemy wstawić za nie $0$. Proces powtarzamy tak długo, aż zostanie nam macierz diagonalna. W zaproponowanej przez nas implementacji wykonujemy dla uproszczenia $200$ iteracji. Musimy spamiętywać i aktualizować (poprzez mnożenie po każdej iteracji) macierze ortogonalne $S = S_n \ldots S_2$ i $T = T_2 \ldots T_n$.

\section{Zastosowania rozkładu SVD}
\subsection{Kompresja danych}
Niech $A \in M_{m \times n} (\mathbb{R}) $ i $A = U \Sigma V^T$ (SVD). Załóżmy, że
$$ \Sigma = \begin{pmatrix}
\sigma_1 &  & & 0 & \ldots & 0 \\
 & \ddots & & \vdots & & \vdots \\
 & & \sigma_r & 0 & \ldots & 0 \\
0 & \ldots & 0 & 0 & \ldots & 0 \\
\vdots & & \vdots & \vdots & & \vdots \\
0 & \ldots & 0 & 0 & \ldots & 0 \\
\end{pmatrix} $$

Oczywiście $\Sigma$ jest macierzą prostokątną.
Zapiszmy macierze $U$ i $V$ jako $U = \begin{pmatrix} u_1 & \ldots & u_m \end{pmatrix}$ i $V = \begin{pmatrix} v_1 & \ldots & v_n \end{pmatrix}$.

Wtedy

$$ A = U \Sigma V^T = \begin{pmatrix} u_1 & \ldots & u_m \end{pmatrix} \Sigma \begin{pmatrix}
v_1^T \\ \vdots \\ v_n^T
\end{pmatrix} = \sigma_1 u_1 v_1^T + \sigma_2 u_2 v_2^T + \ldots + \sigma_r u_r v_r^T  $$

% Skoro wartości na przekątnej macierzy $\Sigma$ są uporządkowane nierosnąco oraz wektory $u_i$ oraz $v_i$ są długości $1$, to wyzerowanie ostatnich niezerowych wyrazów z przekątnej powinno spowodować niewielką zmianę w macierzy $A$, a zmniejszy to ilość pamięci potrzebnej do przechowywania danych. Jeżeli wzięlibyśmy pierwszych $l$ wyrazów, to potrzebowalibyśmy tylko pamiętać wektory $u_1$, \ldots, $u_l$, $v_1$, \ldots, $v_l$ i wyrazy $\sigma_1$,~\ldots,~$\sigma_l$.

Skoro wartości na przekątnej macierzy $\Sigma$ są uporządkowane nierosnąco oraz wektory $u_i$ oraz $v_i$ są długości $1$, to usunięcie ostatnich niezerowych wartości szczególnych oraz odpowiadających im wektorów z macierzy $U$ i $V$ powinno spowodować niewielką zmianę w macierzy $A$, a zmniejszy to ilość pamięci potrzebnej do przechowywania danych. Jeżeli wzięlibyśmy pierwszych $l$ wyrazów, to potrzebowalibyśmy tylko pamiętać wektory $u_1$, \ldots, $u_l$, $v_1$, \ldots, $v_l$ i wyrazy $\sigma_1$,~\ldots,~$\sigma_l$.

Dane często są przechowywane w postaci bardzo dużych prostokątnych macierzy, więc przedstawienie ich w postaci zredukowanego rozkładu SVD może przynieść dużą oszczędność pamięciową. W programie \textit{Kompresja.ipynb} przedstawiona została kompresja stratna obrazów.

\subsection{Pseudoodwrotność}

\begin{definition}
Pseudoodwrotnością (lub odwrotnością Moore'a-Penrose'a lub uogólnioną odwrotnością) macierzy $A \in M_{m \times n} (\mathbb{R})$ nazywamy macierz $A^\dag$ spełniającą następujące warunki:
\begin{itemize}
    \item[(P1)] $AA^\dag A = A$
    \item[(P2)] $A^\dag A A^\dag = A^\dag$
    \item[(P3)] $(AA^\dag)^T = AA^\dag$
    \item[(P4)] $(A^\dag A)^T = A^\dag A$
\end{itemize}
Okazuje się, że macierz $A^\dag$ jest jednoznacznie określona. 
\end{definition}

\begin{fact}
Niech $A \in M_{m \times n} (\mathbb{R})$. Niech $A = U \Sigma V^T$ będzie rozkładem SVD macierzy. Wtedy
$$ A^\dag = V \Sigma^\dag U^T $$
gdzie
$$ \Sigma^\dag = \begin{pmatrix}
\frac{1}{\sigma_1} &  & & 0 & \ldots & 0 \\
 & \ddots & & \vdots & & \vdots \\
 & & \frac{1}{\sigma_r} & 0 & \ldots & 0 \\
0 & \ldots & 0 & 0 & \ldots & 0 \\
\vdots & & \vdots & \vdots & & \vdots \\
0 & \ldots & 0 & 0 & \ldots & 0 \\
\end{pmatrix} $$
\end{fact}

Jak widzimy pseudoodwrotność macierzy możemy łatwo policzyć przez policzenie rozkładu SVD. Wykorzystuje się ją m.in. w statystyce i w rozwiązywaniu prostokątnych układów równań liniowych.

\subsection{Rozwiązywanie równań jednorodnych}
Mamy zadanie wyznaczenia przestrzeni rozwiązań układu jednorodnego
$$Ax = 0$$
Wykorzystamy do tego następujący fakt.
\begin{fact}
 $(\forall x \in \mathbb{R} ^n) (Ax=0 \Leftrightarrow A^TAx=0) $
\end{fact}
\begin{proof}Weźmy dowolny wekotr $x \in \mathbb{R}^n$
\begin{itemize}
    \item[$\Rightarrow$] Załóżmy, że $Ax=0$. Wtedy $$A^TAx = A^T(Ax) = A^T 0 = 0 $$ 
    \item[$\Leftarrow$] Załóżmy, że $A^TAx=0$. Przemnóżmy nasze równanie z lewej strony przez $x^T$. Mamy
    $$ x^TA^TAx = 0 $$
    $$ (Ax)^TAx = 0 $$
    $$ Ax \circ Ax = 0 \Leftrightarrow Ax = 0 $$
    z definicji iloczynu skalarnego.
\end{itemize}
\end{proof}
Wiemy, że macierz $A^TA$ jest symetryczna, więc z twierdzenia spektralnego możemy ją przedstawić w postaci iloczynu $A = VDV^T$. Teraz wystarczy zauważyć, że pytanie o rozwiązanie układu $A^TAx =0$ można zamienić na pytanie o jądro przekształcenia liniowego zadanego macierzą $A^TA$. Zauważmy, że jądro jest przestrzenią rozpinaną przez kolumny macierzy $V$ odpowiadające zerowym wartościom własnym.

Podobnie dla rozkładu SVD ($A = U\Sigma V^T$) wiersze z $V^T$ odpowiadające zerowym wartościom z macierzy $\Sigma$ rozpinają przestrzeń rozwiązań równania $A^TA x =0$. 

\subsection{Obraz, jądro i rząd}
\begin{fact}
Dla macierzy $A = U \Sigma V^T$:
        \begin{enumerate}
            \item kolumny macierzy $U$ odpowiadające niezerowym wartościom szczególnym rozpinają obraz przekształcenia liniowego zadanego macierzą $A$
            \item kolumny macierzy $V$ odpowiadające zerowym wartościom własnym $A^TA$ rozpinają jądro przekształcenia liniowego zadanego macierzą $A$
            \item $rank(A) = \text{ \# wartości szczególnych}$
        \end{enumerate}
\end{fact}
\begin{proof}
Przypomnijmy, że możemy zapisać nasz rozkład jako
$$ U \Sigma V^T = \begin{pmatrix}
U_1 & U_2
\end{pmatrix}
\begin{pmatrix}
\Sigma_1 & 0 \\
0 & 0
\end{pmatrix}
\begin{pmatrix}
V_1^T \\ V_2^T
\end{pmatrix} = U_1 \Sigma_1 V_1^T
$$
gdzie $\Sigma_1$ ma niezerowe wykłady na przekątnej $\sigma_1$, \ldots, $\sigma_r$ oraz $U_1 = A V_1 \Sigma_1^{-1}$. Zauważmy zatem, że kolumny $U_1$ rozpinają podprzestrzeń obrazu przekształcenia (ponadto są liniowo niezależne). Z \textit{Faktu 4.3} wiemy już, że punkt 2. jest prawdziwy, czyli $dim \: Ker \: F_A = n-r$. Z twierdzenia o indeksie
$$ dim \: Ker \: F_A + dim \: Im \: F_A = n $$
Czyli $dim \: Im \: F_A = n - (n-r) = r$. Otrzymaliśmy zatem, że kolumny $U_1$ rozpinają cały obraz. Rząd macierzy można definiować jako wymiar obrazu, więc $rank(A) = r$.
\end{proof}
Znając rozkład SVD macierzy $A$ i korzystając z powyższego faktu można w łatwy sposób wyznaczyć jej rząd oraz obraz i jądro przekształcenia liniowego zadanego tą macierzą. 
\begin{thebibliography}{9}
\bibitem{1970} 
G. H. Golub and C. Reinsch \textit{Handbook Series Linear Algebra. Singular Value Decomposition and Least Squares Solutions.} Numer. Math. 14, 403--420 (1970)

\bibitem{1965}
G. Golub and W. Kahan, \textit{Calculating the Singular Values and Pseudo-inverse of a Matrix.} J. Siam Numer. Anal., Ser. B, Vol. 2, No. 2. Printed in U.S.A.. 1965

\bibitem{wiki-svd}
\textit{Singular value decomposition.} Wikipedia: The Free Encyklopedia (16.05.2020)
\\\texttt{https://en.wikipedia.org/wiki/Singular\_value\_decomposition} 

\bibitem{zastos}
\textit{Understanding Singular Value Decomposition and its Application in Data Science.} (16.05.2020)
\\\texttt{https://towardsdatascience.com/understanding-singular-value-decomposition\\-and-its-application-in-data-science-388a54be95d} 

\bibitem{dowod}
\textit{Proof of the Singular Value Decomposition} (16.05.2020)
\\\texttt{https://gregorygundersen.com/blog/2018/12/20/svd-proof/}

\bibitem{kielbas}
A. Kiełbasiński and Hubert Schwetlick, \textit{Numeryczna algebra liniowa.} Wydawnictwo Naukowo-Techniczne

\bibitem{djvu}
Åke Björck and Germund Dahlquist, \textit{Numerical Methods in Scientific Computing. Volume II.}

\bibitem{algos}
\textit{MATH2071: LAB \#9: The Singular Value Decomposition.} (16.05.2020)
\\\texttt{http://www.math.pitt.edu/~sussmanm/2071Spring08/lab09/index.html}

\bibitem{juzniemoge}
\textit{Wikiwand. Singular value decomposition.} (16.05.2020)
\\\texttt{https://www.wikiwand.com/en/Singular\_value\_decomposition?fbclid=\\IwAR0\_B1ohQBsQnXalAqx869TnLd2jWU2Mc2TQTwnkXkV\_pMLyT3H189kIbz4\#}
\end{thebibliography}
\end{document}

% to do
% zastosowania SVD
% kompresja stratna obrazów - intuicja

% bibliografia
% dowód:
% https://gregorygundersen.com/blog/2018/12/20/svd-proof/#2-a-and-atop-a-have-the-same-rank
% https://gregorygundersen.com/blog/2018/12/10/svd/
% https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf
